{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\mausa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id                                               text\n0         0                 Just happened a terrible car crash\n1         2  Heard about #earthquake is different cities, s...\n2         3  there is a forest fire at spot pond, geese are...\n3         9           Apocalypse lighting. #Spokane #wildfires\n4        11      Typhoon Soudelor kills 28 in China and Taiwan\n...     ...                                                ...\n3258  10861  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n3259  10865  Storm in RI worse than last hurricane. My city...\n3260  10868  Green Line derailment in Chicago http://t.co/U...\n3261  10874  MEG issues Hazardous Weather Outlook (HWO) htt...\n3262  10875  #CityofCalgary has activated its Municipal Eme...\n\n[3263 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>Storm in RI worse than last hurricane. My city...</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>Green Line derailment in Chicago http://t.co/U...</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>#CityofCalgary has activated its Municipal Eme...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\", usecols=['id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7434 entries, 0 to 7612\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      7434 non-null   int64 \n 1   text    7434 non-null   object\n 2   target  7434 non-null   int64 \ndtypes: int64(2), object(1)\nmemory usage: 232.3+ KB\n"
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fichur Inginierin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  target  \\\n0   1  Our Deeds are the Reason of this #earthquake M...       1   \n1   4             Forest fire near La Ronge Sask. Canada       1   \n2   5  All residents asked to 'shelter in place' are ...       1   \n3   6  13,000 people receive #wildfires evacuation or...       1   \n4   7  Just got sent this photo from Ruby #Alaska as ...       1   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0              Our Deeds Reason May ALLAH Forgive us      69         4.384615   \n1                   Forest fire near La Ronge Canada      38         4.571429   \n2  All residents asked notified No evacuation she...     133         5.090909   \n3        people receive evacuation orders California      65         7.125000   \n4        Just got sent photo Ruby smoke pours school      88         4.500000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0               13                      13     0.2732                6   \n1                7                       7    -0.3400                0   \n2               22                      20    -0.2960               11   \n3                8                       8     0.0000                1   \n4               16                      15     0.0000                7   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  1               0               1   \n1                  1               0               0   \n2                  3               0               0   \n3                  2               0               1   \n4                  2               0               2   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      7             0.461538   \n1                                      6             0.000000   \n2                                     10             0.500000   \n3                                     10             0.125000   \n4                                      6             0.437500   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 0            6            1              0  \n1                 0            6            0              0  \n2                 1            7            7              0  \n3                 1            4            1              0  \n4                 0            6            3              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>Our Deeds Reason May ALLAH Forgive us</td>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.461538</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Canada</td>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>All residents asked notified No evacuation she...</td>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>20</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>people receive evacuation orders California</td>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.125000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>Just got sent photo Ruby smoke pours school</td>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.437500</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)\n",
    "\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  \\\n0   0                 Just happened a terrible car crash   \n1   2  Heard about #earthquake is different cities, s...   \n2   3  there is a forest fire at spot pond, geese are...   \n3   9           Apocalypse lighting. #Spokane #wildfires   \n4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0                   Just happened terrible car crash      34         4.833333   \n1                          Heard different stay safe      64         6.222222   \n2  forest fire spot geese fleeing across I cannot...      96         4.105263   \n3                                         Apocalypse      40         9.250000   \n4                Typhoon Soudelor kills China Taiwan      45         4.750000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0                6                    13.0    -0.7003                2   \n1                9                     7.0     0.4404                2   \n2               19                    20.0    -0.6159                9   \n3                4                     8.0     0.0000                0   \n4                8                    15.0    -0.5423                2   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  0               0               0   \n1                  3               0               1   \n2                  2               0               0   \n3                  3               0               2   \n4                  0               0               0   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      8             0.153846   \n1                                      9             0.285714   \n2                                      7             0.409091   \n3                                     10             0.000000   \n4                                      8             0.125000   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 1            2            1              1  \n1                 2            4            2              0  \n2                 2            4            4              1  \n3                 0            4            0              0  \n4                 0            4            1              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n      <td>Just happened terrible car crash</td>\n      <td>34</td>\n      <td>4.833333</td>\n      <td>6</td>\n      <td>13.0</td>\n      <td>-0.7003</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.153846</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td>Heard different stay safe</td>\n      <td>64</td>\n      <td>6.222222</td>\n      <td>9</td>\n      <td>7.0</td>\n      <td>0.4404</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0.285714</td>\n      <td>2</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td>forest fire spot geese fleeing across I cannot...</td>\n      <td>96</td>\n      <td>4.105263</td>\n      <td>19</td>\n      <td>20.0</td>\n      <td>-0.6159</td>\n      <td>9</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0.409091</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td>Apocalypse</td>\n      <td>40</td>\n      <td>9.250000</td>\n      <td>4</td>\n      <td>8.0</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td>Typhoon Soudelor kills China Taiwan</td>\n      <td>45</td>\n      <td>4.750000</td>\n      <td>8</td>\n      <td>15.0</td>\n      <td>-0.5423</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.125000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "test = test[['id','text']]\n",
    "test['text_without_stopwords'] = test['text'].str.split()\n",
    "test['text_without_stopwords'] = test['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test['length'] = test['text'].apply(lambda x: len(x))\n",
    "test['avg_word_length'] = test['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test['amount_of_words'] = test['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test['sentiment'] = test['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test['stopwords_count'] = test['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test['punctuation_count'] = test['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test['longest_word_length_without_stopwords'] = test['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test['stopword_word_ratio'] = test['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "test['adjectives_count'] = test['text'].apply(get_adjectives)\n",
    "test['nouns_count'] = test['text'].apply(get_nouns)\n",
    "test['verbs_count'] = test['text'].apply(get_verbs)\n",
    "test['adverbs_count'] = test['text'].apply(get_adverbs)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   length  avg_word_length  amount_of_words  sentiment  stopwords_count  \\\n0      69         4.384615               13     0.2732                6   \n1      38         4.571429                7    -0.3400                0   \n2     133         5.090909               22    -0.2960               11   \n3      65         7.125000                8     0.0000                1   \n4      88         4.500000               16     0.0000                7   \n\n   punctuation_count  longest_word_length_without_stopwords  \\\n0                  1                                      7   \n1                  1                                      6   \n2                  3                                     10   \n3                  2                                     10   \n4                  2                                      6   \n\n   amount_of_unique_words  hashtags_count  mentions_count  \n0                      13               1               0  \n1                       7               0               0  \n2                      20               0               0  \n3                       8               1               0  \n4                      15               2               0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>amount_of_unique_words</th>\n      <th>hashtags_count</th>\n      <th>mentions_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>7</td>\n      <td>13</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>10</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>10</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>6</td>\n      <td>15</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]\n",
    "basic_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   length  avg_word_length  amount_of_words  sentiment  stopwords_count  \\\n0      34         4.833333                6    -0.7003                2   \n1      64         6.222222                9     0.4404                2   \n2      96         4.105263               19    -0.6159                9   \n3      40         9.250000                4     0.0000                0   \n4      45         4.750000                8    -0.5423                2   \n\n   punctuation_count  longest_word_length_without_stopwords  \\\n0                  0                                      8   \n1                  3                                      9   \n2                  2                                      7   \n3                  3                                     10   \n4                  0                                      8   \n\n   amount_of_unique_words  hashtags_count  mentions_count  \n0                    13.0               0               0  \n1                     7.0               1               0  \n2                    20.0               0               0  \n3                     8.0               2               0  \n4                    15.0               0               0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>amount_of_unique_words</th>\n      <th>hashtags_count</th>\n      <th>mentions_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34</td>\n      <td>4.833333</td>\n      <td>6</td>\n      <td>-0.7003</td>\n      <td>2</td>\n      <td>0</td>\n      <td>8</td>\n      <td>13.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>64</td>\n      <td>6.222222</td>\n      <td>9</td>\n      <td>0.4404</td>\n      <td>2</td>\n      <td>3</td>\n      <td>9</td>\n      <td>7.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96</td>\n      <td>4.105263</td>\n      <td>19</td>\n      <td>-0.6159</td>\n      <td>9</td>\n      <td>2</td>\n      <td>7</td>\n      <td>20.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40</td>\n      <td>9.250000</td>\n      <td>4</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>10</td>\n      <td>8.0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45</td>\n      <td>4.750000</td>\n      <td>8</td>\n      <td>-0.5423</td>\n      <td>2</td>\n      <td>0</td>\n      <td>8</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#basic_features_test = test[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "basic_features_test = test[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]\n",
    "basic_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,346,529\n",
      "Trainable params: 2,346,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 8s 1ms/step - loss: 0.5909 - accuracy: 0.6987\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 8s 1ms/step - loss: 0.3571 - accuracy: 0.8518\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    batch_size=65)\n",
    "#loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "#print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "#loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.743716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.566755</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.961872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.941275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.066874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.047537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.053390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.135765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob  target\n",
       "0   0  0.879785       1\n",
       "1   2  0.908730       1\n",
       "2   3  0.743716       1\n",
       "3   9  0.566755       1\n",
       "4  11  0.961872       1\n",
       "5  12  0.941275       1\n",
       "6  21  0.066874       0\n",
       "7  22  0.047537       0\n",
       "8  27  0.053390       0\n",
       "9  29  0.135765       0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test['id']\n",
    "submission1['prob'] = model.predict(x_test_kagle)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submission1['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(\"submit_prueba_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una mierda (al final no xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luego del submit, mas pruebas con esto (probando con Glove) (dio 0,80570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#test = pd.read_csv('test.csv')\n",
    "#test = test[['id','text']]\n",
    "\n",
    "#x_test_kagle = test['text'].values\n",
    "\n",
    "#x = tweets_metrics['text'].values\n",
    "#y = tweets_metrics['target'].values\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.23, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "#x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 100\n",
    "#embedding_matrix = create_embedding_matrix('glove.6B.200d.txt',tokenizer.word_index, embedding_dim)\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 140, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 134, 128)          89728     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 2,372,389\n",
      "Trainable params: 2,372,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras import layers\n",
    "#embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 15s 2ms/step - loss: 0.4875 - accuracy: 0.7638\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 14s 2ms/step - loss: 0.3456 - accuracy: 0.8536\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 14s 2ms/step - loss: 0.2587 - accuracy: 0.9015\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5724/5724 [==============================] - 2s 349us/step\n",
      "Training Accuracy: 0.9415\n",
      "1710/1710 [==============================] - 1s 352us/step\n",
      "Testing Accuracy:  0.8158, Loss  0.4759\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}, Loss  {:.4f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.806478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.946599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.870620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.977202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.805770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.018496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.021431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.072302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob  target\n",
       "0   0  0.806478       1\n",
       "1   2  0.946599       1\n",
       "2   3  0.870620       1\n",
       "3   9  0.914281       1\n",
       "4  11  0.977202       1\n",
       "5  12  0.805770       1\n",
       "6  21  0.018496       0\n",
       "7  22  0.021431       0\n",
       "8  27  0.010347       0\n",
       "9  29  0.072302       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test['id']\n",
    "submission1['prob'] = model.predict(x_test_kagle)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del submission1['prob']\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(\"submit_prueba_14.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras: K fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5141 - accuracy: 0.7556 - val_loss: 0.4774 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3620 - accuracy: 0.8483 - val_loss: 0.4435 - val_accuracy: 0.8029\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2710 - accuracy: 0.8964 - val_loss: 0.4952 - val_accuracy: 0.7921\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 257us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  29.7s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.6s remaining:    0.0s\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5259 - accuracy: 0.7588 - val_loss: 0.4598 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3761 - accuracy: 0.8401 - val_loss: 0.4288 - val_accuracy: 0.8029\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2877 - accuracy: 0.8872 - val_loss: 0.4336 - val_accuracy: 0.8011\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 262us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  29.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.4751 - accuracy: 0.7784 - val_loss: 0.4593 - val_accuracy: 0.7688\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3191 - accuracy: 0.8699 - val_loss: 0.4573 - val_accuracy: 0.7849\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.2290 - accuracy: 0.9149 - val_loss: 0.4693 - val_accuracy: 0.7814\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 288us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  31.3s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.5092 - accuracy: 0.7559 - val_loss: 0.4199 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3562 - accuracy: 0.8509 - val_loss: 0.4166 - val_accuracy: 0.8280\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.2612 - accuracy: 0.9022 - val_loss: 0.4358 - val_accuracy: 0.8172\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 0s 267us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  31.0s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5251 - accuracy: 0.7377 - val_loss: 0.4503 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3499 - accuracy: 0.8543 - val_loss: 0.4259 - val_accuracy: 0.8065\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2603 - accuracy: 0.9047 - val_loss: 0.4399 - val_accuracy: 0.7921\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 454us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  29.2s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.4991 - accuracy: 0.7654 - val_loss: 0.4462 - val_accuracy: 0.8136\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 12s 2ms/step - loss: 0.3369 - accuracy: 0.8575 - val_loss: 0.4642 - val_accuracy: 0.7832\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  25.0s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5079 - accuracy: 0.7642 - val_loss: 0.4476 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.3421 - accuracy: 0.8555 - val_loss: 0.4287 - val_accuracy: 0.8047\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2454 - accuracy: 0.9073 - val_loss: 0.4657 - val_accuracy: 0.7849\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 445us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  38.1s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.5054 - accuracy: 0.7615 - val_loss: 0.4291 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3408 - accuracy: 0.8613 - val_loss: 0.4206 - val_accuracy: 0.8082\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2457 - accuracy: 0.9119 - val_loss: 0.4993 - val_accuracy: 0.8100\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 424us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  34.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.5406 - accuracy: 0.7297 - val_loss: 0.4604 - val_accuracy: 0.7939\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.3779 - accuracy: 0.8352 - val_loss: 0.4234 - val_accuracy: 0.8154\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2889 - accuracy: 0.8880 - val_loss: 0.4546 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 354us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  33.5s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.4939 - accuracy: 0.7648 - val_loss: 0.4612 - val_accuracy: 0.7867\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.3503 - accuracy: 0.8495 - val_loss: 0.4238 - val_accuracy: 0.8118\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 12s 2ms/step - loss: 0.2678 - accuracy: 0.9009 - val_loss: 0.4252 - val_accuracy: 0.8190\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 457us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  35.3s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5261 - accuracy: 0.7635 - val_loss: 0.4986 - val_accuracy: 0.7509\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.3573 - accuracy: 0.8539 - val_loss: 0.4417 - val_accuracy: 0.7975\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2805 - accuracy: 0.8886 - val_loss: 0.4388 - val_accuracy: 0.7993\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2286 - accuracy: 0.9177 - val_loss: 0.4454 - val_accuracy: 0.7975\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 403us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  47.6s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5251 - accuracy: 0.7453 - val_loss: 0.4337 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3875 - accuracy: 0.8352 - val_loss: 0.3988 - val_accuracy: 0.8441\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3029 - accuracy: 0.8804 - val_loss: 0.4095 - val_accuracy: 0.8423\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 424us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  35.4s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5044 - accuracy: 0.7562 - val_loss: 0.4389 - val_accuracy: 0.8100\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3768 - accuracy: 0.8344 - val_loss: 0.4330 - val_accuracy: 0.7993\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3049 - accuracy: 0.8780 - val_loss: 0.4466 - val_accuracy: 0.7903\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 262us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  29.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.4953 - accuracy: 0.7632 - val_loss: 0.4946 - val_accuracy: 0.7652\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.3618 - accuracy: 0.8413 - val_loss: 0.4354 - val_accuracy: 0.8011\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.2861 - accuracy: 0.8890 - val_loss: 0.4320 - val_accuracy: 0.8190\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.2375 - accuracy: 0.9153 - val_loss: 0.4631 - val_accuracy: 0.8082\n",
      "Epoch 00004: early stopping\n",
      "1859/1859 [==============================] - 0s 253us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  30.4s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.5642 - accuracy: 0.6825 - val_loss: 0.4918 - val_accuracy: 0.7527\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3923 - accuracy: 0.8330 - val_loss: 0.4887 - val_accuracy: 0.7616\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3194 - accuracy: 0.8677 - val_loss: 0.4350 - val_accuracy: 0.8029\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.2526 - accuracy: 0.9083 - val_loss: 0.4424 - val_accuracy: 0.8029\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 282us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  29.6s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.5183 - accuracy: 0.7475 - val_loss: 0.4363 - val_accuracy: 0.8208\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3567 - accuracy: 0.8551 - val_loss: 0.4273 - val_accuracy: 0.8172\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.2758 - accuracy: 0.9033 - val_loss: 0.4326 - val_accuracy: 0.8172\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 309us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  22.6s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5139 - accuracy: 0.7441 - val_loss: 0.4678 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3837 - accuracy: 0.8350 - val_loss: 0.4287 - val_accuracy: 0.8136\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2938 - accuracy: 0.8850 - val_loss: 0.4342 - val_accuracy: 0.8065\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  28.0s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.4898 - accuracy: 0.7688 - val_loss: 0.4453 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3395 - accuracy: 0.8565 - val_loss: 0.5039 - val_accuracy: 0.7545\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 328us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  21.6s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.4900 - accuracy: 0.7654 - val_loss: 0.4399 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3345 - accuracy: 0.8587 - val_loss: 0.4420 - val_accuracy: 0.7957\n",
      "Epoch 00002: early stopping\n",
      "1858/1858 [==============================] - 1s 370us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  23.5s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.5100 - accuracy: 0.7505 - val_loss: 0.4274 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3813 - accuracy: 0.8400 - val_loss: 0.4016 - val_accuracy: 0.8405\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2900 - accuracy: 0.8870 - val_loss: 0.4243 - val_accuracy: 0.8082\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 365us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  31.3s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 10.3min finished\n",
      "Train on 6690 samples, validate on 744 samples\n",
      "Epoch 1/15\n",
      "6690/6690 [==============================] - 14s 2ms/step - loss: 0.4663 - accuracy: 0.7834 - val_loss: 0.4185 - val_accuracy: 0.8145\n",
      "Epoch 2/15\n",
      "6690/6690 [==============================] - 14s 2ms/step - loss: 0.3259 - accuracy: 0.8646 - val_loss: 0.4480 - val_accuracy: 0.7970\n",
      "Epoch 00002: early stopping\n",
      "Best Accuracy : 0.7942\n",
      "{'vocab_size': 22811, 'num_filters': 128, 'maxlen': 140, 'kernel_size': 5, 'embedding_dim': 100, 'batch_size': 45}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "# Main settings\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "\n",
    "# Train-test split\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1, random_state=1000)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences with zeros\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Parameter grid for grid search\n",
    "param_grid = dict(num_filters=[32, 128, 144],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [45,65,76,88])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=5, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train, callbacks=[callback])\n",
    "\n",
    "# Evaluate testing set\n",
    "#test_accuracy = grid.score(x_test, y_test)\n",
    "\n",
    "# Save and evaluate results\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "            \n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit del 0,81274 con lo extraido del random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#test = pd.read_csv('test.csv')\n",
    "#test = test[['id','text']]\n",
    "\n",
    "#x_test_kagle = test['text'].values\n",
    "\n",
    "#x = tweets_metrics['text'].values\n",
    "#y = tweets_metrics['target'].values\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "#x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_156 (Embedding)    (None, 140, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 134, 128)          89728     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_156 (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,372,129\n",
      "Trainable params: 2,372,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras import layers\n",
    "#embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 7, activation='relu'))\n",
    "#model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 12s 2ms/step - loss: 0.5673 - accuracy: 0.7171\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 11s 2ms/step - loss: 0.3841 - accuracy: 0.8351\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 12s 2ms/step - loss: 0.2866 - accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    #validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}, Loss  {:.4f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test['id']\n",
    "submission1['prob'] = model.predict(x_test_kagle)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission1['prob']\n",
    "submission1.to_csv(\"submit_prueba_13.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeras pruebas con los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "#x_train_features = pad_sequences(x_train_features, padding='post', maxlen=maxlen)\n",
    "#x_test_features = features.transform(basic_features_test)\n",
    "\n",
    "#x_test_kagle = test['text'].values\n",
    "\n",
    "#x = tweets_metrics['text'].values\n",
    "#y = tweets_metrics['target'].values\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "#x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d():\n",
    "        \n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (10, ), name = 'x_features_train')\n",
    "    emb = embedding(x_train_input)\n",
    "    \n",
    "    conv_out = Conv1D(128, 7, activation='relu')(emb)\n",
    "\n",
    "    max_pool = GlobalMaxPooling1D()(conv_out)\n",
    "    conc = Concatenate()([max_pool, x_train_features_input])\n",
    "    #x = Dropout(0.2)(conc)\n",
    "    \n",
    "    dense1 = Dense(10, activation='relu')(conc)\n",
    "    dense2 = Dense(1, activation='sigmoid')(dense1)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense2)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_9\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nx_train_input (InputLayer)      (None, 140)          0                                            \n__________________________________________________________________________________________________\nembedding_11 (Embedding)        (None, 140, 100)     2281100     x_train_input[0][0]              \n__________________________________________________________________________________________________\nconv1d_10 (Conv1D)              (None, 134, 128)     89728       embedding_11[0][0]               \n__________________________________________________________________________________________________\nglobal_max_pooling1d_10 (Global (None, 128)          0           conv1d_10[0][0]                  \n__________________________________________________________________________________________________\nx_features_train (InputLayer)   (None, 10)           0                                            \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 138)          0           global_max_pooling1d_10[0][0]    \n                                                                 x_features_train[0][0]           \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 10)           1390        concatenate_7[0][0]              \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 1)            11          dense_19[0][0]                   \n==================================================================================================\nTotal params: 2,372,229\nTrainable params: 2,372,229\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = create_conv1d()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n7434/7434 [==============================] - 12s 2ms/step - loss: 0.4838 - accuracy: 0.7723\nEpoch 2/2\n7434/7434 [==============================] - 12s 2ms/step - loss: 0.3464 - accuracy: 0.8511\n"
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    #validation_split=0.2,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_csv('test.csv')\n",
    "test_kagle = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "x_test_features = features.transform(basic_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = pd.DataFrame()\n",
    "submission1['id'] = test_kagle['id']\n",
    "submission1['prob'] = model.predict([x_test_kagle,x_test_features])\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submission1['prob']\n",
    "submission1.to_csv(\"submit_prueba_22.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id  prob  target\n40      125   NaN       1\n48      154   NaN       1\n106     359   NaN       1\n115     378   NaN       1\n118     385   NaN       1\n...     ...   ...     ...\n3240  10781   NaN       1\n3243  10796   NaN       1\n3248  10807   NaN       1\n3251  10828   NaN       1\n3261  10874   NaN       1\n\n[75 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prob</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40</th>\n      <td>125</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>154</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>359</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>378</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>385</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3240</th>\n      <td>10781</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3243</th>\n      <td>10796</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3248</th>\n      <td>10807</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3251</th>\n      <td>10828</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "submission1[submission1['prob'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mausa': virtualenv)",
   "language": "python",
   "name": "python37664bitmausavirtualenv161e16f0fd2c481895212b5856f17cf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}