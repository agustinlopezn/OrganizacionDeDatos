{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\", usecols=['id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_features = pd.read_csv('../TP2/train_features.csv')\n",
    "test_w_features = pd.read_csv('../TP2/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_only_train = train_w_features[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "#features_only_train = tweets_metrics[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_only_test = test_w_features[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "#features_only_test = test[['length','avg_word_length','amount_of_words','sentiment', 'stopwords_count', 'punctuation_count', 'longest_word_length_without_stopwords', 'amount_of_unique_words', 'hashtags_count', 'mentions_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_embedding_matrix_840(route, word_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(route, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-embedding_dim])\n",
    "        coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: # Si la palabra no esta queda llena de 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm, min_max_norm\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size):\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "    emb = embedding(x_train_input)\n",
    "    \n",
    "    conv_out = Conv1D(num_filters, kernel_size, activation='relu')(emb)\n",
    "    max_pool = GlobalMaxPooling1D()(conv_out)\n",
    "    \n",
    "    conc = Concatenate()([max_pool, x_train_features_input])\n",
    "   \n",
    "    dense1 = Dense(dense1_size, activation='relu')(conc)\n",
    "    dense2 = Dense(dense2_size, activation='relu')(dense1)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense2)\n",
    "    return model\n",
    "\n",
    "def create_port_to_model(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size):\n",
    "    combi_input = Input(shape = (155,), name = 'port')\n",
    "    input_train = Lambda(lambda x: x[:,:-15])(combi_input)\n",
    "    input_features = Lambda(lambda x: x[:,140:])(combi_input)\n",
    "\n",
    "    base_network = create_conv1d(num_filters, kernel_size,vocab_size,embedding_dim,maxlen,batch_size,dense1_size,dense2_size)\n",
    "    processed = base_network([input_train,input_features])\n",
    "\n",
    "    dense3 = Dense(1, activation='sigmoid')(processed)\n",
    "    model = Model(combi_input,dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5171 - accuracy: 0.7473 - val_loss: 0.5379 - val_accuracy: 0.7509\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3783 - accuracy: 0.8385 - val_loss: 0.4833 - val_accuracy: 0.7652\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2915 - accuracy: 0.8882 - val_loss: 0.4813 - val_accuracy: 0.7796\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.2236 - accuracy: 0.9197 - val_loss: 0.4517 - val_accuracy: 0.8047\n",
      "Epoch 5/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.1452 - accuracy: 0.9569 - val_loss: 0.5113 - val_accuracy: 0.7778\n",
      "Epoch 00005: early stopping\n",
      "1859/1859 [==============================] - 1s 332us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  46.1s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.0s remaining:    0.0s\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.5077 - accuracy: 0.7642 - val_loss: 0.5022 - val_accuracy: 0.7616\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3717 - accuracy: 0.8399 - val_loss: 0.5288 - val_accuracy: 0.7527\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 332us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  18.4s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.6008 - accuracy: 0.6736 - val_loss: 0.4639 - val_accuracy: 0.7885\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.3958 - accuracy: 0.8348 - val_loss: 0.4314 - val_accuracy: 0.8100\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.3038 - accuracy: 0.8786 - val_loss: 0.4457 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 333us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  26.3s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.5724 - accuracy: 0.7033 - val_loss: 0.4244 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.4062 - accuracy: 0.8296 - val_loss: 0.4107 - val_accuracy: 0.8423\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.3247 - accuracy: 0.8762 - val_loss: 0.4219 - val_accuracy: 0.8369\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 373us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=20, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  26.9s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5724 - accuracy: 0.6938 - val_loss: 0.4634 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.4289 - accuracy: 0.8146 - val_loss: 0.4527 - val_accuracy: 0.7903\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3472 - accuracy: 0.8585 - val_loss: 0.4449 - val_accuracy: 0.7921\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3001 - accuracy: 0.8806 - val_loss: 0.4707 - val_accuracy: 0.7832\n",
      "Epoch 00004: early stopping\n",
      "1859/1859 [==============================] - 1s 340us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  36.6s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5294 - accuracy: 0.7233 - val_loss: 0.4912 - val_accuracy: 0.7581\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3692 - accuracy: 0.8417 - val_loss: 0.4598 - val_accuracy: 0.7867\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3132 - accuracy: 0.8712 - val_loss: 0.4520 - val_accuracy: 0.8029\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2406 - accuracy: 0.9107 - val_loss: 0.4488 - val_accuracy: 0.7921\n",
      "Epoch 5/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.1823 - accuracy: 0.9386 - val_loss: 0.4679 - val_accuracy: 0.7975\n",
      "Epoch 00005: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  45.5s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.5339 - accuracy: 0.7367 - val_loss: 0.4659 - val_accuracy: 0.7849\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.3744 - accuracy: 0.8426 - val_loss: 0.4386 - val_accuracy: 0.8100\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2799 - accuracy: 0.8880 - val_loss: 0.4639 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 360us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  27.8s\n",
      "[CV] batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.6379 - accuracy: 0.6250 - val_loss: 0.4886 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.4340 - accuracy: 0.8236 - val_loss: 0.4155 - val_accuracy: 0.8280\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.3304 - accuracy: 0.8743 - val_loss: 0.4099 - val_accuracy: 0.8441\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2516 - accuracy: 0.9093 - val_loss: 0.4359 - val_accuracy: 0.8262\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 369us/step\n",
      "[CV]  batch_size=88, dense1_size=10, dense2_size=10, embedding_dim=100, kernel_size=7, maxlen=140, num_filters=128, vocab_size=22811, total=  38.2s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.4min finished\n",
      "Train on 6690 samples, validate on 744 samples\n",
      "Epoch 1/15\n",
      "6690/6690 [==============================] - 12s 2ms/step - loss: 0.5066 - accuracy: 0.7581 - val_loss: 0.4362 - val_accuracy: 0.7944\n",
      "Epoch 2/15\n",
      "6690/6690 [==============================] - 12s 2ms/step - loss: 0.3665 - accuracy: 0.8472 - val_loss: 0.4244 - val_accuracy: 0.8105\n",
      "Epoch 3/15\n",
      "6690/6690 [==============================] - 11s 2ms/step - loss: 0.2750 - accuracy: 0.8966 - val_loss: 0.4665 - val_accuracy: 0.7970\n",
      "Epoch 00003: early stopping\n",
      "Best Accuracy : 0.7854\n",
      "{'batch_size': 88, 'dense1_size': 10, 'dense2_size': 10, 'embedding_dim': 100, 'kernel_size': 7, 'maxlen': 140, 'num_filters': 128, 'vocab_size': 22811}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(num_filters=[128],\n",
    "                      kernel_size=[7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [88],\n",
    "                      dense1_size = [10],\n",
    "                      dense2_size = [20,10])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_port_to_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                              cv=4, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(np.concatenate((x_train,x_train_features), axis = 1), y_train, callbacks=[callback])\n",
    "\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_dense1_size</th>\n",
       "      <th>param_dense2_size</th>\n",
       "      <th>param_embedding_dim</th>\n",
       "      <th>param_kernel_size</th>\n",
       "      <th>param_maxlen</th>\n",
       "      <th>param_num_filters</th>\n",
       "      <th>param_vocab_size</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.782677</td>\n",
       "      <td>10.218281</td>\n",
       "      <td>0.639944</td>\n",
       "      <td>0.032818</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>140</td>\n",
       "      <td>128</td>\n",
       "      <td>22811</td>\n",
       "      <td>{'batch_size': 88, 'dense1_size': 10, 'dense2_...</td>\n",
       "      <td>0.789672</td>\n",
       "      <td>0.766541</td>\n",
       "      <td>0.771798</td>\n",
       "      <td>0.789020</td>\n",
       "      <td>0.779258</td>\n",
       "      <td>0.010261</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.360590</td>\n",
       "      <td>6.298348</td>\n",
       "      <td>0.661582</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>140</td>\n",
       "      <td>128</td>\n",
       "      <td>22811</td>\n",
       "      <td>{'batch_size': 88, 'dense1_size': 10, 'dense2_...</td>\n",
       "      <td>0.791286</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.785791</td>\n",
       "      <td>0.785444</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      28.782677     10.218281         0.639944        0.032818   \n",
       "1      36.360590      6.298348         0.661582        0.020972   \n",
       "\n",
       "  param_batch_size param_dense1_size param_dense2_size param_embedding_dim  \\\n",
       "0               88                10                20                 100   \n",
       "1               88                10                10                 100   \n",
       "\n",
       "  param_kernel_size param_maxlen param_num_filters param_vocab_size  \\\n",
       "0                 7          140               128            22811   \n",
       "1                 7          140               128            22811   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'batch_size': 88, 'dense1_size': 10, 'dense2_...           0.789672   \n",
       "1  {'batch_size': 88, 'dense1_size': 10, 'dense2_...           0.791286   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.766541           0.771798           0.789020         0.779258   \n",
       "1           0.786444           0.778256           0.785791         0.785444   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.010261                2  \n",
       "1        0.004661                1  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_result.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm, min_max_norm\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_features = scaler.fit_transform(features_only_train)\n",
    "\n",
    "x_train = train_w_features['text'].values\n",
    "y_train = train_w_features['target'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 50\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_1 = 100\n",
    "embedding_matrix_1 = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim_1)\n",
    "\n",
    "embedding_dim_2 = 300\n",
    "embedding_matrix_2 = create_embedding_matrix_840('Embeddings/glove.840B.300d.txt',tokenizer.word_index, embedding_dim_2)\n",
    "\n",
    "embedding_dim_3 = 200\n",
    "embedding_matrix_3 = create_embedding_matrix('Embeddings/glove.6B.200d.txt',tokenizer.word_index, embedding_dim_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d(): #original\n",
    "        \n",
    "    embedding = Embedding(vocab_size, embedding_dim_1, input_length=maxlen, weights=[embedding_matrix_1], trainable=False)\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim_2, input_length=maxlen, weights=[embedding_matrix_2], trainable=True)\n",
    "    embedding3 = Embedding(vocab_size, embedding_dim_3, input_length=maxlen, weights=[embedding_matrix_3], trainable=False)\n",
    "\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "\n",
    "    emb = embedding(x_train_input)\n",
    "    emb2 = embedding2(x_train_input)\n",
    "    emb3 = embedding3(x_train_input)\n",
    "\n",
    "    #Emb 100\n",
    "    #conv_out1_1 = Conv1D(128, 2, activation='relu')(emb)\n",
    "    #activation1_1 = Activation('relu')(conv_out1_1)\n",
    "    #max_pool1_1 = GlobalMaxPooling1D()(activation1_1)\n",
    "    #conv_out1_2 = Conv1D(128, 3, activation='relu')(emb)\n",
    "    #activation1_2 = Activation('relu')(conv_out1_2)\n",
    "    #max_pool1_2 = GlobalMaxPooling1D()(activation1_2)\n",
    "\n",
    "    #Emb 200\n",
    "    conv_out2_1 = Conv1D(128, 2, activation='relu', kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(emb2)\n",
    "    activation2_1 = Activation('relu')(conv_out2_1)\n",
    "    max_pool2_1 = GlobalMaxPooling1D()(activation2_1)\n",
    "    conv_out2_2 = Conv1D(128, 3, activation='relu', kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(activation2_1)\n",
    "    activation2_2 = Activation('relu')(conv_out2_2)\n",
    "    max_pool2_2 = GlobalMaxPooling1D()(activation2_2)\n",
    "\n",
    "    #Emb 300\n",
    "    #conv_out3_1 = Conv1D(128, 2, activation='relu')(emb3)\n",
    "    #activation3_1 = Activation('relu')(conv_out3_1)\n",
    "    #max_pool3_1 = GlobalMaxPooling1D()(activation3_1)\n",
    "    #conv_out3_2 = Conv1D(128, 3, activation='relu')(emb3)\n",
    "    #activation3_2 = Activation('relu')(conv_out3_2)\n",
    "    #max_pool3_2 = GlobalMaxPooling1D()(activation3_2)\n",
    "\n",
    "    conc = Concatenate()([max_pool2_2,  x_train_features_input])\n",
    "    \n",
    "    #noise1 = GaussianNoise(0.1)(conc)\n",
    "    dense1 = Dense(100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(conc)\n",
    "    noise2 = GaussianNoise(0.1)(dense1)\n",
    "    \n",
    "    dense2 = Dense(10, activation='relu')(dense1)\n",
    "    #noise1 = GaussianNoise(0.001)(dense2)\n",
    "    dense3 = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 21s 3ms/step - loss: 1.0982 - accuracy: 0.7818\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.5130 - accuracy: 0.8608\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.3117 - accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "model = create_conv1d()\n",
    "history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    batch_size=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7925222188170395\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv('perfect_submission.csv')\n",
    "test_kagle = test_w_features[['id','text']]\n",
    "\n",
    "x_test_kagle = test_w_features['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)\n",
    "x_test_features = features.transform(features_only_test)\n",
    "\n",
    "submit_df = pd.DataFrame()\n",
    "submit_df['id'] = test_kagle['id']\n",
    "submit_df['prob'] = model.predict([x_test_kagle,x_test_features])\n",
    "submit_df['target'] = submit_df['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "\n",
    "accuracy = accuracy_score(submit_df['target'], result['target'])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.820410665032179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submit_df['prob']\n",
    "submit_df.to_csv(\"submit_prueba_45.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de pre-trained words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = np.load('Embeddings/glove.840B.300d.pkl', allow_pickle=True)\n",
    "fasttext_embeddings = np.load('Embeddings/crawl-300d-2M.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 52.17% of vocabulary and 82.73% of text in Training Set\n",
      "GloVe Embeddings cover 57.21% of vocabulary and 81.85% of text in Test Set\n",
      "FastText Embeddings cover 51.63% of vocabulary and 81.88% of text in Training Set\n",
      "FastText Embeddings cover 56.55% of vocabulary and 81.12% of text in Test Set\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "def build_vocab(X):\n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    vocab = build_vocab(X)    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage\n",
    "\n",
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(tweets_metrics['text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test['text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n",
    "\n",
    "train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(tweets_metrics['text'], fasttext_embeddings)\n",
    "test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(test['text'], fasttext_embeddings)\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5652974442155101"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de misslabeled tweets individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "pa_checkear = test_w_features[['id','text']]\n",
    "pa_checkear['target'] = result['target']\n",
    "with open('perfect_submission.csv') as perfecto, open('submit_prueba_43.csv') as submit:\n",
    "    perfect_csv = csv.reader(perfecto)\n",
    "    submit_csv = csv.reader(submit)\n",
    "    perf = {}\n",
    "    subm = {}\n",
    "    misseados = []\n",
    "    for id,target in perfect_csv:\n",
    "        perf[id] = perf.get(id,target)\n",
    "    for id,target in submit_csv:\n",
    "        subm[id] = subm.get(id,target)\n",
    "    for id in subm:\n",
    "        if subm[id] != perf[id]:\n",
    "            misseados.append(id)\n",
    "    for i in range(len(misseados)):\n",
    "        misseados[i] = int(misseados[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misseados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_checkear['wrong_label'] = pa_checkear['id'].apply(lambda x: x in misseados)\n",
    "pa_checkear[pa_checkear['wrong_label'] == True].to_csv('mal_labeleados.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    287\n",
       "True     282\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_checkear[pa_checkear['wrong_label'] == True]['text'].str.contains('http').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mausa': virtualenv)",
   "language": "python",
   "name": "python37664bitmausavirtualenv161e16f0fd2c481895212b5856f17cf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
