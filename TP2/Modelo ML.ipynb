{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read csv, remove puctuation, stopwords and apply stemming, replace keywords with 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "tweets['keyword'] = tweets['keyword'].fillna('NULL')\n",
    "test['keyword'] = test['keyword'].fillna('NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = tweets.duplicated(subset = 'text', keep = False)\n",
    "duplicates.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets.keyword.str.replace('%20',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].str.split()\n",
    "tweets['text'] = tweets['text'].apply(remove_stopword)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].apply(stemm)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering graphic\n",
    "\n",
    "* SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(x_train_tfidf)\n",
    "svd_result = svd.transform(x_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = pd.DataFrame({'x': svd_result[:, 0], 'y': svd_result[:, 1], 'target': tweets['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = to_plot[to_plot['target'] == 1].plot.scatter(x='x', y='y', s=8, alpha=0.8, color='blue', label='Real', figsize=(10, 10))\n",
    "to_plot[to_plot['target'] == 0].plot.scatter(x='x', y='y', s=8, alpha=0.8, color='orange', label='Not real', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(tweets['text'], tweets.loc[:,['id','target']], test_size = 0.25, random_state = 123)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_vectors = tfidf_vectorizer.fit_transform(x_train)\n",
    "test_vectors = tfidf_vectorizer.transform(x_test)\n",
    "array = train_vectors.todense()\n",
    "\n",
    "tfidf_matrix = pd.DataFrame(array)\n",
    "tfidf_matrix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Naive Bayes \n",
    "\n",
    "En el caso de una predicci√≥n binaria es recomendable bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli = BernoulliNB().fit(train_vectors, y_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted = bernoulli.predict(test_vectors)\n",
    "print(accuracy_score(y_test['target'], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(0.5, 1.0, 20)\n",
    "alpha = np.around(alpha, decimals=4)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{\"alpha\":alpha}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#classifier = BernoulliNB()\n",
    "#gridsearch = GridSearchCV(classifier, grid, scoring = 'neg_log_loss', cv = 4)\n",
    "#gridsearch.fit(df[features], y_train['target'])\n",
    "#print(\"Best parameter: \",gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_t = BernoulliNB(alpha=0.9474).fit(train_vectors, y_train['target'])\n",
    "predicted_t = bernoulli_t.predict(test_vectors)\n",
    "print(accuracy_score(y_test['target'], predicted_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "* Count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(tweets[['text', 'keyword']], tweets['target'], test_size = 0.25, random_state = 123)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vectorizer = CountVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english', max_features=5000)\n",
    "train_vectors = tfidf_vectorizer.fit_transform(x_train['text'])\n",
    "test_vectors = tfidf_vectorizer.transform(x_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = CountVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english', max_features=300)\n",
    "key_train_vectors = tfidf_vectorizer.fit_transform(x_train['keyword'])\n",
    "key_test_vectors = tfidf_vectorizer.transform(x_test['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "matrix_final = hstack([train_vectors, key_train_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_final = matrix_final.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(matrix_final, label=y_train)\n",
    "\n",
    "params = {\n",
    "    'learning_rate' : 0.02,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'binary_logloss',\n",
    "    'num_leaves' : 50,\n",
    "    'max_depth' : 5\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params, d_train, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = hstack([test_vectors, key_test_vectors])\n",
    "test_final = test_final.astype('float32')\n",
    "y_pred = gbm.predict(test_final)\n",
    "\n",
    "for i in range (0, len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:       \n",
    "        y_pred[i] = 1\n",
    "    else:  \n",
    "        y_pred[i] = 0\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_features = pd.read_csv(\"train_features.csv\")\n",
    "test_features = pd.read_csv(\"test_features.csv\")\n",
    "keyword_w2v = pd.read_csv(\"keyword_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_features = tweets_features.drop(columns=['text', 'text_without_stopwords', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_features.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tweets.merge(tweets_features, left_on='id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = merged.merge(keyword_w2v, left_on='keyword', right_on='keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = total_features.columns\n",
    "features = features.drop('target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features with tweet vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(total_features[features], total_features['target'], test_size = 0.25, random_state = 123)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vectorizer = CountVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english', max_features=11000)\n",
    "train_vectors = tfidf_vectorizer.fit_transform(x_train['text'])\n",
    "test_vectors = tfidf_vectorizer.transform(x_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_vectors.todense()\n",
    "train_matrix = pd.DataFrame(train_array)\n",
    "x_train.reset_index(inplace=True, drop=True)\n",
    "train_matrix['id'] = x_train['id']\n",
    "X_train = x_train.merge(train_matrix).drop(columns=['keyword', 'location', 'text', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = test_vectors.todense()\n",
    "test_matrix = pd.DataFrame(test_array)\n",
    "x_test.reset_index(inplace=True, drop=True)\n",
    "test_matrix['id'] = x_test['id']\n",
    "X_test = x_test.merge(test_matrix).drop(columns=['keyword', 'location', 'text', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "params : {\n",
    "    'learning_rate' : 0.02,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'binary_logloss',\n",
    "    'num_leaves' : 500,\n",
    "    'max_depth' : 2,\n",
    "    'max_bin': 1000\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params, d_train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test)\n",
    "\n",
    "for i in range (0, len(y_pred)):\n",
    "    if y_pred[i] > 0.5:       \n",
    "        y_pred[i] = 1\n",
    "    else:  \n",
    "        y_pred[i] = 0\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"train.csv\", usecols=['text', 'target'])\n",
    "test_text = pd.read_csv(\"test.csv\", usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7434 entries, 0 to 7612\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7434 non-null   object\n",
      " 1   target  7434 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 174.2+ KB\n"
     ]
    }
   ],
   "source": [
    "text.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['text'] = text['text'].apply(lambda x: x.lower())\n",
    "test_text['text'] = test_text['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  our deeds are the reason of this #earthquake m...       1\n",
       "1             forest fire near la ronge sask. canada       1\n",
       "2  all residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  just got sent this photo from ruby #alaska as ...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apocalypse lighting. #spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>typhoon soudelor kills 28 in china and taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 just happened a terrible car crash\n",
       "1  heard about #earthquake is different cities, s...\n",
       "2  there is a forest fire at spot pond, geese are...\n",
       "3           apocalypse lighting. #spokane #wildfires\n",
       "4      typhoon soudelor kills 28 in china and taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 't': 4,\n",
       " 'a': 5,\n",
       " 'o': 6,\n",
       " 'i': 7,\n",
       " 'n': 8,\n",
       " 's': 9,\n",
       " 'r': 10,\n",
       " 'h': 11,\n",
       " 'l': 12,\n",
       " 'c': 13,\n",
       " 'd': 14,\n",
       " 'u': 15,\n",
       " 'p': 16,\n",
       " 'm': 17,\n",
       " '/': 18,\n",
       " 'g': 19,\n",
       " 'f': 20,\n",
       " 'y': 21,\n",
       " 'w': 22,\n",
       " '.': 23,\n",
       " 'b': 24,\n",
       " 'k': 25,\n",
       " 'v': 26,\n",
       " ':': 27,\n",
       " '#': 28,\n",
       " 'j': 29,\n",
       " \"'\": 30,\n",
       " '?': 31,\n",
       " 'x': 32,\n",
       " '@': 33,\n",
       " 'z': 34,\n",
       " '0': 35,\n",
       " '1': 36,\n",
       " 'q': 37,\n",
       " '-': 38,\n",
       " '2': 39,\n",
       " '5': 40,\n",
       " '3': 41,\n",
       " '4': 42,\n",
       " '7': 43,\n",
       " '9': 44,\n",
       " '6': 45,\n",
       " '!': 46,\n",
       " '8': 47,\n",
       " '\\n': 48,\n",
       " '_': 49,\n",
       " '\\x89': 50,\n",
       " '√ª': 51,\n",
       " ';': 52,\n",
       " '&': 53,\n",
       " ')': 54,\n",
       " '(': 55,\n",
       " '*': 56,\n",
       " '¬™': 57,\n",
       " '|': 58,\n",
       " '[': 59,\n",
       " ']': 60,\n",
       " '√•': 61,\n",
       " '+': 62,\n",
       " '√Ø': 63,\n",
       " '√™': 64,\n",
       " '=': 65,\n",
       " '√∑': 66,\n",
       " '%': 67,\n",
       " '√≤': 68,\n",
       " '$': 69,\n",
       " '\\x9d': 70,\n",
       " '~': 71,\n",
       " '√≥': 72,\n",
       " '√¨': 73,\n",
       " '¬©': 74,\n",
       " '¬¢': 75,\n",
       " '¬£': 76,\n",
       " '^': 77,\n",
       " '¬®': 78,\n",
       " '√®': 79,\n",
       " '\\\\': 80,\n",
       " '¬º': 81,\n",
       " '}': 82,\n",
       " '√±': 83,\n",
       " '¬§': 84,\n",
       " '¬°': 85,\n",
       " '`': 86,\n",
       " '{': 87,\n",
       " ',': 88,\n",
       " '√£': 89,\n",
       " '√º': 90,\n",
       " '√ß': 91,\n",
       " '√¢': 92,\n",
       " '¬´': 93,\n",
       " '>': 94,\n",
       " '¬¥': 95,\n",
       " '¬¨': 96}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caracteres muy horrendos\n",
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "    \n",
    "tk.word_index = char_dict.copy() \n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1 #UNK es el valor mas alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ',': 37,\n",
       " ';': 38,\n",
       " '.': 39,\n",
       " '!': 40,\n",
       " '?': 41,\n",
       " ':': 42,\n",
       " \"'\": 43,\n",
       " '\"': 44,\n",
       " '/': 45,\n",
       " '\\\\': 46,\n",
       " '|': 47,\n",
       " '_': 48,\n",
       " '@': 49,\n",
       " '#': 50,\n",
       " '$': 51,\n",
       " '%': 52,\n",
       " '^': 53,\n",
       " '&': 54,\n",
       " '*': 55,\n",
       " '~': 56,\n",
       " '`': 57,\n",
       " '+': 58,\n",
       " '-': 59,\n",
       " '=': 60,\n",
       " '<': 61,\n",
       " '>': 62,\n",
       " '(': 63,\n",
       " ')': 64,\n",
       " '[': 65,\n",
       " ']': 66,\n",
       " '{': 67,\n",
       " '}': 68,\n",
       " 'UNK': 69}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora el texto se representa con una secuencia de caracteres\n",
    "sequences = tk.texts_to_sequences(text['text'])\n",
    "test_sequences = tk.texts_to_sequences(test_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 21,\n",
       " 18,\n",
       " 69,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 19,\n",
       " 69,\n",
       " 1,\n",
       " 18,\n",
       " 5,\n",
       " 69,\n",
       " 20,\n",
       " 8,\n",
       " 5,\n",
       " 69,\n",
       " 18,\n",
       " 5,\n",
       " 1,\n",
       " 19,\n",
       " 15,\n",
       " 14,\n",
       " 69,\n",
       " 15,\n",
       " 6,\n",
       " 69,\n",
       " 20,\n",
       " 8,\n",
       " 9,\n",
       " 19,\n",
       " 69,\n",
       " 50,\n",
       " 5,\n",
       " 1,\n",
       " 18,\n",
       " 20,\n",
       " 8,\n",
       " 17,\n",
       " 21,\n",
       " 1,\n",
       " 11,\n",
       " 5,\n",
       " 69,\n",
       " 13,\n",
       " 1,\n",
       " 25,\n",
       " 69,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 8,\n",
       " 69,\n",
       " 6,\n",
       " 15,\n",
       " 18,\n",
       " 7,\n",
       " 9,\n",
       " 22,\n",
       " 5,\n",
       " 69,\n",
       " 21,\n",
       " 19,\n",
       " 69,\n",
       " 1,\n",
       " 12,\n",
       " 12]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 21,\n",
       " 19,\n",
       " 20,\n",
       " 69,\n",
       " 8,\n",
       " 1,\n",
       " 16,\n",
       " 16,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 4,\n",
       " 69,\n",
       " 1,\n",
       " 69,\n",
       " 20,\n",
       " 5,\n",
       " 18,\n",
       " 18,\n",
       " 9,\n",
       " 2,\n",
       " 12,\n",
       " 5,\n",
       " 69,\n",
       " 3,\n",
       " 1,\n",
       " 18,\n",
       " 69,\n",
       " 3,\n",
       " 18,\n",
       " 1,\n",
       " 19,\n",
       " 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding de cada secuencia para que todas tengan el mismo largo\n",
    "data = pad_sequences(sequences, maxlen=1014, padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=1014, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7434, 1014)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 1014)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array(test_data)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = text['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHAR CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = len(tk.word_index)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(size))\n",
    "\n",
    "for char, i in tk.word_index.items():\n",
    "    row = np.zeros(size)\n",
    "    row[i-1] = 1\n",
    "    embedding_weights.append(row)\n",
    "    \n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 69)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1014)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1014, 69)          4830      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1008, 256)         123904    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 504, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 256)          459008    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 498, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 248, 256)          131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 248, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 247, 256)          131328    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 247, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 246, 256)          131328    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 246, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 245, 256)          131328    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 245, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 245, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 62720)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              64226304  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 66,389,983\n",
      "Trainable params: 66,389,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parametros\n",
    "input_size = 1014\n",
    "embedding_size = 69\n",
    "conv_layers = [[256, 7, 2], \n",
    "               [256, 7, 2], \n",
    "               [256, 2, -1], \n",
    "               [256, 2, -1], \n",
    "               [256, 2, -1], \n",
    "               [256, 2, 1]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 1\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "embedding_layer = Embedding(size+1, \n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "\n",
    "# Embedding \n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Conv \n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x) \n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        \n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)\n",
    "    x = Dropout(dropout_p)(x)\n",
    "    \n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='sigmoid')(x)\n",
    "\n",
    "# Build model\n",
    "model1 = Model(inputs=inputs, outputs=predictions)\n",
    "model1.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "44/44 - 136s - loss: 0.6944 - accuracy: 0.5726 - val_loss: 0.6785 - val_accuracy: 0.5675\n",
      "Epoch 2/6\n",
      "44/44 - 141s - loss: 0.6528 - accuracy: 0.6151 - val_loss: 0.6365 - val_accuracy: 0.6541\n",
      "Epoch 3/6\n",
      "44/44 - 136s - loss: 0.6396 - accuracy: 0.6326 - val_loss: 0.6213 - val_accuracy: 0.6595\n",
      "Epoch 4/6\n",
      "44/44 - 144s - loss: 0.6093 - accuracy: 0.6691 - val_loss: 0.5821 - val_accuracy: 0.7052\n",
      "Epoch 5/6\n",
      "44/44 - 143s - loss: 0.5665 - accuracy: 0.7175 - val_loss: 0.5326 - val_accuracy: 0.7214\n",
      "Epoch 6/6\n",
      "44/44 - 142s - loss: 0.4324 - accuracy: 0.8131 - val_loss: 0.5312 - val_accuracy: 0.7633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1514a7b90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(data, train_classes, test_size = 0.25, random_state = 123)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 2, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model1.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = model1.predict(submit_data)\n",
    "\n",
    "y_submit\n",
    "\n",
    "s = []\n",
    "\n",
    "for l in y_submit:\n",
    "    if l[0] > l[1]:\n",
    "        s.append(0)\n",
    "    else:\n",
    "        s.append(1)\n",
    "\n",
    "s\n",
    "\n",
    "test_text = pd.read_csv(\"test.csv\", usecols=['id'])\n",
    "\n",
    "test_text['target'] = s\n",
    "\n",
    "test_text\n",
    "\n",
    "test_text.to_csv('submit_prueba_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1014)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1014, 69)          4830      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1008, 256)         123904    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1002, 256)         459008    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 1002, 256)         65792     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1002, 256)         65792     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 1002, 256)         65792     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1002, 256)         65792     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1002, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256512)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              262669312 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 264,570,847\n",
      "Trainable params: 264,570,847\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parametros\n",
    "input_size = 1014\n",
    "embedding_size = 69\n",
    "conv_layers = [[256, 7, 1], \n",
    "               [256, 7, 1], \n",
    "               [256, 1, -1], \n",
    "               [256, 1, -1], \n",
    "               [256, 1, -1], \n",
    "               [256, 1, 1]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 1\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "embedding_layer = Embedding(size+1, \n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "\n",
    "# Embedding \n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Conv \n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x) \n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        \n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)\n",
    "    x = Dropout(dropout_p)(x)\n",
    "    \n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='sigmoid')(x)\n",
    "\n",
    "# Build model\n",
    "model2 = Model(inputs=inputs, outputs=predictions)\n",
    "model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 - 321s - loss: 0.6758 - accuracy: 0.5670 - val_loss: 0.6429 - val_accuracy: 0.6498\n",
      "Epoch 2/10\n",
      "44/44 - 302s - loss: 0.6466 - accuracy: 0.6326 - val_loss: 0.6286 - val_accuracy: 0.6692\n",
      "Epoch 3/10\n",
      "44/44 - 331s - loss: 0.6313 - accuracy: 0.6511 - val_loss: 0.6183 - val_accuracy: 0.6638\n",
      "Epoch 4/10\n",
      "44/44 - 335s - loss: 0.6213 - accuracy: 0.6604 - val_loss: 0.6086 - val_accuracy: 0.6525\n",
      "Epoch 5/10\n",
      "44/44 - 309s - loss: 0.5739 - accuracy: 0.7031 - val_loss: 0.5543 - val_accuracy: 0.7257\n",
      "Epoch 6/10\n",
      "44/44 - 298s - loss: 0.4835 - accuracy: 0.7742 - val_loss: 0.5292 - val_accuracy: 0.7552\n",
      "Epoch 7/10\n",
      "44/44 - 298s - loss: 0.3713 - accuracy: 0.8377 - val_loss: 0.5945 - val_accuracy: 0.7171\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x164680890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(data, train_classes, test_size = 0.25, random_state = 123)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model2.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1014)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1014, 69)          4830      \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1008, 256)         123904    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 1008, 256)         65792     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1008, 256)         65792     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 258048)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              264242176 \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 265,553,119\n",
      "Trainable params: 265,553,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parametros\n",
    "input_size = 1014\n",
    "embedding_size = 69\n",
    "conv_layers = [[256, 7, 1], \n",
    "               [256, 1, -1], \n",
    "               [256, 1, 1]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 1\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "embedding_layer = Embedding(size+1, \n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "\n",
    "# Embedding \n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Conv \n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x) \n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        \n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)\n",
    "    x = Dropout(dropout_p)(x)\n",
    "    \n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='sigmoid')(x)\n",
    "\n",
    "# Build model\n",
    "model3 = Model(inputs=inputs, outputs=predictions)\n",
    "model3.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 - 179s - loss: 0.6925 - accuracy: 0.5598 - val_loss: 0.6527 - val_accuracy: 0.6089\n",
      "Epoch 2/10\n",
      "44/44 - 197s - loss: 0.6396 - accuracy: 0.6352 - val_loss: 0.6010 - val_accuracy: 0.6918\n",
      "Epoch 3/10\n",
      "44/44 - 166s - loss: 0.5769 - accuracy: 0.7013 - val_loss: 0.5584 - val_accuracy: 0.7294\n",
      "Epoch 4/10\n",
      "44/44 - 174s - loss: 0.4244 - accuracy: 0.8124 - val_loss: 0.6095 - val_accuracy: 0.7278\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1615a1f10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(data, train_classes, test_size = 0.25, random_state = 123)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model3.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
