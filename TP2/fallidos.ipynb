{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/agustin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import string\n",
    "import io\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords']\n",
    "basic_data_cols = ['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','longest_word_length_without_stopwords']\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(tweets_metrics[data_cols], tweets_metrics['target'], test_size = 0.25, random_state = 123)\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_lgbm = vectorizer.fit_transform(tweets_metrics.loc[:, 'text'])\n",
    "array = tfidf_lgbm.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(array)\n",
    "df['output'] = tweets['target']\n",
    "basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords']]\n",
    "tfidf_features = df.merge(basic_features, left_index = True, right_index = True)\n",
    "tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_features.columns.drop('output')\n",
    "x = tfidf_features.loc[:, features].values\n",
    "y = tfidf_features.loc[:, 'output'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tfidf_train, x_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_tfidf_train = sc.fit_transform(x_tfidf_train)\n",
    "x_tfidf_test = sc.transform(x_tfidf_test)\n",
    "print(x_tfidf_train.shape, x_tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_train, x_text_test, y_text_train, y_text_test = \\\n",
    "train_test_split(tweets_metrics['text'], tweets_metrics['target'], test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vectorizer = TfidfVectorizer()\n",
    "train_vectors = tfid_vectorizer.fit_transform(x_text_train)\n",
    "test_vectors = tfid_vectorizer.transform(x_text_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "count_train = count_vectorizer.fit_transform(x_text_train)\n",
    "count_test = count_vectorizer.transform(x_text_test)\n",
    "print(count_train.shape, count_test.shape, y_text_train.shape,y_text_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split de Hash vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vectorizer = HashingVectorizer()\n",
    "hash_train = hash_vectorizer.fit_transform(x_text_train)\n",
    "hash_test = hash_vectorizer.fit_transform(x_text_test)\n",
    "sc = StandardScaler(with_mean=False)\n",
    "hash_train = sc.fit_transform(hash_train)\n",
    "hash_test = sc.transform(hash_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN : baseline\n",
    " *En principio usando un bootstrap del set de entrenamiento para medir accuracy. Se utilizo un 25% del set para entrenar y el 75% restante para la predicción. Se utilizó la representación TF-IDF para el entrenamiento ya que KNN funciona con valores numericos.*\n",
    " \n",
    "Methods\n",
    "\n",
    "* fit(self, X, y) Fit the model using X as training data and y as target values\n",
    "\n",
    "* get_params(self[, deep]) Get parameters for this estimator.\n",
    "\n",
    "* kneighbors(self[, X, n_neighbors, …]) Finds the K-neighbors of a point.\n",
    "\n",
    "* kneighbors_graph(self[, X, n_neighbors, mode]) Computes the (weighted) graph of k-Neighbors for points in X\n",
    "\n",
    "* predict(self, X) Predict the class labels for the provided data.\n",
    "\n",
    "* predict_proba(self, X) Return probability estimates for the test data X.\n",
    "\n",
    "* score(self, X, y[, sample_weight]) Return the mean accuracy on the given test data and labels.\n",
    "\n",
    "* set_params(self, \\*\\*params) Set the parameters of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train, y_train)\n",
    "max = knn.score(x_test, y_test)\n",
    "actual = max\n",
    "optimo = 1\n",
    "\n",
    "for i in range(2, 100):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(x_train, y_train)\n",
    "    actual = knn.score(x_test, y_test)\n",
    "    if max < actual:\n",
    "        optimo = i \n",
    "        max = actual\n",
    "\n",
    "print(optimo, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=43)\n",
    "all_accuracies = cross_val_score(estimator=knn, X=x_train, y=y_train, cv=40)\n",
    "print(all_accuracies.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(train_vectors, y_text_train)\n",
    "max = knn.score(test_vectors, y_text_test)\n",
    "actual = max\n",
    "optimo = 1\n",
    "\n",
    "for i in range(2, 100):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(train_vectors, y_text_train)\n",
    "    actual = knn.score(test_vectors, y_text_test)\n",
    "    if max < actual:\n",
    "        optimo = i \n",
    "        max = actual\n",
    "\n",
    "print(optimo, max) #0.7961269499731038 con 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN con CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(count_train, y_text_train)\n",
    "max = knn.score(count_test, y_text_test)\n",
    "actual = max\n",
    "optimo = 1\n",
    "\n",
    "for i in range(2, 100):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(count_train, y_text_train)\n",
    "    actual = knn.score(count_test, y_text_test)\n",
    "    if max < actual:\n",
    "        optimo = i \n",
    "        max = actual\n",
    "\n",
    "print(optimo, max) #0.6778749159381304 con 1 (basura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#acc = 0.7305002689618074(objective ='binary:logistic', colsample_bytree = 0.6, learning_rate = 0.01, max_depth = 35,alpha = 0.5, n_estimators = 140) \n",
    "xg_reg = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.6, learning_rate = 0.005,\n",
    "                max_depth = 35,alpha = 0.5, n_estimators = 140) \n",
    "xg_reg.fit(x_train,y_train) \n",
    "y_pred = xg_reg.predict(x_test)\n",
    "\n",
    "for i in range (0, len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:       \n",
    "        y_pred[i] = 1 \n",
    "    else:  \n",
    "        y_pred[i]=0\n",
    "        \n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(xg_reg, x_train, y_train, cv=kfold)\n",
    "results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 7]\n",
    "xgb.plot_importance(xg_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busqueda de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = dict(learning_rate = np.arange(0.001,0.1,0.005),\n",
    "                              n_estimators = np.arange(15,300,15),\n",
    "                              scale_pos_weight = np.arange(2,6,1),\n",
    "                              max_depth = np.arange(15,40,2),min_child_weight= np.arange(1,10,1),\n",
    "                              gamma = np.arange(0,1,0.1), alpha= np.arange(0.1,1,0.1),\n",
    "                              subsample = np.arange(0,1,0.1), colsample_bytree = np.arange(0.5,0.8,0.05),\n",
    "                              colsample_bylevel = np.arange(0.6,0.91,0.05))\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=20, n_jobs=1)\n",
    "grid_result = grid.fit(train_vectors,y_text_train)\n",
    "params_xgb_tfidf = grid_result.best_params_\n",
    "print(\"Best parameters: \", params_xgb_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBClassifier(**params_xgb_tfidf)\n",
    "xg_reg.fit(train_vectors,y_text_train)\n",
    "preds = xg_reg.predict(test_vectors)\n",
    "for i in range (0, len(preds)):\n",
    "    if preds[i] >= 0.5:       \n",
    "        preds[i] = 1 \n",
    "    else:  \n",
    "        preds[i] = 0\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(preds,y_text_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busqueda de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = dict(learning_rate = np.arange(0.01,0.5,0.02),\n",
    "                              n_estimators = np.arange(15,300,15),\n",
    "                              scale_pos_weight = np.arange(2,6,1),\n",
    "                              max_depth = np.arange(15,40,2),min_child_weight= np.arange(1,10,1),\n",
    "                              gamma = np.arange(0,0.5,0.1), alpha= np.arange(0.1,1,0.1),\n",
    "                              subsample = np.arange(0.6,1,0.1), colsample_bytree = np.arange(0.5,0.8,0.05),\n",
    "                              colsample_bylevel = np.arange(0.6,0.91,0.05))\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=classifier, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=20, n_jobs=1)\n",
    "grid_result = grid.fit(count_train,y_text_train)\n",
    "params_xgb_count = grid_result.best_params_\n",
    "print(\"Best parameters: \", params_xgb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Mejores parametros encontrados\n",
    "# (objective ='reg:logistic', colsample_bytree = 0.45, colsample_bylevel = 0.4, learning_rate = 0.05, max_depth = 25, min_child_weight = 1.1, alpha = 0.5, gamma =  0.4, n_estimators = 210)\n",
    "xg_reg = xgb.XGBRegressor(**params_xgb_count)\n",
    "xg_reg.fit(count_train,y_train) \n",
    "y_pred = xg_reg.predict(count_test)\n",
    "\n",
    "for i in range (0, len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:       \n",
    "        y_pred[i] = 1 \n",
    "    else:  \n",
    "        y_pred[i]=0\n",
    "        \n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:logistic', \n",
    "                colsample_bytree = 0.45, colsample_bylevel = 0.4, learning_rate = 0.05,\n",
    "                max_depth = 25, min_child_weight = 1.1, alpha = 0.5, gamma =  0.4, n_estimators = 210)\n",
    "xg_reg.fit(hash_train, y_train)\n",
    "y_pred = xg_reg.predict(hash_test)\n",
    "\n",
    "for i in range (0, len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:       \n",
    "        y_pred[i] = 1 \n",
    "    else:  \n",
    "        y_pred[i]=0\n",
    "        \n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHEQUEAR ESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics['text'] = tweets_metrics['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(lambda x: x.lower())\n",
    "tweets_metrics['text'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(remove_stopword)\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(stemm)\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics['text'] = test_metrics['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "test_metrics['text'] = test_metrics['text'].apply(lambda x: x.lower())\n",
    "test_metrics['text'] = test_metrics['text'].str.split()\n",
    "test_metrics['text'] = test_metrics['text'].apply(remove_stopword)\n",
    "test_metrics['text'] = test_metrics['text'].apply(stemm)\n",
    "test_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional, Concatenate, Flatten, Attention\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_len = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = Bidirectional(LSTM(256,return_sequences=True))(layer)\n",
    "    layer = Bidirectional(LSTM(4))(layer)\n",
    "    layer = Dense(64)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1)(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "model = RNN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Features - 75% del set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiples parametros - Entrenar con 75% del set\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "X_train = tweets_metrics.iloc[:,4:]\n",
    "X_train[\"text\"] = tweets_metrics[\"text\"]\n",
    "\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train[\"text\"])\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train[\"text\"])\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test[\"text\"])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "features = StandardScaler()\n",
    "X_train_features = features.fit_transform(X_train.iloc[:,:-1])\n",
    "X_test_features = features.transform(X_test.iloc[:,:-1])\n",
    "\n",
    "#Guarda el mejor\n",
    "weight_path=\"Checkpoints/LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = RNN()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit([sequences_matrix,X_train_features],Y_train,batch_size=24,epochs=10,validation_split=0.2,callbacks=callbacks,verbose=1)\n",
    "\n",
    "#Carga el mejor y evalua\n",
    "model.load_weights(weight_path)\n",
    "accr = model.evaluate([test_sequences_matrix,X_test_features],Y_test)\n",
    "\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin Features - 75% del set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sin features\n",
    "#Preparar datos para test sin features\n",
    "X_train = tweets_metrics.text\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "#Comentar para generar submit\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "#Guarda el mejor\n",
    "weight_path=\"Checkpoints/LSTM_No_Features.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = RNN()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(sequences_matrix,Y_train,batch_size=71,epochs=10,validation_split=0.2,callbacks=callbacks,verbose=1)\n",
    "\n",
    "#Carga el mejor y evalua\n",
    "model.load_weights(weight_path)\n",
    "\n",
    "#Comentar para generar submit - Sin features\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.20)\n",
    "\n",
    "model = LogisticRegression(solver=\"newton-cg\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", smooth_idf = False)\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.20)\n",
    "\n",
    "model = LogisticRegression(solver=\"newton-cg\")\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    \n",
    "vectorizer = HashingVectorizer(analyzer=\"word\",n_features=60000)\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tweets_metrics.text\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "#Comentar para generar submit\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\", smooth_idf = False)\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(analyzer=\"word\",n_features=800000)\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAURO WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    batch_size=65)\n",
    "#loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "#print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "#loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "#print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras import layers\n",
    "#embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(x_test, y_test),\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "# Main settings\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "\n",
    "# Train-test split\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1, random_state=1000)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences with zeros\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "#x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Parameter grid for grid search\n",
    "param_grid = dict(num_filters=[32, 128, 144],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [45,65,76,88])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=5, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train, callbacks=[callback])\n",
    "\n",
    "# Evaluate testing set\n",
    "#test_accuracy = grid.score(x_test, y_test)\n",
    "\n",
    "# Save and evaluate results\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "            \n",
    "print(output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
