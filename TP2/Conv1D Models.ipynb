{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos con Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda, Activation, GaussianNoise, GaussianDropout\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm, min_max_norm\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Básico. 1er modelo. -> 1 Conv1d + Maxpool + 1 Densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test[['id','text']]\n",
    "x_test_kagle = test['text'].values\n",
    "\n",
    "x = tweets_metrics['text'].values\n",
    "y = tweets_metrics['target'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    batch_size=65)\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Básico + Word embedding [Glove]. Resultado: 0,80570. -> 2 Conv1d + Maxpool + 1 Densa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tweets_metrics['text'].values\n",
    "y = tweets_metrics['target'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.23, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}, Loss  {:.4f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search sobre CNN básico. -> 1 Conv1d + Maxpool + 1 Densa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5141 - accuracy: 0.7556 - val_loss: 0.4774 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3620 - accuracy: 0.8483 - val_loss: 0.4435 - val_accuracy: 0.8029\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2710 - accuracy: 0.8964 - val_loss: 0.4952 - val_accuracy: 0.7921\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 257us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  29.7s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.6s remaining:    0.0s\n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5259 - accuracy: 0.7588 - val_loss: 0.4598 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3761 - accuracy: 0.8401 - val_loss: 0.4288 - val_accuracy: 0.8029\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2877 - accuracy: 0.8872 - val_loss: 0.4336 - val_accuracy: 0.8011\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 262us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  29.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.4751 - accuracy: 0.7784 - val_loss: 0.4593 - val_accuracy: 0.7688\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3191 - accuracy: 0.8699 - val_loss: 0.4573 - val_accuracy: 0.7849\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.2290 - accuracy: 0.9149 - val_loss: 0.4693 - val_accuracy: 0.7814\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 288us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  31.3s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.5092 - accuracy: 0.7559 - val_loss: 0.4199 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3562 - accuracy: 0.8509 - val_loss: 0.4166 - val_accuracy: 0.8280\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.2612 - accuracy: 0.9022 - val_loss: 0.4358 - val_accuracy: 0.8172\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 0s 267us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=45, total=  31.0s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5251 - accuracy: 0.7377 - val_loss: 0.4503 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3499 - accuracy: 0.8543 - val_loss: 0.4259 - val_accuracy: 0.8065\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2603 - accuracy: 0.9047 - val_loss: 0.4399 - val_accuracy: 0.7921\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 454us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  29.2s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.4991 - accuracy: 0.7654 - val_loss: 0.4462 - val_accuracy: 0.8136\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 12s 2ms/step - loss: 0.3369 - accuracy: 0.8575 - val_loss: 0.4642 - val_accuracy: 0.7832\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  25.0s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5079 - accuracy: 0.7642 - val_loss: 0.4476 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.3421 - accuracy: 0.8555 - val_loss: 0.4287 - val_accuracy: 0.8047\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2454 - accuracy: 0.9073 - val_loss: 0.4657 - val_accuracy: 0.7849\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 445us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  38.1s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.5054 - accuracy: 0.7615 - val_loss: 0.4291 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3408 - accuracy: 0.8613 - val_loss: 0.4206 - val_accuracy: 0.8082\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2457 - accuracy: 0.9119 - val_loss: 0.4993 - val_accuracy: 0.8100\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 424us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=65, total=  34.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.5406 - accuracy: 0.7297 - val_loss: 0.4604 - val_accuracy: 0.7939\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.3779 - accuracy: 0.8352 - val_loss: 0.4234 - val_accuracy: 0.8154\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.2889 - accuracy: 0.8880 - val_loss: 0.4546 - val_accuracy: 0.7939\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 354us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  33.5s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.4939 - accuracy: 0.7648 - val_loss: 0.4612 - val_accuracy: 0.7867\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 11s 2ms/step - loss: 0.3503 - accuracy: 0.8495 - val_loss: 0.4238 - val_accuracy: 0.8118\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 12s 2ms/step - loss: 0.2678 - accuracy: 0.9009 - val_loss: 0.4252 - val_accuracy: 0.8190\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 457us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  35.3s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5261 - accuracy: 0.7635 - val_loss: 0.4986 - val_accuracy: 0.7509\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.3573 - accuracy: 0.8539 - val_loss: 0.4417 - val_accuracy: 0.7975\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2805 - accuracy: 0.8886 - val_loss: 0.4388 - val_accuracy: 0.7993\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.2286 - accuracy: 0.9177 - val_loss: 0.4454 - val_accuracy: 0.7975\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 403us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  47.6s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.5251 - accuracy: 0.7453 - val_loss: 0.4337 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3875 - accuracy: 0.8352 - val_loss: 0.3988 - val_accuracy: 0.8441\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3029 - accuracy: 0.8804 - val_loss: 0.4095 - val_accuracy: 0.8423\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 424us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  35.4s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.5044 - accuracy: 0.7562 - val_loss: 0.4389 - val_accuracy: 0.8100\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3768 - accuracy: 0.8344 - val_loss: 0.4330 - val_accuracy: 0.7993\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.3049 - accuracy: 0.8780 - val_loss: 0.4466 - val_accuracy: 0.7903\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 0s 262us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  29.8s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.4953 - accuracy: 0.7632 - val_loss: 0.4946 - val_accuracy: 0.7652\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.3618 - accuracy: 0.8413 - val_loss: 0.4354 - val_accuracy: 0.8011\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.2861 - accuracy: 0.8890 - val_loss: 0.4320 - val_accuracy: 0.8190\n",
      "Epoch 4/15\n",
      "5017/5017 [==============================] - 7s 1ms/step - loss: 0.2375 - accuracy: 0.9153 - val_loss: 0.4631 - val_accuracy: 0.8082\n",
      "Epoch 00004: early stopping\n",
      "1859/1859 [==============================] - 0s 253us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  30.4s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.5642 - accuracy: 0.6825 - val_loss: 0.4918 - val_accuracy: 0.7527\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3923 - accuracy: 0.8330 - val_loss: 0.4887 - val_accuracy: 0.7616\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3194 - accuracy: 0.8677 - val_loss: 0.4350 - val_accuracy: 0.8029\n",
      "Epoch 4/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.2526 - accuracy: 0.9083 - val_loss: 0.4424 - val_accuracy: 0.8029\n",
      "Epoch 00004: early stopping\n",
      "1858/1858 [==============================] - 1s 282us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  29.6s\n",
      "[CV] vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.5183 - accuracy: 0.7475 - val_loss: 0.4363 - val_accuracy: 0.8208\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 7s 1ms/step - loss: 0.3567 - accuracy: 0.8551 - val_loss: 0.4273 - val_accuracy: 0.8172\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 8s 2ms/step - loss: 0.2758 - accuracy: 0.9033 - val_loss: 0.4326 - val_accuracy: 0.8172\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 309us/step\n",
      "[CV]  vocab_size=22811, num_filters=128, maxlen=140, kernel_size=5, embedding_dim=100, batch_size=88, total=  22.6s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.5139 - accuracy: 0.7441 - val_loss: 0.4678 - val_accuracy: 0.7778\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 8s 2ms/step - loss: 0.3837 - accuracy: 0.8350 - val_loss: 0.4287 - val_accuracy: 0.8136\n",
      "Epoch 3/15\n",
      "5017/5017 [==============================] - 9s 2ms/step - loss: 0.2938 - accuracy: 0.8850 - val_loss: 0.4342 - val_accuracy: 0.8065\n",
      "Epoch 00003: early stopping\n",
      "1859/1859 [==============================] - 1s 349us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  28.0s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5017 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.4898 - accuracy: 0.7688 - val_loss: 0.4453 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "5017/5017 [==============================] - 10s 2ms/step - loss: 0.3395 - accuracy: 0.8565 - val_loss: 0.5039 - val_accuracy: 0.7545\n",
      "Epoch 00002: early stopping\n",
      "1859/1859 [==============================] - 1s 328us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  21.6s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 12s 2ms/step - loss: 0.4900 - accuracy: 0.7654 - val_loss: 0.4399 - val_accuracy: 0.7993\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.3345 - accuracy: 0.8587 - val_loss: 0.4420 - val_accuracy: 0.7957\n",
      "Epoch 00002: early stopping\n",
      "1858/1858 [==============================] - 1s 370us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  23.5s\n",
      "[CV] vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88 \n",
      "Train on 5018 samples, validate on 558 samples\n",
      "Epoch 1/15\n",
      "5018/5018 [==============================] - 10s 2ms/step - loss: 0.5100 - accuracy: 0.7505 - val_loss: 0.4274 - val_accuracy: 0.8172\n",
      "Epoch 2/15\n",
      "5018/5018 [==============================] - 11s 2ms/step - loss: 0.3813 - accuracy: 0.8400 - val_loss: 0.4016 - val_accuracy: 0.8405\n",
      "Epoch 3/15\n",
      "5018/5018 [==============================] - 9s 2ms/step - loss: 0.2900 - accuracy: 0.8870 - val_loss: 0.4243 - val_accuracy: 0.8082\n",
      "Epoch 00003: early stopping\n",
      "1858/1858 [==============================] - 1s 365us/step\n",
      "[CV]  vocab_size=22811, num_filters=144, maxlen=140, kernel_size=7, embedding_dim=100, batch_size=88, total=  31.3s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 10.3min finished\n",
      "Train on 6690 samples, validate on 744 samples\n",
      "Epoch 1/15\n",
      "6690/6690 [==============================] - 14s 2ms/step - loss: 0.4663 - accuracy: 0.7834 - val_loss: 0.4185 - val_accuracy: 0.8145\n",
      "Epoch 2/15\n",
      "6690/6690 [==============================] - 14s 2ms/step - loss: 0.3259 - accuracy: 0.8646 - val_loss: 0.4480 - val_accuracy: 0.7970\n",
      "Epoch 00002: early stopping\n",
      "Best Accuracy : 0.7942\n",
      "{'vocab_size': 22811, 'num_filters': 128, 'maxlen': 140, 'kernel_size': 5, 'embedding_dim': 100, 'batch_size': 45}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "epochs = 15\n",
    "embedding_dim = 100\n",
    "maxlen = 140\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)\n",
    "\n",
    "param_grid = dict(num_filters=[32, 128, 144],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen],\n",
    "                      batch_size = [45,65,76,88])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, validation_split=0.1,\n",
    "                            verbose=1)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=5, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train, callbacks=[callback])\n",
    "\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior ejecución del modelo a partir de resultados de la Random Search. Resultado: 0,81274."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tweets_metrics['text'].values\n",
    "y = tweets_metrics['target'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 123)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 140\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_156 (Embedding)    (None, 140, 100)          2281100   \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 134, 128)          89728     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_156 (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,372,129\n",
      "Trainable params: 2,372,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
    "model.add(layers.Conv1D(128, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=88,\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}, Loss  {:.4f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior ejecución del modelo a partir de resultados de la Grid Search. Resultado: 0,81703. -> 1 Conv1d + 2 Densas. Con Glove y features de texto concatenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 140\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "embedding_dim_1 = 100\n",
    "embedding_matrix_1 = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d():\n",
    "    embedding = Embedding(vocab_size, embedding_dim_1, input_length=maxlen, weights=[embedding_matrix_1], trainable=True)\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "    emb = embedding(x_train_input)\n",
    "    \n",
    "    conv_out = Conv1D(128, 2, activation='relu')(emb)\n",
    "    max_pool = GlobalMaxPooling1D()(conv_out)\n",
    "\n",
    "    conc = Concatenate()([max_pool, x_train_features_input])\n",
    "    \n",
    "    dense1 = Dense(100, activation='relu')(conc)\n",
    "    dense2 = Dense(50, activation='relu')(dense1)\n",
    "    dense3 = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = create_conv1d()\n",
    "\n",
    "history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Channel CNN + Multi-word embedding. Se utilizan 3 pre-trained embeddings de Glove. -> 2 Conv1d en paralelo para cada word embedding + posterior concatenación (total 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = StandardScaler()\n",
    "x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 50\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_1 = 100\n",
    "embedding_matrix_1 = create_embedding_matrix('Embeddings/glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim_1)\n",
    "\n",
    "embedding_dim_2 = 300\n",
    "embedding_matrix_2 = create_embedding_matrix_840('Embeddings/glove.840B.300d.txt',tokenizer.word_index, embedding_dim_2)\n",
    "\n",
    "embedding_dim_3 = 200\n",
    "embedding_matrix_3 = create_embedding_matrix('Embeddings/glove.6B.200d.txt',tokenizer.word_index, embedding_dim_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d():\n",
    "        \n",
    "    embedding = Embedding(vocab_size, embedding_dim_1, input_length=maxlen, weights=[embedding_matrix_1], trainable=False)\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim_2, input_length=maxlen, weights=[embedding_matrix_2], trainable=True)\n",
    "    embedding3 = Embedding(vocab_size, embedding_dim_3, input_length=maxlen, weights=[embedding_matrix_3], trainable=False)\n",
    "\n",
    "    x_train_input = Input(shape = (maxlen,), name = 'x_train_input')\n",
    "    x_train_features_input = Input(shape = (15, ), name = 'x_features_train')\n",
    "\n",
    "    emb = embedding(x_train_input)\n",
    "    emb2 = embedding2(x_train_input)\n",
    "    emb3 = embedding3(x_train_input)\n",
    "\n",
    "    #Emb 100\n",
    "    conv_out1_1 = Conv1D(128, 2, activation='relu')(emb)\n",
    "    activation1_1 = Activation('relu')(conv_out1_1)\n",
    "    max_pool1_1 = GlobalMaxPooling1D()(activation1_1)\n",
    "    conv_out1_2 = Conv1D(128, 3, activation='relu')(emb)\n",
    "    activation1_2 = Activation('relu')(conv_out1_2)\n",
    "    max_pool1_2 = GlobalMaxPooling1D()(activation1_2)\n",
    "\n",
    "    #Emb 200\n",
    "    conv_out2_1 = Conv1D(128, 2, activation='relu', kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(emb2)\n",
    "    activation2_1 = Activation('relu')(conv_out2_1)\n",
    "    max_pool2_1 = GlobalMaxPooling1D()(activation2_1)\n",
    "    conv_out2_2 = Conv1D(128, 3, activation='relu', kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(activation2_1)\n",
    "    activation2_2 = Activation('relu')(conv_out2_2)\n",
    "    max_pool2_2 = GlobalMaxPooling1D()(activation2_2)\n",
    "\n",
    "    #Emb 300\n",
    "    conv_out3_1 = Conv1D(128, 2, activation='relu')(emb3)\n",
    "    activation3_1 = Activation('relu')(conv_out3_1)\n",
    "    max_pool3_1 = GlobalMaxPooling1D()(activation3_1)\n",
    "    conv_out3_2 = Conv1D(128, 3, activation='relu')(emb3)\n",
    "    activation3_2 = Activation('relu')(conv_out3_2)\n",
    "    max_pool3_2 = GlobalMaxPooling1D()(activation3_2)\n",
    "\n",
    "    conc = Concatenate()([max_pool2_2,  x_train_features_input])\n",
    "    \n",
    "    dense1 = Dense(100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(conc)\n",
    "    noise1 = GaussianNoise(0.1)(dense1)\n",
    "    dense2 = Dense(10, activation='relu')(noise1)\n",
    "    dense3 = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs = [x_train_input , x_train_features_input], outputs = dense3)\n",
    "    optimizer = Adam() #default\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 21s 3ms/step - loss: 1.0982 - accuracy: 0.7818\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.5130 - accuracy: 0.8608\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.3117 - accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "model = create_conv1d()\n",
    "\n",
    "history = model.fit([x_train,x_train_features], y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_kagle = test[['id','text']]\n",
    "\n",
    "x_test_kagle = test['text'].values\n",
    "x_test_kagle = tokenizer.texts_to_sequences(x_test_kagle)\n",
    "x_test_kagle = pad_sequences(x_test_kagle, padding='post', maxlen=maxlen)\n",
    "x_test_features = features.transform(basic_features_test)\n",
    "\n",
    "submit_df = pd.DataFrame()\n",
    "submit_df['id'] = test_kagle['id']\n",
    "submit_df['prob'] = model.predict([x_test_kagle,x_test_features])\n",
    "submit_df['target'] = submit_df['prob'].apply(lambda x: 0 if x < .5 else 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mausa': virtualenv)",
   "language": "python",
   "name": "python37664bitmausavirtualenv161e16f0fd2c481895212b5856f17cf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
