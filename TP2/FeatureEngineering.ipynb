{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "#import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "tweets['keyword'] = tweets.keyword.str.replace('%20',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7434 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7434 non-null   int64 \n",
      " 1   keyword   7378 non-null   object\n",
      " 2   location  4982 non-null   object\n",
      " 3   text      7434 non-null   object\n",
      " 4   target    7434 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 348.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3243\n",
       "True       20\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics.to_csv('train_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test[['id','text']]\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text'].str.split()\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test_metrics['length'] = test_metrics['text'].apply(lambda x: len(x))\n",
    "test_metrics['avg_word_length'] = test_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test_metrics['amount_of_words'] = test_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = test_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test_metrics['sentiment'] = test_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test_metrics['stopwords_count'] = test_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test_metrics['punctuation_count'] = test_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test_metrics['longest_word_length_without_stopwords'] = test_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test_metrics['stopword_word_ratio'] = test_metrics['stopwords_count'] / test_metrics['amount_of_words']\n",
    "\n",
    "test_metrics['adjectives_count'] = test_metrics['text'].apply(get_adjectives)\n",
    "test_metrics['nouns_count'] = test_metrics['text'].apply(get_nouns)\n",
    "test_metrics['verbs_count'] = test_metrics['text'].apply(get_verbs)\n",
    "test_metrics['adverbs_count'] = test_metrics['text'].apply(get_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "#### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].fillna('NULL')\n",
    "test['keyword'] = test['keyword'].fillna('NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NULL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NULL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NULL</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NULL</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword  id\n",
       "0    NULL   1\n",
       "1    NULL   4\n",
       "2    NULL   5\n",
       "3    NULL   6\n",
       "4    NULL   7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = tweets[['keyword', 'id']]\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_tokens = keywords.keyword.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NULL'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Parameters\n",
    "# sg ({0, 1}, optional) - Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "keyword_vectors = Word2Vec([keyword_tokens], min_count=1, size= 100, workers=3, window =3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.9782532e-03, -4.6778829e-03,  2.6636261e-03,  1.3892590e-03,\n",
       "       -4.1829501e-04, -2.2293918e-03, -4.5149596e-03,  3.5410111e-03,\n",
       "       -1.2479150e-03,  1.1896417e-03,  4.9190470e-03, -2.1441414e-03,\n",
       "        2.3400637e-03,  2.6803168e-03, -1.8570827e-03, -2.0209479e-03,\n",
       "       -4.4037956e-03,  1.8780368e-03, -3.9049496e-03, -4.4517661e-03,\n",
       "       -6.0586346e-04,  4.4179936e-03,  1.9494499e-03, -1.3178665e-03,\n",
       "       -2.5463670e-03,  3.7509988e-03, -9.1646833e-04, -1.0098880e-03,\n",
       "        2.1787037e-03, -1.8912348e-03, -3.4733759e-03, -3.4088921e-04,\n",
       "       -3.4694674e-03, -1.8341526e-03, -4.8559005e-04, -2.9973371e-03,\n",
       "       -2.6874964e-03, -9.6715416e-04, -3.2191214e-03,  4.3508359e-03,\n",
       "        6.1339342e-05,  3.0415037e-03,  2.6784365e-03,  1.7985675e-03,\n",
       "        1.7651119e-03, -3.9943969e-03, -2.8382847e-03,  1.4784534e-03,\n",
       "       -4.0552719e-03,  4.1793748e-03,  2.5931003e-03, -2.2637420e-03,\n",
       "        3.3299834e-03,  8.0845342e-04, -6.7411043e-04,  1.7605064e-03,\n",
       "        3.1922972e-03, -3.8574983e-03, -4.2025940e-03, -1.2603013e-03,\n",
       "       -1.9923225e-03,  1.0042273e-03,  2.9899436e-03, -1.1144179e-03,\n",
       "        6.2272092e-04, -3.8566182e-03,  2.9291394e-03, -4.5166798e-03,\n",
       "        3.1535663e-03,  2.6525955e-03, -1.8848410e-03,  4.3695755e-03,\n",
       "       -1.2336837e-03, -4.5906641e-03, -3.1950774e-03, -1.3918199e-03,\n",
       "        1.4489682e-03, -2.7484975e-03,  1.3534029e-05,  1.9987142e-03,\n",
       "        2.9653714e-03, -4.7487989e-03,  3.4782437e-03, -2.7957498e-03,\n",
       "       -6.6604948e-04,  1.5292383e-03, -1.1305596e-03,  1.7471197e-03,\n",
       "       -3.1916017e-03,  2.5888158e-03,  2.1174289e-03,  3.1541204e-03,\n",
       "        4.2204787e-03,  4.9466076e-03, -4.8446655e-04,  3.8421124e-03,\n",
       "        3.3216993e-03,  2.0549078e-03,  3.1547276e-03,  2.9219596e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "keyword_vectors['ablaze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "to_vector_matrix = {}\n",
    "\n",
    "for k in keyword_tokens:\n",
    "    to_vector_matrix[k] = keyword_vectors[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>-0.001299</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002605</td>\n",
       "      <td>-0.004927</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>-0.004160</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>-0.002978</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>-0.004515</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>-0.002235</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane accident</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>-0.003184</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>-0.003797</td>\n",
       "      <td>-0.001352</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.001173</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>-0.000939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>wounded</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>-0.003270</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.004593</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>-0.000958</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.001115</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.003310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>wounds</td>\n",
       "      <td>-0.001814</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>-0.001831</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>-0.001981</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.003419</td>\n",
       "      <td>-0.004136</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>-0.001755</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.004597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>wreck</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.004270</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>-0.001561</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>-0.003265</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>-0.001886</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.003812</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>-0.000785</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>wreckage</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.003009</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>-0.003924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.004607</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>-0.004637</td>\n",
       "      <td>0.004220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>-0.002689</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>-0.003890</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000345</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003891</td>\n",
       "      <td>-0.003487</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>-0.002276</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>0.003217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index         0         1         2         3         4  \\\n",
       "0                 NULL -0.002684 -0.001899 -0.000798 -0.001893  0.003133   \n",
       "1               ablaze -0.002978 -0.004678  0.002664  0.001389 -0.000418   \n",
       "2             accident -0.003384  0.002203  0.000089  0.003893  0.001827   \n",
       "3           aftershock  0.001398  0.003535 -0.002235 -0.003819 -0.004369   \n",
       "4    airplane accident -0.002153 -0.003184 -0.000135 -0.000386 -0.003797   \n",
       "..                 ...       ...       ...       ...       ...       ...   \n",
       "217            wounded  0.001796 -0.003270  0.004549  0.004226  0.002533   \n",
       "218             wounds -0.001814  0.003487  0.003526 -0.001831 -0.000307   \n",
       "219              wreck  0.001275  0.001559 -0.004270  0.003340 -0.001561   \n",
       "220           wreckage  0.004161  0.000882  0.003419  0.000611  0.000960   \n",
       "221            wrecked -0.002689  0.004489 -0.003890  0.002912  0.000473   \n",
       "\n",
       "            5         6         7         8  ...        90        91  \\\n",
       "0    0.002236 -0.001299 -0.003281 -0.004414  ... -0.002605 -0.004927   \n",
       "1   -0.002229 -0.004515  0.003541 -0.001248  ...  0.002117  0.003154   \n",
       "2    0.001415  0.002242 -0.003304  0.000639  ...  0.002065  0.000828   \n",
       "3   -0.004255 -0.001596  0.000770  0.003828  ...  0.002328 -0.002230   \n",
       "4   -0.001352  0.002524 -0.001643  0.001755  ... -0.000326 -0.001173   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "217  0.000455  0.004593  0.002722  0.003202  ... -0.000337 -0.004437   \n",
       "218  0.002191  0.004440  0.002222 -0.002227  ...  0.004556 -0.001981   \n",
       "219  0.002126 -0.003265 -0.002016 -0.000342  ...  0.002935  0.003843   \n",
       "220 -0.003009 -0.003214  0.003773 -0.003924  ... -0.001317 -0.000323   \n",
       "221 -0.000430 -0.000135 -0.000345  0.000126  ... -0.003891 -0.003487   \n",
       "\n",
       "           92        93        94        95        96        97        98  \\\n",
       "0    0.001684  0.004702 -0.004160  0.003781 -0.003558  0.003921  0.000945   \n",
       "1    0.004220  0.004947 -0.000484  0.003842  0.003322  0.002055  0.003155   \n",
       "2    0.000448 -0.004822 -0.004947  0.003784  0.003708 -0.000240  0.004170   \n",
       "3    0.000262  0.001067  0.002255  0.004911 -0.002622  0.001655 -0.000919   \n",
       "4    0.004366  0.001570  0.001540  0.001214  0.002249  0.004110  0.002640   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "217 -0.000958  0.004536  0.001090  0.000493 -0.001115  0.000474  0.000006   \n",
       "218  0.000711 -0.003419 -0.004136  0.004411 -0.001755 -0.002368 -0.003373   \n",
       "219 -0.001886  0.003956  0.001262 -0.003812  0.001750 -0.000785 -0.000671   \n",
       "220  0.001569 -0.000505 -0.004607  0.003270  0.003372  0.002703 -0.004637   \n",
       "221 -0.000359  0.003178  0.001096 -0.004598 -0.002276 -0.000871 -0.003968   \n",
       "\n",
       "           99  \n",
       "0   -0.002725  \n",
       "1    0.002922  \n",
       "2    0.004662  \n",
       "3   -0.000489  \n",
       "4   -0.000939  \n",
       "..        ...  \n",
       "217 -0.003310  \n",
       "218 -0.004597  \n",
       "219 -0.004001  \n",
       "220  0.004220  \n",
       "221  0.003217  \n",
       "\n",
       "[222 rows x 101 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_w2v = pd.DataFrame.from_dict(to_vector_matrix).T.reset_index()\n",
    "keyword_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = []\n",
    "aux.append('keyword')\n",
    "for i in range (0, 100):\n",
    "    name = 'v' + str(i)\n",
    "    aux.append(name)\n",
    "len(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.columns = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>-0.001299</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002605</td>\n",
       "      <td>-0.004927</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>-0.004160</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>-0.002978</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>-0.004515</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>-0.003304</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>-0.002235</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane accident</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>-0.003184</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>-0.003797</td>\n",
       "      <td>-0.001352</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.001173</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>-0.000939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword        v0        v1        v2        v3        v4  \\\n",
       "0               NULL -0.002684 -0.001899 -0.000798 -0.001893  0.003133   \n",
       "1             ablaze -0.002978 -0.004678  0.002664  0.001389 -0.000418   \n",
       "2           accident -0.003384  0.002203  0.000089  0.003893  0.001827   \n",
       "3         aftershock  0.001398  0.003535 -0.002235 -0.003819 -0.004369   \n",
       "4  airplane accident -0.002153 -0.003184 -0.000135 -0.000386 -0.003797   \n",
       "\n",
       "         v5        v6        v7        v8  ...       v90       v91       v92  \\\n",
       "0  0.002236 -0.001299 -0.003281 -0.004414  ... -0.002605 -0.004927  0.001684   \n",
       "1 -0.002229 -0.004515  0.003541 -0.001248  ...  0.002117  0.003154  0.004220   \n",
       "2  0.001415  0.002242 -0.003304  0.000639  ...  0.002065  0.000828  0.000448   \n",
       "3 -0.004255 -0.001596  0.000770  0.003828  ...  0.002328 -0.002230  0.000262   \n",
       "4 -0.001352  0.002524 -0.001643  0.001755  ... -0.000326 -0.001173  0.004366   \n",
       "\n",
       "        v93       v94       v95       v96       v97       v98       v99  \n",
       "0  0.004702 -0.004160  0.003781 -0.003558  0.003921  0.000945 -0.002725  \n",
       "1  0.004947 -0.000484  0.003842  0.003322  0.002055  0.003155  0.002922  \n",
       "2 -0.004822 -0.004947  0.003784  0.003708 -0.000240  0.004170  0.004662  \n",
       "3  0.001067  0.002255  0.004911 -0.002622  0.001655 -0.000919 -0.000489  \n",
       "4  0.001570  0.001540  0.001214  0.002249  0.004110  0.002640 -0.000939  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.to_csv('keyword_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations (proximamente)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
