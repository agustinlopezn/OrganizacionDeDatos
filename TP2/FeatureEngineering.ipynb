{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/matiascano/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "#import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7434 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7434 non-null   int64 \n",
      " 1   keyword   7378 non-null   object\n",
      " 2   location  4982 non-null   object\n",
      " 3   text      7434 non-null   object\n",
      " 4   target    7434 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 348.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3243\n",
       "True       20\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics.to_csv('train_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_metrics = test[['id','text']]\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text'].str.split()\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test_metrics['length'] = test_metrics['text'].apply(lambda x: len(x))\n",
    "test_metrics['avg_word_length'] = test_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test_metrics['amount_of_words'] = test_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = test_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test_metrics['sentiment'] = test_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test_metrics['stopwords_count'] = test_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test_metrics['punctuation_count'] = test_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test_metrics['longest_word_length_without_stopwords'] = test_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test_metrics['stopword_word_ratio'] = test_metrics['stopwords_count'] / test_metrics['amount_of_words']\n",
    "\n",
    "test_metrics['adjectives_count'] = test_metrics['text'].apply(get_adjectives)\n",
    "test_metrics['nouns_count'] = test_metrics['text'].apply(get_nouns)\n",
    "test_metrics['verbs_count'] = test_metrics['text'].apply(get_verbs)\n",
    "test_metrics['adverbs_count'] = test_metrics['text'].apply(get_adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "#### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].fillna('NULL')\n",
    "test['keyword'] = test['keyword'].fillna('NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NULL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NULL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NULL</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NULL</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword  id\n",
       "0    NULL   1\n",
       "1    NULL   4\n",
       "2    NULL   5\n",
       "3    NULL   6\n",
       "4    NULL   7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = tweets[['keyword', 'id']]\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_tokens = keywords.keyword.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NULL'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Parameters\n",
    "# sg ({0, 1}, optional) - Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "keyword_vectors = Word2Vec([keyword_tokens], min_count=1, size= 100, workers=3, window =3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.9691348e-03, -4.6726349e-03,  2.6698408e-03,  1.3936218e-03,\n",
       "       -4.0975353e-04, -2.2376438e-03, -4.5118001e-03,  3.5424023e-03,\n",
       "       -1.2515603e-03,  1.1874196e-03,  4.9171234e-03, -2.1516369e-03,\n",
       "        2.3249865e-03,  2.6831164e-03, -1.8474258e-03, -2.0176845e-03,\n",
       "       -4.4094790e-03,  1.8744356e-03, -3.8899099e-03, -4.4326098e-03,\n",
       "       -5.9894077e-04,  4.4174064e-03,  1.9389419e-03, -1.3160927e-03,\n",
       "       -2.5508045e-03,  3.7587201e-03, -9.0516126e-04, -1.0121947e-03,\n",
       "        2.1766676e-03, -1.9006715e-03, -3.4770861e-03, -3.4123656e-04,\n",
       "       -3.4737966e-03, -1.8389890e-03, -4.8271392e-04, -2.9958151e-03,\n",
       "       -2.6793319e-03, -9.7406458e-04, -3.2200413e-03,  4.3561123e-03,\n",
       "        5.7204859e-05,  3.0434027e-03,  2.6703451e-03,  1.8030163e-03,\n",
       "        1.7714408e-03, -4.0001585e-03, -2.8440841e-03,  1.4819726e-03,\n",
       "       -4.0566339e-03,  4.1848528e-03,  2.5892959e-03, -2.2633339e-03,\n",
       "        3.3276931e-03,  8.0631132e-04, -6.7944662e-04,  1.7627685e-03,\n",
       "        3.1954318e-03, -3.8562063e-03, -4.1983672e-03, -1.2377854e-03,\n",
       "       -1.9822204e-03,  9.9925790e-04,  2.9878248e-03, -1.1212400e-03,\n",
       "        6.1446469e-04, -3.8561476e-03,  2.9307648e-03, -4.5094742e-03,\n",
       "        3.1588743e-03,  2.6611700e-03, -1.8866744e-03,  4.3747891e-03,\n",
       "       -1.2364265e-03, -4.5926985e-03, -3.1962153e-03, -1.3932163e-03,\n",
       "        1.4525249e-03, -2.7508987e-03,  1.8023231e-05,  1.9929016e-03,\n",
       "        2.9624410e-03, -4.7479291e-03,  3.4803790e-03, -2.7995219e-03,\n",
       "       -6.6481496e-04,  1.5372862e-03, -1.1367077e-03,  1.7433799e-03,\n",
       "       -3.1760673e-03,  2.5934267e-03,  2.1281380e-03,  3.1617905e-03,\n",
       "        4.2154263e-03,  4.9474686e-03, -4.7245133e-04,  3.8406749e-03,\n",
       "        3.3103453e-03,  2.0579703e-03,  3.1448565e-03,  2.9196921e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "keyword_vectors['ablaze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "to_vector_matrix = {}\n",
    "\n",
    "for k in keyword_tokens:\n",
    "    to_vector_matrix[k] = keyword_vectors[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL</td>\n",
       "      <td>-0.002675</td>\n",
       "      <td>-0.001897</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.001888</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>-0.003276</td>\n",
       "      <td>-0.004409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002599</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>-0.004160</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>-0.002726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>-0.002969</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.001252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>-0.003294</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.004813</td>\n",
       "      <td>-0.004949</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>-0.002217</td>\n",
       "      <td>-0.003810</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>-0.002628</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>-0.000927</td>\n",
       "      <td>-0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>-0.002005</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>-0.004914</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>wounded</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.004431</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>wounds</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>-0.001836</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>-0.002202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>-0.003406</td>\n",
       "      <td>-0.004135</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>-0.001768</td>\n",
       "      <td>-0.002348</td>\n",
       "      <td>-0.003370</td>\n",
       "      <td>-0.004599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>wreck</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>-0.004231</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>-0.003257</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>-0.003790</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>-0.000663</td>\n",
       "      <td>-0.003983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>wreckage</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>-0.003203</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>-0.003912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>-0.004597</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>0.004226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>-0.002679</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003893</td>\n",
       "      <td>-0.003481</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>-0.002282</td>\n",
       "      <td>-0.000848</td>\n",
       "      <td>-0.003963</td>\n",
       "      <td>0.003217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index         0         1         2         3         4  \\\n",
       "0                   NULL -0.002675 -0.001897 -0.000787 -0.001888  0.003148   \n",
       "1                 ablaze -0.002969 -0.004673  0.002670  0.001394 -0.000410   \n",
       "2               accident -0.003366  0.002198  0.000095  0.003895  0.001833   \n",
       "3             aftershock  0.001413  0.003539 -0.002217 -0.003810 -0.004349   \n",
       "4    airplane%20accident  0.000383  0.000226  0.004611  0.003410  0.003898   \n",
       "..                   ...       ...       ...       ...       ...       ...   \n",
       "217              wounded  0.001823 -0.003264  0.004590  0.004210  0.002555   \n",
       "218               wounds -0.001801  0.003490  0.003534 -0.001836 -0.000305   \n",
       "219                wreck  0.001305  0.001566 -0.004231  0.003320 -0.001554   \n",
       "220             wreckage  0.004178  0.000893  0.003457  0.000586  0.000962   \n",
       "221              wrecked -0.002679  0.004492 -0.003883  0.002910  0.000485   \n",
       "\n",
       "            5         6         7         8  ...        90        91  \\\n",
       "0    0.002227 -0.001302 -0.003276 -0.004409  ... -0.002599 -0.004922   \n",
       "1   -0.002238 -0.004512  0.003542 -0.001252  ...  0.002128  0.003162   \n",
       "2    0.001408  0.002246 -0.003294  0.000644  ...  0.002074  0.000825   \n",
       "3   -0.004274 -0.001596  0.000768  0.003827  ...  0.002345 -0.002228   \n",
       "4   -0.004922  0.000915  0.001955  0.001060  ...  0.003806  0.001122   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "217  0.000461  0.004600  0.002716  0.003216  ... -0.000338 -0.004431   \n",
       "218  0.002193  0.004454  0.002216 -0.002202  ...  0.004561 -0.001985   \n",
       "219  0.002129 -0.003257 -0.002014 -0.000336  ...  0.002939  0.003866   \n",
       "220 -0.003000 -0.003203  0.003771 -0.003912  ... -0.001317 -0.000308   \n",
       "221 -0.000432 -0.000131 -0.000343  0.000127  ... -0.003893 -0.003481   \n",
       "\n",
       "           92        93        94        95        96        97        98  \\\n",
       "0    0.001684  0.004708 -0.004160  0.003790 -0.003561  0.003940  0.000939   \n",
       "1    0.004215  0.004947 -0.000472  0.003841  0.003310  0.002058  0.003145   \n",
       "2    0.000442 -0.004813 -0.004949  0.003789  0.003702 -0.000228  0.004167   \n",
       "3    0.000241  0.001078  0.002260  0.004903 -0.002628  0.001688 -0.000927   \n",
       "4    0.004927 -0.002005  0.002223  0.004315 -0.000272  0.001431 -0.004914   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "217 -0.000998  0.004568  0.001108  0.000484 -0.001134  0.000524  0.000014   \n",
       "218  0.000687 -0.003406 -0.004135  0.004414 -0.001768 -0.002348 -0.003370   \n",
       "219 -0.001899  0.003966  0.001296 -0.003790  0.001715 -0.000759 -0.000663   \n",
       "220  0.001555 -0.000482 -0.004597  0.003271  0.003350  0.002725 -0.004631   \n",
       "221 -0.000374  0.003192  0.001094 -0.004600 -0.002282 -0.000848 -0.003963   \n",
       "\n",
       "           99  \n",
       "0   -0.002726  \n",
       "1    0.002920  \n",
       "2    0.004664  \n",
       "3   -0.000476  \n",
       "4    0.000174  \n",
       "..        ...  \n",
       "217 -0.003300  \n",
       "218 -0.004599  \n",
       "219 -0.003983  \n",
       "220  0.004226  \n",
       "221  0.003217  \n",
       "\n",
       "[222 rows x 101 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_w2v = pd.DataFrame.from_dict(to_vector_matrix).T.reset_index()\n",
    "keyword_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = []\n",
    "aux.append('keyword')\n",
    "for i in range (0, 100):\n",
    "    name = 'v' + str(i)\n",
    "    aux.append(name)\n",
    "len(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.columns = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NULL</td>\n",
       "      <td>-0.002675</td>\n",
       "      <td>-0.001897</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.001888</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>-0.003276</td>\n",
       "      <td>-0.004409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002599</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>-0.004160</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>-0.002726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>-0.002969</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.001252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accident</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>-0.003294</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.004813</td>\n",
       "      <td>-0.004949</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>-0.002217</td>\n",
       "      <td>-0.003810</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>-0.002628</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>-0.000927</td>\n",
       "      <td>-0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>-0.002005</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>-0.004914</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword        v0        v1        v2        v3        v4  \\\n",
       "0                 NULL -0.002675 -0.001897 -0.000787 -0.001888  0.003148   \n",
       "1               ablaze -0.002969 -0.004673  0.002670  0.001394 -0.000410   \n",
       "2             accident -0.003366  0.002198  0.000095  0.003895  0.001833   \n",
       "3           aftershock  0.001413  0.003539 -0.002217 -0.003810 -0.004349   \n",
       "4  airplane%20accident  0.000383  0.000226  0.004611  0.003410  0.003898   \n",
       "\n",
       "         v5        v6        v7        v8  ...       v90       v91       v92  \\\n",
       "0  0.002227 -0.001302 -0.003276 -0.004409  ... -0.002599 -0.004922  0.001684   \n",
       "1 -0.002238 -0.004512  0.003542 -0.001252  ...  0.002128  0.003162  0.004215   \n",
       "2  0.001408  0.002246 -0.003294  0.000644  ...  0.002074  0.000825  0.000442   \n",
       "3 -0.004274 -0.001596  0.000768  0.003827  ...  0.002345 -0.002228  0.000241   \n",
       "4 -0.004922  0.000915  0.001955  0.001060  ...  0.003806  0.001122  0.004927   \n",
       "\n",
       "        v93       v94       v95       v96       v97       v98       v99  \n",
       "0  0.004708 -0.004160  0.003790 -0.003561  0.003940  0.000939 -0.002726  \n",
       "1  0.004947 -0.000472  0.003841  0.003310  0.002058  0.003145  0.002920  \n",
       "2 -0.004813 -0.004949  0.003789  0.003702 -0.000228  0.004167  0.004664  \n",
       "3  0.001078  0.002260  0.004903 -0.002628  0.001688 -0.000927 -0.000476  \n",
       "4 -0.002005  0.002223  0.004315 -0.000272  0.001431 -0.004914  0.000174  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w2v.to_csv('keyword_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations (proximamente)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
