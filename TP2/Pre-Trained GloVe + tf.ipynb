{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('train.csv') \n",
    "tests = pd.read_csv('test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7434 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7434 non-null   int64 \n",
      " 1   keyword   7378 non-null   object\n",
      " 2   location  4982 non-null   object\n",
      " 3   text      7434 non-null   object\n",
      " 4   target    7434 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 348.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22586\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(tweets['text'])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 1,\n",
       " 'co': 2,\n",
       " 'http': 3,\n",
       " 'the': 4,\n",
       " 'a': 5,\n",
       " 'in': 6,\n",
       " 'to': 7,\n",
       " 'of': 8,\n",
       " 'and': 9,\n",
       " 'i': 10,\n",
       " 'is': 11,\n",
       " 'for': 12,\n",
       " 'on': 13,\n",
       " 'you': 14,\n",
       " 'my': 15,\n",
       " 'that': 16,\n",
       " 'it': 17,\n",
       " 'with': 18,\n",
       " 'at': 19,\n",
       " 'by': 20,\n",
       " 'this': 21,\n",
       " 'https': 22,\n",
       " 'from': 23,\n",
       " 'be': 24,\n",
       " 'are': 25,\n",
       " 'was': 26,\n",
       " 'have': 27,\n",
       " 'like': 28,\n",
       " 'amp': 29,\n",
       " 'me': 30,\n",
       " 'as': 31,\n",
       " 'up': 32,\n",
       " 'but': 33,\n",
       " 'just': 34,\n",
       " 'so': 35,\n",
       " 'not': 36,\n",
       " 'your': 37,\n",
       " 'out': 38,\n",
       " 'no': 39,\n",
       " 'all': 40,\n",
       " 'will': 41,\n",
       " 'after': 42,\n",
       " 'an': 43,\n",
       " 'has': 44,\n",
       " 'when': 45,\n",
       " 'fire': 46,\n",
       " \"i'm\": 47,\n",
       " 'get': 48,\n",
       " 'now': 49,\n",
       " 'we': 50,\n",
       " 'new': 51,\n",
       " 'if': 52,\n",
       " 'more': 53,\n",
       " '2': 54,\n",
       " 'via': 55,\n",
       " 'about': 56,\n",
       " 'or': 57,\n",
       " 'news': 58,\n",
       " 'what': 59,\n",
       " 'they': 60,\n",
       " 'one': 61,\n",
       " 'how': 62,\n",
       " 'people': 63,\n",
       " 'he': 64,\n",
       " \"it's\": 65,\n",
       " \"don't\": 66,\n",
       " 'been': 67,\n",
       " 'who': 68,\n",
       " 'over': 69,\n",
       " 'into': 70,\n",
       " 'do': 71,\n",
       " 'video': 72,\n",
       " 'can': 73,\n",
       " 'emergency': 74,\n",
       " \"'\": 75,\n",
       " 'there': 76,\n",
       " 'disaster': 77,\n",
       " 'police': 78,\n",
       " 'than': 79,\n",
       " '3': 80,\n",
       " 'u': 81,\n",
       " 'her': 82,\n",
       " 'his': 83,\n",
       " 'would': 84,\n",
       " 'still': 85,\n",
       " 'were': 86,\n",
       " 'body': 87,\n",
       " 'us': 88,\n",
       " 'some': 89,\n",
       " 'back': 90,\n",
       " 'crash': 91,\n",
       " 'storm': 92,\n",
       " 'day': 93,\n",
       " 'them': 94,\n",
       " 'off': 95,\n",
       " 'got': 96,\n",
       " 'california': 97,\n",
       " '1': 98,\n",
       " 'why': 99,\n",
       " 'man': 100,\n",
       " 'had': 101,\n",
       " 'time': 102,\n",
       " 'know': 103,\n",
       " 'rt': 104,\n",
       " 'first': 105,\n",
       " 'suicide': 106,\n",
       " 'see': 107,\n",
       " 'going': 108,\n",
       " 'world': 109,\n",
       " 's': 110,\n",
       " 'nuclear': 111,\n",
       " 'love': 112,\n",
       " 'burning': 113,\n",
       " 'fires': 114,\n",
       " 'killed': 115,\n",
       " 'our': 116,\n",
       " 'youtube': 117,\n",
       " 'dead': 118,\n",
       " 'attack': 119,\n",
       " 'bomb': 120,\n",
       " 'two': 121,\n",
       " 'gt': 122,\n",
       " 'go': 123,\n",
       " '4': 124,\n",
       " 'their': 125,\n",
       " 'full': 126,\n",
       " 'car': 127,\n",
       " 'life': 128,\n",
       " 'buildings': 129,\n",
       " 'being': 130,\n",
       " '5': 131,\n",
       " 'hiroshima': 132,\n",
       " \"can't\": 133,\n",
       " 'today': 134,\n",
       " 'good': 135,\n",
       " 'may': 136,\n",
       " 'here': 137,\n",
       " 'accident': 138,\n",
       " 'only': 139,\n",
       " '2015': 140,\n",
       " 'say': 141,\n",
       " 'watch': 142,\n",
       " 'war': 143,\n",
       " 'down': 144,\n",
       " 'families': 145,\n",
       " 'train': 146,\n",
       " 'think': 147,\n",
       " 'could': 148,\n",
       " 'did': 149,\n",
       " 'last': 150,\n",
       " 'way': 151,\n",
       " 'years': 152,\n",
       " 'then': 153,\n",
       " 'too': 154,\n",
       " 'w': 155,\n",
       " 'home': 156,\n",
       " 'make': 157,\n",
       " 'its': 158,\n",
       " 'many': 159,\n",
       " 'year': 160,\n",
       " 'work': 161,\n",
       " 'because': 162,\n",
       " 'best': 163,\n",
       " 'look': 164,\n",
       " 'want': 165,\n",
       " 'help': 166,\n",
       " 'need': 167,\n",
       " 'take': 168,\n",
       " 'collapse': 169,\n",
       " 'am': 170,\n",
       " 'really': 171,\n",
       " 'army': 172,\n",
       " 'mass': 173,\n",
       " 'please': 174,\n",
       " 'death': 175,\n",
       " 'wildfire': 176,\n",
       " 'lol': 177,\n",
       " 'black': 178,\n",
       " 'right': 179,\n",
       " 'should': 180,\n",
       " 'mh370': 181,\n",
       " 'forest': 182,\n",
       " \"you're\": 183,\n",
       " 'another': 184,\n",
       " 'hot': 185,\n",
       " 'school': 186,\n",
       " 'those': 187,\n",
       " 'bombing': 188,\n",
       " 'old': 189,\n",
       " 'much': 190,\n",
       " '\\x89รป': 191,\n",
       " 'live': 192,\n",
       " 'never': 193,\n",
       " 'pm': 194,\n",
       " 'him': 195,\n",
       " '8': 196,\n",
       " '9': 197,\n",
       " 'even': 198,\n",
       " 'she': 199,\n",
       " 'wreck': 200,\n",
       " 'latest': 201,\n",
       " 'city': 202,\n",
       " 'any': 203,\n",
       " 'read': 204,\n",
       " 'northern': 205,\n",
       " 'let': 206,\n",
       " 'homes': 207,\n",
       " 'flood': 208,\n",
       " '6': 209,\n",
       " 'where': 210,\n",
       " 'flames': 211,\n",
       " 'fear': 212,\n",
       " 'every': 213,\n",
       " 'great': 214,\n",
       " 'im': 215,\n",
       " 'under': 216,\n",
       " 'injured': 217,\n",
       " 'obama': 218,\n",
       " 'atomic': 219,\n",
       " 'getting': 220,\n",
       " 'damage': 221,\n",
       " 'god': 222,\n",
       " 'come': 223,\n",
       " 'feel': 224,\n",
       " 'bomber': 225,\n",
       " 'fatal': 226,\n",
       " 'ever': 227,\n",
       " 'typhoon': 228,\n",
       " 'top': 229,\n",
       " 'hit': 230,\n",
       " \"that's\": 231,\n",
       " 'since': 232,\n",
       " 'japan': 233,\n",
       " 'cross': 234,\n",
       " 'oil': 235,\n",
       " 'floods': 236,\n",
       " 'shit': 237,\n",
       " 'everyone': 238,\n",
       " 'hope': 239,\n",
       " 'military': 240,\n",
       " 'content': 241,\n",
       " 'near': 242,\n",
       " 'coming': 243,\n",
       " 'stop': 244,\n",
       " 'most': 245,\n",
       " 'said': 246,\n",
       " '10': 247,\n",
       " 'weather': 248,\n",
       " 'night': 249,\n",
       " 'next': 250,\n",
       " 'before': 251,\n",
       " 'while': 252,\n",
       " 'found': 253,\n",
       " 'flooding': 254,\n",
       " 'times': 255,\n",
       " 'during': 256,\n",
       " 'without': 257,\n",
       " 'ass': 258,\n",
       " 'plan': 259,\n",
       " 'smoke': 260,\n",
       " 'set': 261,\n",
       " 'truck': 262,\n",
       " '08': 263,\n",
       " '15': 264,\n",
       " 'debris': 265,\n",
       " 'well': 266,\n",
       " 'wild': 267,\n",
       " 'movie': 268,\n",
       " 'lightning': 269,\n",
       " 'thunderstorm': 270,\n",
       " 'earthquake': 271,\n",
       " 'area': 272,\n",
       " 'heat': 273,\n",
       " 'face': 274,\n",
       " 'state': 275,\n",
       " 'through': 276,\n",
       " 'm': 277,\n",
       " 'water': 278,\n",
       " 'severe': 279,\n",
       " 'fucking': 280,\n",
       " 'explosion': 281,\n",
       " 'looks': 282,\n",
       " 'made': 283,\n",
       " 'wounded': 284,\n",
       " 'malaysia': 285,\n",
       " 'rain': 286,\n",
       " 'these': 287,\n",
       " 'cause': 288,\n",
       " 'services': 289,\n",
       " 'check': 290,\n",
       " 'high': 291,\n",
       " 'bad': 292,\n",
       " 'warning': 293,\n",
       " 'says': 294,\n",
       " 'natural': 295,\n",
       " 'thunder': 296,\n",
       " 'always': 297,\n",
       " 'until': 298,\n",
       " '11': 299,\n",
       " 'also': 300,\n",
       " 'bloody': 301,\n",
       " 'little': 302,\n",
       " 'run': 303,\n",
       " 'liked': 304,\n",
       " 'hail': 305,\n",
       " 'fall': 306,\n",
       " 'hurricane': 307,\n",
       " 'spill': 308,\n",
       " 'loud': 309,\n",
       " 'reddit': 310,\n",
       " 'photo': 311,\n",
       " 'gonna': 312,\n",
       " 'head': 313,\n",
       " 'free': 314,\n",
       " 'house': 315,\n",
       " 'which': 316,\n",
       " 'red': 317,\n",
       " 'again': 318,\n",
       " '7': 319,\n",
       " 'weapon': 320,\n",
       " 'evacuate': 321,\n",
       " 'evacuation': 322,\n",
       " 'summer': 323,\n",
       " 'end': 324,\n",
       " 'screaming': 325,\n",
       " 'family': 326,\n",
       " 'blood': 327,\n",
       " 'missing': 328,\n",
       " 'wind': 329,\n",
       " 'bags': 330,\n",
       " 'sinking': 331,\n",
       " 'trapped': 332,\n",
       " 'injuries': 333,\n",
       " 'fatalities': 334,\n",
       " 'destroy': 335,\n",
       " '70': 336,\n",
       " 'change': 337,\n",
       " 'girl': 338,\n",
       " 'big': 339,\n",
       " 'released': 340,\n",
       " 'attacked': 341,\n",
       " 'explode': 342,\n",
       " 'failure': 343,\n",
       " 'panic': 344,\n",
       " 'sinkhole': 345,\n",
       " \"i've\": 346,\n",
       " 'around': 347,\n",
       " 'someone': 348,\n",
       " 'air': 349,\n",
       " 'ambulance': 350,\n",
       " \"he's\": 351,\n",
       " 'destruction': 352,\n",
       " 'destroyed': 353,\n",
       " 'post': 354,\n",
       " 'saudi': 355,\n",
       " 'weapons': 356,\n",
       " 'bag': 357,\n",
       " 'hazard': 358,\n",
       " 'harm': 359,\n",
       " 'rescue': 360,\n",
       " 'devastated': 361,\n",
       " 'week': 362,\n",
       " '\\x89รปรฒ': 363,\n",
       " 'burned': 364,\n",
       " 'does': 365,\n",
       " 'game': 366,\n",
       " 'murder': 367,\n",
       " 'trauma': 368,\n",
       " 'survivors': 369,\n",
       " 'survive': 370,\n",
       " '05': 371,\n",
       " 'battle': 372,\n",
       " 'keep': 373,\n",
       " 'drought': 374,\n",
       " 'collision': 375,\n",
       " 'deaths': 376,\n",
       " 'twister': 377,\n",
       " 'wrecked': 378,\n",
       " 'county': 379,\n",
       " 'breaking': 380,\n",
       " 'tonight': 381,\n",
       " 'n': 382,\n",
       " '40': 383,\n",
       " 'p': 384,\n",
       " 'call': 385,\n",
       " 'ok': 386,\n",
       " 'self': 387,\n",
       " 'away': 388,\n",
       " 'white': 389,\n",
       " 'fuck': 390,\n",
       " 'real': 391,\n",
       " 'terrorism': 392,\n",
       " 'bombed': 393,\n",
       " 'bridge': 394,\n",
       " 'ruin': 395,\n",
       " 'service': 396,\n",
       " 'collided': 397,\n",
       " 'rescued': 398,\n",
       " 'crush': 399,\n",
       " 'survived': 400,\n",
       " 'dust': 401,\n",
       " 'outbreak': 402,\n",
       " 'windstorm': 403,\n",
       " 'update': 404,\n",
       " \"there's\": 405,\n",
       " 'bus': 406,\n",
       " '30': 407,\n",
       " 'crashed': 408,\n",
       " 'whole': 409,\n",
       " 'august': 410,\n",
       " '0': 411,\n",
       " 'things': 412,\n",
       " 'twitter': 413,\n",
       " 'terrorist': 414,\n",
       " 'put': 415,\n",
       " 'curfew': 416,\n",
       " 'wreckage': 417,\n",
       " 'massacre': 418,\n",
       " 'whirlwind': 419,\n",
       " 'other': 420,\n",
       " 'd': 421,\n",
       " 'least': 422,\n",
       " 'injury': 423,\n",
       " 'o': 424,\n",
       " 'suspect': 425,\n",
       " 'long': 426,\n",
       " 'saw': 427,\n",
       " 'against': 428,\n",
       " 'hostage': 429,\n",
       " 'wanna': 430,\n",
       " 'tragedy': 431,\n",
       " 'traumatised': 432,\n",
       " 'catastrophe': 433,\n",
       " 'chemical': 434,\n",
       " 'collapsed': 435,\n",
       " 'landslide': 436,\n",
       " 'deluge': 437,\n",
       " 'derailed': 438,\n",
       " 'devastation': 439,\n",
       " 'screamed': 440,\n",
       " 'famine': 441,\n",
       " 'investigators': 442,\n",
       " 'hostages': 443,\n",
       " 'quarantined': 444,\n",
       " 'sandstorm': 445,\n",
       " 'sunk': 446,\n",
       " 'wave': 447,\n",
       " 'better': 448,\n",
       " 'past': 449,\n",
       " 'road': 450,\n",
       " 'heard': 451,\n",
       " 'thing': 452,\n",
       " 'kills': 453,\n",
       " 'national': 454,\n",
       " 'apocalypse': 455,\n",
       " 'heart': 456,\n",
       " 'story': 457,\n",
       " 'woman': 458,\n",
       " 'power': 459,\n",
       " 'women': 460,\n",
       " 'oh': 461,\n",
       " 'bleeding': 462,\n",
       " 'blown': 463,\n",
       " 'show': 464,\n",
       " 'rioting': 465,\n",
       " 'riot': 466,\n",
       " 'casualties': 467,\n",
       " 'danger': 468,\n",
       " 'stock': 469,\n",
       " 'migrants': 470,\n",
       " 'derail': 471,\n",
       " 'desolation': 472,\n",
       " 'refugees': 473,\n",
       " 'drowning': 474,\n",
       " 'inundated': 475,\n",
       " 'bang': 476,\n",
       " 'quarantine': 477,\n",
       " 'screams': 478,\n",
       " 'structural': 479,\n",
       " \"i'll\": 480,\n",
       " 'meltdown': 481,\n",
       " 'island': 482,\n",
       " 'plane': 483,\n",
       " 'iran': 484,\n",
       " 'went': 485,\n",
       " 'save': 486,\n",
       " 'violent': 487,\n",
       " 'fedex': 488,\n",
       " 'light': 489,\n",
       " 'rescuers': 490,\n",
       " 'boat': 491,\n",
       " 'catastrophic': 492,\n",
       " 'cliff': 493,\n",
       " 'wounds': 494,\n",
       " 'electrocuted': 495,\n",
       " 'fatality': 496,\n",
       " 'flattened': 497,\n",
       " 'lava': 498,\n",
       " 'tornado': 499,\n",
       " 'st': 500,\n",
       " 'something': 501,\n",
       " 'use': 502,\n",
       " 'thank': 503,\n",
       " \"'the\": 504,\n",
       " 'airplane': 505,\n",
       " 'ebay': 506,\n",
       " 'lot': 507,\n",
       " 'food': 508,\n",
       " 'zone': 509,\n",
       " 'soon': 510,\n",
       " 'armageddon': 511,\n",
       " 'part': 512,\n",
       " 'mosque': 513,\n",
       " 'blew': 514,\n",
       " 'drown': 515,\n",
       " 'bagging': 516,\n",
       " '00': 517,\n",
       " 'anniversary': 518,\n",
       " 'caused': 519,\n",
       " 'derailment': 520,\n",
       " 'trouble': 521,\n",
       " 'tsunami': 522,\n",
       " 'evacuated': 523,\n",
       " 'pandemonium': 524,\n",
       " 'panicking': 525,\n",
       " 'razed': 526,\n",
       " 'cool': 527,\n",
       " 'left': 528,\n",
       " 'minute': 529,\n",
       " 'phone': 530,\n",
       " 'sure': 531,\n",
       " 'river': 532,\n",
       " 'b': 533,\n",
       " 'calgary': 534,\n",
       " 'bioterror': 535,\n",
       " 'blazing': 536,\n",
       " 'market': 537,\n",
       " 'send': 538,\n",
       " 'baby': 539,\n",
       " 'hazardous': 540,\n",
       " 'exploded': 541,\n",
       " 'hijacking': 542,\n",
       " 'rainstorm': 543,\n",
       " 'care': 544,\n",
       " 'r': 545,\n",
       " 'horrible': 546,\n",
       " 'doing': 547,\n",
       " 'possible': 548,\n",
       " 'goes': 549,\n",
       " 'government': 550,\n",
       " 'must': 551,\n",
       " 'thought': 552,\n",
       " 'kill': 553,\n",
       " 'tomorrow': 554,\n",
       " 'officials': 555,\n",
       " 'india': 556,\n",
       " 'group': 557,\n",
       " 'longer': 558,\n",
       " 'security': 559,\n",
       " 'song': 560,\n",
       " 'affected': 561,\n",
       " 'casualty': 562,\n",
       " 'collide': 563,\n",
       " 'crushed': 564,\n",
       " 'isis': 565,\n",
       " 'demolish': 566,\n",
       " 'demolition': 567,\n",
       " 'detonation': 568,\n",
       " 'drowned': 569,\n",
       " 'hijacker': 570,\n",
       " 'murderer': 571,\n",
       " 'obliterated': 572,\n",
       " 'three': 573,\n",
       " 'used': 574,\n",
       " 'kids': 575,\n",
       " 'came': 576,\n",
       " 'very': 577,\n",
       " 'issues': 578,\n",
       " 'same': 579,\n",
       " 'reunion': 580,\n",
       " 'airport': 581,\n",
       " 'annihilated': 582,\n",
       " 'arson': 583,\n",
       " 'sound': 584,\n",
       " 'stay': 585,\n",
       " 'beautiful': 586,\n",
       " 'c': 587,\n",
       " 'half': 588,\n",
       " 'shoulder': 589,\n",
       " 'responders': 590,\n",
       " 'officer': 591,\n",
       " 'cyclone': 592,\n",
       " 'blast': 593,\n",
       " 'hundreds': 594,\n",
       " \"legionnaires'\": 595,\n",
       " 'demolished': 596,\n",
       " 'prebreak': 597,\n",
       " 'mudslide': 598,\n",
       " 'obliterate': 599,\n",
       " 'due': 600,\n",
       " 'south': 601,\n",
       " 'days': 602,\n",
       " 'thanks': 603,\n",
       " 'few': 604,\n",
       " 'already': 605,\n",
       " 'making': 606,\n",
       " 'done': 607,\n",
       " 'believe': 608,\n",
       " 'lt': 609,\n",
       " 'hours': 610,\n",
       " 'start': 611,\n",
       " 'yet': 612,\n",
       " 'remember': 613,\n",
       " '50': 614,\n",
       " 'report': 615,\n",
       " 'ur': 616,\n",
       " 'electrocute': 617,\n",
       " 'engulfed': 618,\n",
       " 'eyewitness': 619,\n",
       " 'obliteration': 620,\n",
       " 'siren': 621,\n",
       " 'upheaval': 622,\n",
       " 'building': 623,\n",
       " 'died': 624,\n",
       " 'far': 625,\n",
       " 'ablaze': 626,\n",
       " 'inside': 627,\n",
       " 'leave': 628,\n",
       " '16': 629,\n",
       " 'shooting': 630,\n",
       " 'actually': 631,\n",
       " 'men': 632,\n",
       " 'wake': 633,\n",
       " 'fan': 634,\n",
       " 'israeli': 635,\n",
       " 'music': 636,\n",
       " 'having': 637,\n",
       " 'nothing': 638,\n",
       " 'blight': 639,\n",
       " 'policy': 640,\n",
       " 'such': 641,\n",
       " 'turkey': 642,\n",
       " 'mp': 643,\n",
       " 'displaced': 644,\n",
       " 'both': 645,\n",
       " 'site': 646,\n",
       " 'shot': 647,\n",
       " 'traffic': 648,\n",
       " 'support': 649,\n",
       " 'die': 650,\n",
       " 'play': 651,\n",
       " 'sirens': 652,\n",
       " 'trying': 653,\n",
       " 'media': 654,\n",
       " 'lab': 655,\n",
       " 'yes': 656,\n",
       " 'pic': 657,\n",
       " 're\\x89รป': 658,\n",
       " 'words': 659,\n",
       " 'nearby': 660,\n",
       " 'bush': 661,\n",
       " 'deluged': 662,\n",
       " 'seismic': 663,\n",
       " 'volcano': 664,\n",
       " 'reactor': 665,\n",
       " 'wait': 666,\n",
       " '\\x89รปรณ': 667,\n",
       " 'nowplaying': 668,\n",
       " 'plans': 669,\n",
       " 'gets': 670,\n",
       " 'brown': 671,\n",
       " 'ago': 672,\n",
       " 'fun': 673,\n",
       " \"i'd\": 674,\n",
       " 'children': 675,\n",
       " 'guys': 676,\n",
       " \"doesn't\": 677,\n",
       " 'abc': 678,\n",
       " 'line': 679,\n",
       " 'low': 680,\n",
       " 'find': 681,\n",
       " 'legionnaires': 682,\n",
       " 'hijack': 683,\n",
       " 'sue': 684,\n",
       " 'rubble': 685,\n",
       " 'swallowed': 686,\n",
       " 'stretcher': 687,\n",
       " 'second': 688,\n",
       " 'outside': 689,\n",
       " 'north': 690,\n",
       " '06': 691,\n",
       " 'tell': 692,\n",
       " 'job': 693,\n",
       " 'almost': 694,\n",
       " 'aircraft': 695,\n",
       " 'helicopter': 696,\n",
       " 'history': 697,\n",
       " 'bc': 698,\n",
       " 'order': 699,\n",
       " 'fight': 700,\n",
       " 'data': 701,\n",
       " 'own': 702,\n",
       " 'business': 703,\n",
       " 'yeah': 704,\n",
       " 'deal': 705,\n",
       " 'health': 706,\n",
       " 'photos': 707,\n",
       " 'watching': 708,\n",
       " 'bigger': 709,\n",
       " 'pkk': 710,\n",
       " 'memories': 711,\n",
       " 'snowstorm': 712,\n",
       " 'detonated': 713,\n",
       " 'lost': 714,\n",
       " 'west': 715,\n",
       " 'happy': 716,\n",
       " \"didn't\": 717,\n",
       " 'book': 718,\n",
       " 'anyone': 719,\n",
       " 'reuters': 720,\n",
       " 'hey': 721,\n",
       " 'bar': 722,\n",
       " 'hell': 723,\n",
       " 'maybe': 724,\n",
       " 'pick': 725,\n",
       " 'tv': 726,\n",
       " 'american': 727,\n",
       " 'makes': 728,\n",
       " 'avalanche': 729,\n",
       " 'transport': 730,\n",
       " 'bioterrorism': 731,\n",
       " 'blizzard': 732,\n",
       " 'lives': 733,\n",
       " 'rise': 734,\n",
       " 'x': 735,\n",
       " 'hear': 736,\n",
       " 'waves': 737,\n",
       " '25': 738,\n",
       " 'desolate': 739,\n",
       " 'projected': 740,\n",
       " 'hat': 741,\n",
       " 'place': 742,\n",
       " 'street': 743,\n",
       " \"what's\": 744,\n",
       " 'side': 745,\n",
       " 'rd': 746,\n",
       " 'finally': 747,\n",
       " 'property': 748,\n",
       " 'might': 749,\n",
       " 'eyes': 750,\n",
       " 'pakistan': 751,\n",
       " 'amid': 752,\n",
       " 'damn': 753,\n",
       " 'team': 754,\n",
       " 'person': 755,\n",
       " 'listen': 756,\n",
       " 'space': 757,\n",
       " 'hollywood': 758,\n",
       " 'pretty': 759,\n",
       " 'online': 760,\n",
       " 'though': 761,\n",
       " 'morning': 762,\n",
       " 'money': 763,\n",
       " 'probably': 764,\n",
       " 'trains': 765,\n",
       " 'soudelor': 766,\n",
       " 'effect': 767,\n",
       " 'detonate': 768,\n",
       " 'declares': 769,\n",
       " 'la': 770,\n",
       " '20': 771,\n",
       " 'fast': 772,\n",
       " 'arsonist': 773,\n",
       " 'center': 774,\n",
       " 'mom': 775,\n",
       " 'once': 776,\n",
       " 'wrong': 777,\n",
       " 'feared': 778,\n",
       " '17': 779,\n",
       " 'hate': 780,\n",
       " 'everything': 781,\n",
       " 'country': 782,\n",
       " 'horror': 783,\n",
       " 'seen': 784,\n",
       " 'case': 785,\n",
       " 'annihilation': 786,\n",
       " \"we're\": 787,\n",
       " 'major': 788,\n",
       " 'child': 789,\n",
       " 'crisis': 790,\n",
       " 'leather': 791,\n",
       " 'caught': 792,\n",
       " 'anything': 793,\n",
       " 'blaze': 794,\n",
       " 'okay': 795,\n",
       " 'literally': 796,\n",
       " 'spot': 797,\n",
       " 'confirmed': 798,\n",
       " 'sensor': 799,\n",
       " '01': 800,\n",
       " 'trench': 801,\n",
       " 'hailstorm': 802,\n",
       " 'refugio': 803,\n",
       " 'costlier': 804,\n",
       " '16yr': 805,\n",
       " 'miners': 806,\n",
       " \"'conclusively\": 807,\n",
       " \"confirmed'\": 808,\n",
       " '13': 809,\n",
       " 'flash': 810,\n",
       " 'flag': 811,\n",
       " 'cars': 812,\n",
       " 'lord': 813,\n",
       " 'vehicle': 814,\n",
       " 'daily': 815,\n",
       " 'guy': 816,\n",
       " 'jobs': 817,\n",
       " \"they're\": 818,\n",
       " 'ship': 819,\n",
       " '12': 820,\n",
       " 'crazy': 821,\n",
       " 'sorry': 822,\n",
       " 'ball': 823,\n",
       " 'usa': 824,\n",
       " 'stand': 825,\n",
       " 'called': 826,\n",
       " 'name': 827,\n",
       " 'class': 828,\n",
       " 'texas': 829,\n",
       " 'worst': 830,\n",
       " 'needs': 831,\n",
       " 'fukushima': 832,\n",
       " 'move': 833,\n",
       " 'land': 834,\n",
       " 'wow': 835,\n",
       " 'russian': 836,\n",
       " 'giant': 837,\n",
       " 'crews': 838,\n",
       " 'angry': 839,\n",
       " 'course': 840,\n",
       " 'aug': 841,\n",
       " 'banned': 842,\n",
       " 'knock': 843,\n",
       " 'saipan': 844,\n",
       " 'mayhem': 845,\n",
       " 'reason': 846,\n",
       " 'heavy': 847,\n",
       " 'across': 848,\n",
       " 'haha': 849,\n",
       " 'others': 850,\n",
       " 'huge': 851,\n",
       " 'talk': 852,\n",
       " 'win': 853,\n",
       " 'yourself': 854,\n",
       " \"she's\": 855,\n",
       " 'omg': 856,\n",
       " 'sign': 857,\n",
       " 'poor': 858,\n",
       " 'boy': 859,\n",
       " 'united': 860,\n",
       " \"isn't\": 861,\n",
       " 'east': 862,\n",
       " 'town': 863,\n",
       " 'gun': 864,\n",
       " 'anthrax': 865,\n",
       " 'running': 866,\n",
       " 'nearly': 867,\n",
       " 'computers': 868,\n",
       " 'dont': 869,\n",
       " 'follow': 870,\n",
       " 'entire': 871,\n",
       " 'searching': 872,\n",
       " 'link': 873,\n",
       " 'meek': 874,\n",
       " '60': 875,\n",
       " 'myself': 876,\n",
       " 'gbbo': 877,\n",
       " 'chance': 878,\n",
       " 'friends': 879,\n",
       " 'bbc': 880,\n",
       " 'bodies': 881,\n",
       " 'uk': 882,\n",
       " 'image': 883,\n",
       " 'ignition': 884,\n",
       " 'offensive': 885,\n",
       " 'try': 886,\n",
       " 'wanted': 887,\n",
       " 'chicago': 888,\n",
       " 'alone': 889,\n",
       " 'hard': 890,\n",
       " 'yours': 891,\n",
       " \"'i\": 892,\n",
       " 'happened': 893,\n",
       " 'radio': 894,\n",
       " 'totally': 895,\n",
       " '100': 896,\n",
       " 'learn': 897,\n",
       " 'charged': 898,\n",
       " 'truth': 899,\n",
       " 'beach': 900,\n",
       " 'ca': 901,\n",
       " 'feeling': 902,\n",
       " 'christian': 903,\n",
       " 'muslims': 904,\n",
       " 'temple': 905,\n",
       " 'mount': 906,\n",
       " 'view': 907,\n",
       " 'eye': 908,\n",
       " 'taken': 909,\n",
       " 'playing': 910,\n",
       " 'mishaps': 911,\n",
       " 'public': 912,\n",
       " 'e': 913,\n",
       " 'mad': 914,\n",
       " \"let's\": 915,\n",
       " 'cake': 916,\n",
       " 'level': 917,\n",
       " 'ladies': 918,\n",
       " 'appears': 919,\n",
       " 'centre': 920,\n",
       " 'alarm': 921,\n",
       " 'china': 922,\n",
       " 'issued': 923,\n",
       " 'emmerdale': 924,\n",
       " 'signs': 925,\n",
       " 'become': 926,\n",
       " 'cnn': 927,\n",
       " 'disea': 928,\n",
       " 'closed': 929,\n",
       " '18': 930,\n",
       " 'front': 931,\n",
       " 'else': 932,\n",
       " '24': 933,\n",
       " 'drive': 934,\n",
       " 'global': 935,\n",
       " 'official': 936,\n",
       " '\\x89รปรฏ': 937,\n",
       " 'dog': 938,\n",
       " 'ready': 939,\n",
       " 'vs': 940,\n",
       " 'film': 941,\n",
       " 'till': 942,\n",
       " 'friend': 943,\n",
       " 'blue': 944,\n",
       " 'green': 945,\n",
       " 'driving': 946,\n",
       " 'favorite': 947,\n",
       " 'star': 948,\n",
       " 'germs': 949,\n",
       " 'looking': 950,\n",
       " 'pain': 951,\n",
       " \"ain't\": 952,\n",
       " 'large': 953,\n",
       " 'womens': 954,\n",
       " 'drake': 955,\n",
       " 'downtown': 956,\n",
       " 'houses': 957,\n",
       " 'insurance': 958,\n",
       " 'mph': 959,\n",
       " 'instead': 960,\n",
       " 'coaches': 961,\n",
       " 'sea': 962,\n",
       " 'flight': 963,\n",
       " 'quiz': 964,\n",
       " \"reddit's\": 965,\n",
       " 'virgin': 966,\n",
       " 'chile': 967,\n",
       " 'bring': 968,\n",
       " 'thousands': 969,\n",
       " 'reported': 970,\n",
       " 'dies': 971,\n",
       " 'aftershock': 972,\n",
       " 'moment': 973,\n",
       " 'behind': 974,\n",
       " 'four': 975,\n",
       " 'early': 976,\n",
       " 'couple': 977,\n",
       " 'trust': 978,\n",
       " \"won't\": 979,\n",
       " 'driver': 980,\n",
       " 'israel': 981,\n",
       " 'park': 982,\n",
       " 'following': 983,\n",
       " 'comes': 984,\n",
       " 'scared': 985,\n",
       " 'escape': 986,\n",
       " 'russia': 987,\n",
       " 'control': 988,\n",
       " 'reports': 989,\n",
       " 'hiring': 990,\n",
       " 'true': 991,\n",
       " 'theater': 992,\n",
       " 'gave': 993,\n",
       " 'gop': 994,\n",
       " 'added': 995,\n",
       " 'miss': 996,\n",
       " 'turn': 997,\n",
       " 'info': 998,\n",
       " 'fans': 999,\n",
       " 'sad': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(tweets['text'])\n",
    "enconded_test = t.texts_to_sequences(tests['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 116 4534   25 ...    0    0    0]\n",
      " [ 182   46  242 ...    0    0    0]\n",
      " [  40 1705 1572 ...    0    0    0]\n",
      " ...\n",
      " [ 106  225  453 ...    0    0    0]\n",
      " [ 121  837 1338 ...    0    0    0]\n",
      " [   4  201   53 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 31 # Maxima cantidad de palabras en los tweets\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_tests = pad_sequences(enconded_test, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.twitter.27B.100d.txt') # Vectores entrenados de 100 dimensiones\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # Si la palabra no esta queda llena de 0s\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 31, 100)           2258600   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 25, 256)           179456    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6401      \n",
      "=================================================================\n",
      "Total params: 2,444,457\n",
      "Trainable params: 185,857\n",
      "Non-trainable params: 2,258,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import layers\n",
    "\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=31, trainable=False)\n",
    "model1.add(e)\n",
    "model1.add(layers.Conv1D(256, 7, activation='relu'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(padded_docs, tweets['target'], test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 - 1s - loss: 0.5262 - accuracy: 0.7444 - val_loss: 0.4449 - val_accuracy: 0.7983\n",
      "Epoch 2/50\n",
      "44/44 - 1s - loss: 0.3909 - accuracy: 0.8335 - val_loss: 0.4308 - val_accuracy: 0.8133\n",
      "Epoch 3/50\n",
      "44/44 - 1s - loss: 0.3256 - accuracy: 0.8687 - val_loss: 0.4464 - val_accuracy: 0.8144\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152fee190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model1.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 31, 100)           2258600   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 25, 256)           179456    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6401      \n",
      "=================================================================\n",
      "Total params: 2,444,457\n",
      "Trainable params: 185,857\n",
      "Non-trainable params: 2,258,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import layers\n",
    "\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=31, trainable=False)\n",
    "model1.add(e)\n",
    "model1.add(layers.Conv1D(256, 7, activation='relu'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "59/59 - 1s - loss: 0.5096 - accuracy: 0.7612\n",
      "Epoch 2/5\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "59/59 - 1s - loss: 0.3848 - accuracy: 0.8366\n",
      "Epoch 3/5\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "59/59 - 1s - loss: 0.3255 - accuracy: 0.8680\n",
      "Epoch 4/5\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "59/59 - 1s - loss: 0.2696 - accuracy: 0.8932\n",
      "Epoch 5/5\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "59/59 - 1s - loss: 0.2180 - accuracy: 0.9257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x155cb0950>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model1.fit(padded_docs, tweets['target'],\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model1.predict(padded_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4535236 ],\n",
       "       [0.6374486 ],\n",
       "       [0.51281166],\n",
       "       ...,\n",
       "       [0.7867126 ],\n",
       "       [0.6981692 ],\n",
       "       [0.24020772]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = []\n",
    "\n",
    "for i in test_result:\n",
    "    if i >= 0.5 :\n",
    "        submit.append(1)\n",
    "    else:\n",
    "        submit.append(0)\n",
    "\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ยรร SAFETY FASTE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows ร 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                    Just happened a terrible car crash       0  \n",
       "1     Heard about #earthquake is different cities, s...       1  \n",
       "2     there is a forest fire at spot pond, geese are...       1  \n",
       "3              Apocalypse lighting. #Spokane #wildfires       1  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan       1  \n",
       "...                                                 ...     ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ยรร SAFETY FASTE...       1  \n",
       "3259  Storm in RI worse than last hurricane. My city...       1  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...       1  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...       1  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...       0  \n",
       "\n",
       "[3263 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = tests[['id', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows ร 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('submit_prueba_36.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 31, 100)           2258600   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 25, 256)           179456    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                64010     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,502,077\n",
      "Trainable params: 243,477\n",
      "Non-trainable params: 2,258,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import layers\n",
    "\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=31, trainable=False)\n",
    "model1.add(e)\n",
    "model1.add(layers.Conv1D(256, 7, activation='relu'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(10, activation='sigmoid'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 - 1s - loss: 0.5305 - accuracy: 0.7365 - val_loss: 0.4629 - val_accuracy: 0.8010\n",
      "Epoch 2/50\n",
      "44/44 - 1s - loss: 0.4246 - accuracy: 0.8240 - val_loss: 0.4222 - val_accuracy: 0.8144\n",
      "Epoch 3/50\n",
      "44/44 - 1s - loss: 0.3736 - accuracy: 0.8560 - val_loss: 0.4243 - val_accuracy: 0.8112\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1536348d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model1.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import layers\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "def build_model(fully_conected, num_filters, kern_size):\n",
    "    model1 = Sequential()\n",
    "    e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=31, trainable=True)\n",
    "    model1.add(e)\n",
    "    model1.add(layers.Conv1D(num_filters, kern_size, activation='relu'))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(fully_conected, activation='sigmoid'))\n",
    "    model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 35ms/step - loss: 0.6060 - accuracy: 0.7068 - val_loss: 0.4906 - val_accuracy: 0.7661\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 0.3714 - accuracy: 0.8437 - val_loss: 0.4632 - val_accuracy: 0.7804\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 0.2524 - accuracy: 0.9003 - val_loss: 0.5939 - val_accuracy: 0.7637\n",
      "Epoch 00003: early stopping\n",
      "WARNING:tensorflow:From /Users/matiascano/.pyenv/versions/3.7.7/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "[CV]  num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88, total=   5.1s\n",
      "[CV] num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 34ms/step - loss: 0.5289 - accuracy: 0.7443 - val_loss: 0.4674 - val_accuracy: 0.7852\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 1s 33ms/step - loss: 0.3331 - accuracy: 0.8623 - val_loss: 0.5105 - val_accuracy: 0.7804\n",
      "Epoch 00002: early stopping\n",
      "16/16 [==============================] - 0s 7ms/step\n",
      "[CV]  num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88, total=   3.7s\n",
      "[CV] num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 35ms/step - loss: 0.5728 - accuracy: 0.7275 - val_loss: 0.4700 - val_accuracy: 0.7828\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 0.3554 - accuracy: 0.8472 - val_loss: 0.4664 - val_accuracy: 0.7852\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 0.2232 - accuracy: 0.9133 - val_loss: 0.4796 - val_accuracy: 0.7804\n",
      "Epoch 00003: early stopping\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "[CV]  num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88, total=   5.2s\n",
      "[CV] num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.5991 - accuracy: 0.6960 - val_loss: 0.5099 - val_accuracy: 0.7661\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 0.3688 - accuracy: 0.8368 - val_loss: 0.5209 - val_accuracy: 0.7733\n",
      "Epoch 00002: early stopping\n",
      "16/16 [==============================] - 0s 7ms/step\n",
      "[CV]  num_filters=128, kern_size=7, fully_conected=800, epochs=15, batch_size=88, total=   3.9s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.5429 - accuracy: 0.7243 - val_loss: 0.5043 - val_accuracy: 0.7446\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.3482 - accuracy: 0.8543 - val_loss: 0.5300 - val_accuracy: 0.7589\n",
      "Epoch 00002: early stopping\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88, total=   4.5s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.5613 - accuracy: 0.7246 - val_loss: 0.4735 - val_accuracy: 0.7852\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.3483 - accuracy: 0.8549 - val_loss: 0.4806 - val_accuracy: 0.7685\n",
      "Epoch 00002: early stopping\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88, total=   4.3s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 40ms/step - loss: 0.5496 - accuracy: 0.7331 - val_loss: 0.4903 - val_accuracy: 0.7733\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 2s 38ms/step - loss: 0.3372 - accuracy: 0.8565 - val_loss: 0.5085 - val_accuracy: 0.7852\n",
      "Epoch 00002: early stopping\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88, total=   4.2s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88 \n",
      "Epoch 1/15\n",
      "43/43 [==============================] - 2s 39ms/step - loss: 0.6071 - accuracy: 0.7048 - val_loss: 0.4979 - val_accuracy: 0.7733\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 0.3622 - accuracy: 0.8390 - val_loss: 0.5194 - val_accuracy: 0.7733\n",
      "Epoch 00002: early stopping\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=800, epochs=15, batch_size=88, total=   4.1s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65 \n",
      "Epoch 1/15\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 0.5295 - accuracy: 0.7379 - val_loss: 0.5360 - val_accuracy: 0.7613\n",
      "Epoch 2/15\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 0.3855 - accuracy: 0.8379 - val_loss: 0.4804 - val_accuracy: 0.7757\n",
      "Epoch 3/15\n",
      "58/58 [==============================] - 1s 25ms/step - loss: 0.2974 - accuracy: 0.8884 - val_loss: 0.4832 - val_accuracy: 0.7757\n",
      "Epoch 00003: early stopping\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65, total=   5.2s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65 \n",
      "Epoch 1/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.5336 - accuracy: 0.7347 - val_loss: 0.4843 - val_accuracy: 0.7780\n",
      "Epoch 2/15\n",
      "58/58 [==============================] - 1s 22ms/step - loss: 0.3812 - accuracy: 0.8384 - val_loss: 0.4817 - val_accuracy: 0.7757\n",
      "Epoch 3/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.2957 - accuracy: 0.8844 - val_loss: 0.4697 - val_accuracy: 0.7828\n",
      "Epoch 4/15\n",
      "58/58 [==============================] - 1s 23ms/step - loss: 0.2031 - accuracy: 0.9322 - val_loss: 0.4872 - val_accuracy: 0.7947\n",
      "Epoch 00004: early stopping\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65, total=   6.4s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65 \n",
      "Epoch 1/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.5360 - accuracy: 0.7265 - val_loss: 0.5276 - val_accuracy: 0.7685\n",
      "Epoch 2/15\n",
      "58/58 [==============================] - 1s 20ms/step - loss: 0.3797 - accuracy: 0.8448 - val_loss: 0.4672 - val_accuracy: 0.7971\n",
      "Epoch 3/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.2813 - accuracy: 0.8900 - val_loss: 0.4783 - val_accuracy: 0.7995\n",
      "Epoch 00003: early stopping\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65, total=   4.4s\n",
      "[CV] num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65 \n",
      "Epoch 1/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.5250 - accuracy: 0.7388 - val_loss: 0.5000 - val_accuracy: 0.7613\n",
      "Epoch 2/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.3853 - accuracy: 0.8366 - val_loss: 0.4928 - val_accuracy: 0.7900\n",
      "Epoch 3/15\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 0.2975 - accuracy: 0.8876 - val_loss: 0.5177 - val_accuracy: 0.7924\n",
      "Epoch 00003: early stopping\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=144, kern_size=3, fully_conected=30, epochs=15, batch_size=65, total=   4.4s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.5467 - accuracy: 0.7251 - val_loss: 0.5045 - val_accuracy: 0.7637\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.3947 - accuracy: 0.8296 - val_loss: 0.4684 - val_accuracy: 0.7852\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 26ms/step - loss: 0.3144 - accuracy: 0.8772 - val_loss: 0.4765 - val_accuracy: 0.7685\n",
      "Epoch 00003: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76, total=   4.2s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.6402 - accuracy: 0.6478 - val_loss: 0.5580 - val_accuracy: 0.7232\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.4614 - accuracy: 0.8020 - val_loss: 0.4832 - val_accuracy: 0.7757\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.3732 - accuracy: 0.8501 - val_loss: 0.4463 - val_accuracy: 0.8019\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 1s 25ms/step - loss: 0.2930 - accuracy: 0.8945 - val_loss: 0.4382 - val_accuracy: 0.7947\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 1s 25ms/step - loss: 0.2180 - accuracy: 0.9314 - val_loss: 0.4482 - val_accuracy: 0.7947\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76, total=   6.7s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.5407 - accuracy: 0.7305 - val_loss: 0.5184 - val_accuracy: 0.7613\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.3922 - accuracy: 0.8328 - val_loss: 0.4679 - val_accuracy: 0.7780\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 26ms/step - loss: 0.3147 - accuracy: 0.8753 - val_loss: 0.4599 - val_accuracy: 0.7924\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.2365 - accuracy: 0.9213 - val_loss: 0.4797 - val_accuracy: 0.8043\n",
      "Epoch 00004: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76, total=   5.6s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.5487 - accuracy: 0.7218 - val_loss: 0.5199 - val_accuracy: 0.7709\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.3916 - accuracy: 0.8262 - val_loss: 0.5034 - val_accuracy: 0.7685\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.3029 - accuracy: 0.8809 - val_loss: 0.5097 - val_accuracy: 0.7780\n",
      "Epoch 00003: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=30, epochs=15, batch_size=76, total=   4.1s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.5502 - accuracy: 0.7180 - val_loss: 0.5194 - val_accuracy: 0.7542\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.3925 - accuracy: 0.8344 - val_loss: 0.4670 - val_accuracy: 0.7780\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.3030 - accuracy: 0.8801 - val_loss: 0.5135 - val_accuracy: 0.7685\n",
      "Epoch 00003: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76, total=   4.1s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.5398 - accuracy: 0.7254 - val_loss: 0.5168 - val_accuracy: 0.7613\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.3804 - accuracy: 0.8442 - val_loss: 0.4922 - val_accuracy: 0.7900\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.2933 - accuracy: 0.8793 - val_loss: 0.4809 - val_accuracy: 0.7733\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.1969 - accuracy: 0.9335 - val_loss: 0.5221 - val_accuracy: 0.7733\n",
      "Epoch 00004: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76, total=   5.3s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.5211 - accuracy: 0.7432 - val_loss: 0.5137 - val_accuracy: 0.7709\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.3728 - accuracy: 0.8376 - val_loss: 0.4845 - val_accuracy: 0.7876\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.2785 - accuracy: 0.8937 - val_loss: 0.4915 - val_accuracy: 0.7924\n",
      "Epoch 00003: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76, total=   4.2s\n",
      "[CV] num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76 \n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.5331 - accuracy: 0.7449 - val_loss: 0.4998 - val_accuracy: 0.7733\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.3773 - accuracy: 0.8363 - val_loss: 0.5006 - val_accuracy: 0.7852\n",
      "Epoch 00002: early stopping\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[CV]  num_filters=128, kern_size=3, fully_conected=60, epochs=15, batch_size=76, total=   3.0s\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 21ms/step - loss: 0.5192 - accuracy: 0.7403 - val_loss: 0.4657 - val_accuracy: 0.7867\n",
      "Epoch 2/15\n",
      "67/67 [==============================] - 1s 21ms/step - loss: 0.3811 - accuracy: 0.8380 - val_loss: 0.4836 - val_accuracy: 0.7849\n",
      "Epoch 00002: early stopping\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "Best Accuracy : 0.8083\n",
      "{'num_filters': 128, 'kern_size': 3, 'fully_conected': 60, 'epochs': 15, 'batch_size': 76}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = dict(num_filters=[32, 128, 144, 256],\n",
    "                      kern_size=[3, 5, 7],\n",
    "                      batch_size = [45,65,76,88],\n",
    "                      fully_conected = [30, 60, 800], epochs = [15])\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, epochs=15, validation_split=0.1,verbose=1)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=2, n_iter=5, n_jobs=1,scoring = 'accuracy')\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train, callbacks=[callback])\n",
    "\n",
    "\n",
    "test_accuracy = grid.score(x_test, y_test)\n",
    "\n",
    "# Save and evaluate results\n",
    "s = ('Best Accuracy : {:.4f}\\n{}\\n\\n\\n')\n",
    "output_string = s.format(\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_)\n",
    "            \n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=31, trainable=True)\n",
    "model.add(e)\n",
    "model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(60, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "74/74 - 1s - loss: 0.5077 - accuracy: 0.7516 - val_loss: 0.4315 - val_accuracy: 0.8112\n",
      "Epoch 2/15\n",
      "74/74 - 1s - loss: 0.3732 - accuracy: 0.8348 - val_loss: 0.4110 - val_accuracy: 0.8289\n",
      "Epoch 3/15\n",
      "74/74 - 1s - loss: 0.2807 - accuracy: 0.8874 - val_loss: 0.4254 - val_accuracy: 0.8219\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e9752a50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "train_test_split(padded_docs, tweets['target'], test_size = 0.25, random_state = 123)\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 1, verbose=1)\n",
    "callbacks = [callback]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=76,\n",
    "          epochs=15,\n",
    "          verbose=2,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=31, trainable=True)\n",
    "model1.add(e)\n",
    "model1.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(60, activation='sigmoid'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "98/98 - 2s - loss: 0.4888 - accuracy: 0.7657\n",
      "Epoch 2/3\n",
      "98/98 - 2s - loss: 0.3660 - accuracy: 0.8446\n",
      "Epoch 3/3\n",
      "98/98 - 2s - loss: 0.2684 - accuracy: 0.8931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ffb6a650>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(padded_docs, tweets['target'],\n",
    "          batch_size=76,\n",
    "          epochs=3,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model1.predict(padded_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8230183 ],\n",
       "       [0.87129927],\n",
       "       [0.86996144],\n",
       "       ...,\n",
       "       [0.8257675 ],\n",
       "       [0.9024044 ],\n",
       "       [0.27452403]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = []\n",
    "\n",
    "for i in test_result:\n",
    "    if i >= 0.5 :\n",
    "        submit.append(1)\n",
    "    else:\n",
    "        submit.append(0)\n",
    "\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests['target'] = submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = tests[['id', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows ร 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('submit_prueba_37.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
