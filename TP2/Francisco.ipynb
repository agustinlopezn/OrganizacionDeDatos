{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fichur Inginierin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)\n",
    "\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = test[['id','text']]\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text'].str.split()\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test_metrics['length'] = test['text'].apply(lambda x: len(x))\n",
    "test_metrics['avg_word_length'] = test_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test_metrics['amount_of_words'] = test_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = test_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test_metrics['sentiment'] = test_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test_metrics['stopwords_count'] = test_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test_metrics['punctuation_count'] = test_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test_metrics['longest_word_length_without_stopwords'] = test_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test_metrics['stopword_word_ratio'] = test_metrics['stopwords_count'] / test_metrics['amount_of_words']\n",
    "\n",
    "test_metrics['adjectives_count'] = test_metrics['text'].apply(get_adjectives)\n",
    "test_metrics['nouns_count'] = test_metrics['text'].apply(get_nouns)\n",
    "test_metrics['verbs_count'] = test_metrics['text'].apply(get_verbs)\n",
    "test_metrics['adverbs_count'] = test_metrics['text'].apply(get_adverbs)\n",
    "\n",
    "test_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_metrics['text'] = tweets_metrics['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(lambda x: x.lower())\n",
    "tweets_metrics['text'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(remove_stopword)\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(stemm)\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics['text'] = test_metrics['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "test_metrics['text'] = test_metrics['text'].apply(lambda x: x.lower())\n",
    "test_metrics['text'] = test_metrics['text'].str.split()\n",
    "test_metrics['text'] = test_metrics['text'].apply(remove_stopword)\n",
    "test_metrics['text'] = test_metrics['text'].apply(stemm)\n",
    "test_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional, Concatenate, Flatten, Attention\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_len = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#layer = LSTM(256,return_sequences=True)(layer)\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = Bidirectional(LSTM(256,return_sequences=True))(layer)\n",
    "    layer = Bidirectional(LSTM(4))(layer)\n",
    "    layer = Dense(64)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1)(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "model = RNN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(vocab_size,embedding_dim,embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=True))\n",
    "    model.add(layers.Conv3D(128, 7, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling2D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Features - 75% del set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multiples parametros - Entrenar con 75% del set\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "X_train = tweets_metrics.iloc[:,4:]\n",
    "X_train[\"text\"] = tweets_metrics[\"text\"]\n",
    "\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train[\"text\"])\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train[\"text\"])\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test[\"text\"])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "features = StandardScaler()\n",
    "X_train_features = features.fit_transform(X_train.iloc[:,:-1])\n",
    "X_test_features = features.transform(X_test.iloc[:,:-1])\n",
    "\n",
    "#Guarda el mejor\n",
    "weight_path=\"Checkpoints/LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = RNN()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit([sequences_matrix,X_train_features],Y_train,batch_size=24,epochs=10,validation_split=0.2,callbacks=callbacks,verbose=1)\n",
    "\n",
    "#Carga el mejor y evalua\n",
    "model.load_weights(weight_path)\n",
    "accr = model.evaluate([test_sequences_matrix,X_test_features],Y_test)\n",
    "\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin features - 75% del set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sin features\n",
    "#Preparar datos para test sin features\n",
    "X_train = tweets_metrics.text\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "#Comentar para generar submit\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "#Guarda el mejor\n",
    "weight_path=\"Checkpoints/LSTM_No_Features.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = RNN()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(sequences_matrix,Y_train,batch_size=71,epochs=10,validation_split=0.2,callbacks=callbacks,verbose=1)\n",
    "\n",
    "#Carga el mejor y evalua\n",
    "model.load_weights(weight_path)\n",
    "\n",
    "#Comentar para generar submit - Sin features\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - TEST.csv (CON  features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multiples parametros - Entrenar con todo el set\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train = tweets_metrics.iloc[:,4:]\n",
    "X_train[\"text\"] = tweets_metrics[\"text\"]\n",
    "\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "X_test = test_metrics.iloc[:,3:]\n",
    "X_test[\"text\"] = test_metrics[\"text\"]\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train[\"text\"])\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train[\"text\"])\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test[\"text\"])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "features = StandardScaler()\n",
    "X_train_features = features.fit_transform(X_train.iloc[:,:-1])\n",
    "X_test_features = features.transform(X_test.iloc[:,:-1])\n",
    "model = RNN()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit([sequences_matrix,X_train_features],Y_train,batch_size=24,epochs=1,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss')],verbose=1)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_metrics['id']\n",
    "submission['prob'] = model.predict(test_sequences_matrix)\n",
    "submission['target'] = submission['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission[\"prob\"]\n",
    "submission.head(10)\n",
    "submission.to_csv(\"submit_prueba_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - TEST.csv (no tiene features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tweets_test.text\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = tweets_test['id']\n",
    "submission['prob'] = model.predict(test_sequences_matrix)\n",
    "submission['target'] = submission['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission1[\"prob\"]\n",
    "submission.head(10)\n",
    "submission.to_csv(\"submit_prueba_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONV2D - Todo de MAURO.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multiples parametros - Entrenar con 75% del set\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Concatenate, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Lambda\n",
    "from keras import layers, Input, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#basic_features = tweets_metrics[['length','avg_word_length','amount_of_words','amount_of_unique_words','sentiment','stopwords_count','punctuation_count','mentions_count','hashtags_count','longest_word_length_without_stopwords','stopword_word_ratio','adjectives_count','nouns_count','verbs_count','adverbs_count']]\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "features = StandardScaler()\n",
    "#x_train_features = features.fit_transform(basic_features)\n",
    "\n",
    "x_train = tweets_metrics['text'].values\n",
    "y_train = tweets_metrics['target'].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.twitter.27B.100d.txt',tokenizer.word_index, embedding_dim)\n",
    "\n",
    "#Guarda el mejor\n",
    "weight_path=\"Checkpoints/CONV2D.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = conv2d(vocab_size,embedding_dim,embedding_matrix)\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,epochs=15,validation_split=0.2,batch_size=88,callbacks=callbacks,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE PARA TODOS LOS METODOS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def tf_idf(): \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    x_train = tweets_metrics.text\n",
    "    y_train = tweets_metrics.target\n",
    "\n",
    "    #Comentar el split si se quiere generar el csv:\n",
    "\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.20)\n",
    "    #x_test = test_metrics.text\n",
    "\n",
    "    model = LogisticRegression(solver=\"newton-cg\")\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", smooth_idf = False)\n",
    "    vectorizer.fit(x_train)\n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = tf_idf()\n",
    "iteracion = 0\n",
    "while score < 0.82:\n",
    "    score = tf_idf()\n",
    "    print (f\"{iteracion}:{score}\")\n",
    "    iteracion += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_metrics.text\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_metrics['id']\n",
    "submission['prob'] = model.predict(x_test)\n",
    "submission['target'] = submission['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission[\"prob\"]\n",
    "submission.head(10)\n",
    "submission.to_csv(\"submit_prueba_16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def h_vec():   \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    x_train = tweets_metrics.text\n",
    "    y_train = tweets_metrics.target\n",
    "\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.15)\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "    vectorizer = HashingVectorizer(analyzer=\"word\",n_features=60000)\n",
    "    vectorizer.fit(x_train)\n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "\n",
    "    #print(\"Presicion:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = h_vec()\n",
    "iteracion = 0\n",
    "while score < 0.825:\n",
    "    score = h_vec()\n",
    "    iteracion += 1\n",
    "    print (f\"{iteracion}:{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_metrics.text\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_metrics['id']\n",
    "submission['prob'] = model.predict(x_test)\n",
    "submission['target'] = submission['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission[\"prob\"]\n",
    "submission.head(10)\n",
    "submission.to_csv(\"submit_prueba_17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "X_train = tweets_metrics.text\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "#Comentar para generar submit\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "X_train = tweets_metrics.text\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "#Comentar para generar submit\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", smooth_idf = False)\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "#Ejecuta el fit\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def h_vec():\n",
    "    X_train = tweets_metrics.text\n",
    "    Y_train = tweets_metrics.target\n",
    "    le = LabelEncoder()\n",
    "    Y_train = le.fit_transform(Y_train)\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "    #Comentar para generar submit\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "    vectorizer = HashingVectorizer(analyzer=\"word\",n_features=800000)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "    #Ejecuta el fit\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    model.fit(X_train,Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    return metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = h_vec()\n",
    "iteracion = 0\n",
    "while score < 0.815:\n",
    "    score = h_vec()\n",
    "    iteracion += 1\n",
    "    print (f\"{iteracion}:{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(analyzer=\"word\",n_features=800000)\n",
    "x_test = test_metrics.text\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_metrics['id']\n",
    "submission['prob'] = model.predict(x_test)\n",
    "submission['target'] = submission['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission[\"prob\"]\n",
    "submission.head(10)\n",
    "submission.to_csv(\"submit_prueba_39.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1596041200971"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}