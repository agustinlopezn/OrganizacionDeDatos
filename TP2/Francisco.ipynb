{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import xgboost as xgb\n",
    "import io\n",
    "import nltk\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"JJ\")])\n",
    "\n",
    "def get_nouns(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"NN\")])\n",
    "\n",
    "def get_verbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"VB\")])\n",
    "\n",
    "def get_adverbs(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len([word for (word,tag) in blob.tags if tag.startswith(\"RB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", usecols=['id','text', 'target'])\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7434 entries, 0 to 7612\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      7434 non-null   int64 \n 1   text    7434 non-null   object\n 2   target  7434 non-null   int64 \ndtypes: int64(2), object(1)\nmemory usage: 232.3+ KB\n"
    }
   ],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fichur Inginierin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  target  \\\n0   1  Our Deeds are the Reason of this #earthquake M...       1   \n1   4             Forest fire near La Ronge Sask. Canada       1   \n2   5  All residents asked to 'shelter in place' are ...       1   \n3   6  13,000 people receive #wildfires evacuation or...       1   \n4   7  Just got sent this photo from Ruby #Alaska as ...       1   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0              Our Deeds Reason May ALLAH Forgive us      69         4.384615   \n1                   Forest fire near La Ronge Canada      38         4.571429   \n2  All residents asked notified No evacuation she...     133         5.090909   \n3        people receive evacuation orders California      65         7.125000   \n4        Just got sent photo Ruby smoke pours school      88         4.500000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0               13                      13     0.2732                6   \n1                7                       7    -0.3400                0   \n2               22                      20    -0.2960               11   \n3                8                       8     0.0000                1   \n4               16                      15     0.0000                7   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  1               0               1   \n1                  1               0               0   \n2                  3               0               0   \n3                  2               0               1   \n4                  2               0               2   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      7             0.461538   \n1                                      6             0.000000   \n2                                     10             0.500000   \n3                                     10             0.125000   \n4                                      6             0.437500   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 0            6            1              0  \n1                 0            6            0              0  \n2                 1            7            7              0  \n3                 1            4            1              0  \n4                 0            6            3              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>Our Deeds Reason May ALLAH Forgive us</td>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.461538</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Canada</td>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>All residents asked notified No evacuation she...</td>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>20</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>people receive evacuation orders California</td>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.125000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>Just got sent photo Ruby smoke pours school</td>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.437500</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tweets_metrics = tweets[['id','text','target']]\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "tweets_metrics['length'] = tweets_metrics['text'].apply(lambda x: len(x))\n",
    "tweets_metrics['avg_word_length'] = tweets_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_metrics['amount_of_words'] = tweets_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = tweets_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_metrics['sentiment'] = tweets_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_metrics['stopwords_count'] = tweets_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "tweets_metrics['punctuation_count'] = tweets_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = tweets_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "tweets_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = tweets_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "tweets_metrics['longest_word_length_without_stopwords'] = tweets_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "tweets_metrics['stopword_word_ratio'] = tweets_metrics['stopwords_count'] / tweets_metrics['amount_of_words']\n",
    "\n",
    "tweets_metrics['adjectives_count'] = tweets_metrics['text'].apply(get_adjectives)\n",
    "tweets_metrics['nouns_count'] = tweets_metrics['text'].apply(get_nouns)\n",
    "tweets_metrics['verbs_count'] = tweets_metrics['text'].apply(get_verbs)\n",
    "tweets_metrics['adverbs_count'] = tweets_metrics['text'].apply(get_adverbs)\n",
    "\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  \\\n0   0                 Just happened a terrible car crash   \n1   2  Heard about #earthquake is different cities, s...   \n2   3  there is a forest fire at spot pond, geese are...   \n3   9           Apocalypse lighting. #Spokane #wildfires   \n4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0                   Just happened terrible car crash      34         4.833333   \n1                          Heard different stay safe      64         6.222222   \n2  forest fire spot geese fleeing across I cannot...      96         4.105263   \n3                                         Apocalypse      40         9.250000   \n4                Typhoon Soudelor kills China Taiwan      45         4.750000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0                6                       6    -0.7003                2   \n1                9                       9     0.4404                2   \n2               19                      19    -0.6159                9   \n3                4                       4     0.0000                0   \n4                8                       8    -0.5423                2   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  0               0               0   \n1                  3               0               1   \n2                  2               0               0   \n3                  3               0               2   \n4                  0               0               0   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      8             0.333333   \n1                                      9             0.222222   \n2                                      7             0.473684   \n3                                     10             0.000000   \n4                                      8             0.250000   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 1            2            1              1  \n1                 2            4            2              0  \n2                 2            4            4              1  \n3                 0            4            0              0  \n4                 0            4            1              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n      <td>Just happened terrible car crash</td>\n      <td>34</td>\n      <td>4.833333</td>\n      <td>6</td>\n      <td>6</td>\n      <td>-0.7003</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td>Heard different stay safe</td>\n      <td>64</td>\n      <td>6.222222</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0.4404</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0.222222</td>\n      <td>2</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td>forest fire spot geese fleeing across I cannot...</td>\n      <td>96</td>\n      <td>4.105263</td>\n      <td>19</td>\n      <td>19</td>\n      <td>-0.6159</td>\n      <td>9</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0.473684</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td>Apocalypse</td>\n      <td>40</td>\n      <td>9.250000</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td>Typhoon Soudelor kills China Taiwan</td>\n      <td>45</td>\n      <td>4.750000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>-0.5423</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.250000</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "test_metrics = test[['id','text']]\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text'].str.split()\n",
    "test_metrics['text_without_stopwords'] = test_metrics['text_without_stopwords'].apply(remove_stopword)\n",
    "\n",
    "test_metrics['length'] = test['text'].apply(lambda x: len(x))\n",
    "test_metrics['avg_word_length'] = test_metrics['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "test_metrics['amount_of_words'] = test_metrics['text'].str.split().transform(lambda x: len(x))\n",
    "unique_words_by_tweet = test_metrics['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "test_metrics['amount_of_unique_words'] = unique_words_by_tweet\n",
    "test_metrics['sentiment'] = test_metrics['text'].apply(lambda x: return_sia_compound_values(x))\n",
    "test_metrics['stopwords_count'] = test_metrics['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords]))\n",
    "test_metrics['punctuation_count'] = test_metrics['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "mentions = test_metrics['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "test_metrics['mentions_count'] = mentions['text'].apply(lambda x: len(x))\n",
    "hashtags = test_metrics['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "test_metrics['hashtags_count'] = hashtags.apply(lambda x: len(x))\n",
    "test_metrics['longest_word_length_without_stopwords'] = test_metrics['text_without_stopwords'].apply(lambda x: ([len(word) for word in str(x).lower().split() if not word.startswith('http')])).apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "test_metrics['stopword_word_ratio'] = test_metrics['stopwords_count'] / test_metrics['amount_of_words']\n",
    "\n",
    "test_metrics['adjectives_count'] = test_metrics['text'].apply(get_adjectives)\n",
    "test_metrics['nouns_count'] = test_metrics['text'].apply(get_nouns)\n",
    "test_metrics['verbs_count'] = test_metrics['text'].apply(get_verbs)\n",
    "test_metrics['adverbs_count'] = test_metrics['text'].apply(get_adverbs)\n",
    "\n",
    "test_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def remove_stopword(text):\n",
    "    new_text = []\n",
    "    for e in text:\n",
    "        if e not in stopwords and e.isalpha():\n",
    "            new_text.append(e)\n",
    "    text = new_text\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stemm(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                               text  target  \\\n0   1          deed reason earthquak may allah forgiv us       1   \n1   4               forest fire near la rong sask canada       1   \n2   5  resid ask shelter place notifi offic evacu she...       1   \n3   6        peopl receiv wildfir evacu order california       1   \n4   7  got sent photo rubi alaska smoke wildfir pour ...       1   \n\n                              text_without_stopwords  length  avg_word_length  \\\n0              Our Deeds Reason May ALLAH Forgive us      69         4.384615   \n1                   Forest fire near La Ronge Canada      38         4.571429   \n2  All residents asked notified No evacuation she...     133         5.090909   \n3        people receive evacuation orders California      65         7.125000   \n4        Just got sent photo Ruby smoke pours school      88         4.500000   \n\n   amount_of_words  amount_of_unique_words  sentiment  stopwords_count  \\\n0               13                      13     0.2732                6   \n1                7                       7    -0.3400                0   \n2               22                      20    -0.2960               11   \n3                8                       8     0.0000                1   \n4               16                      15     0.0000                7   \n\n   punctuation_count  mentions_count  hashtags_count  \\\n0                  1               0               1   \n1                  1               0               0   \n2                  3               0               0   \n3                  2               0               1   \n4                  2               0               2   \n\n   longest_word_length_without_stopwords  stopword_word_ratio  \\\n0                                      7             0.461538   \n1                                      6             0.000000   \n2                                     10             0.500000   \n3                                     10             0.125000   \n4                                      6             0.437500   \n\n   adjectives_count  nouns_count  verbs_count  adverbs_count  \n0                 0            6            1              0  \n1                 0            6            0              0  \n2                 1            7            7              0  \n3                 1            4            1              0  \n4                 0            6            3              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_without_stopwords</th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>deed reason earthquak may allah forgiv us</td>\n      <td>1</td>\n      <td>Our Deeds Reason May ALLAH Forgive us</td>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.461538</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>forest fire near la rong sask canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Canada</td>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>resid ask shelter place notifi offic evacu she...</td>\n      <td>1</td>\n      <td>All residents asked notified No evacuation she...</td>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>20</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>peopl receiv wildfir evacu order california</td>\n      <td>1</td>\n      <td>people receive evacuation orders California</td>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.125000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n      <td>1</td>\n      <td>Just got sent photo Ruby smoke pours school</td>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.437500</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tweets_metrics['text'] = tweets_metrics['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(lambda x: x.lower())\n",
    "tweets_metrics['text'] = tweets_metrics['text'].str.split()\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(remove_stopword)\n",
    "tweets_metrics['text'] = tweets_metrics['text'].apply(stemm)\n",
    "tweets_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      length  avg_word_length  amount_of_words  amount_of_unique_words  \\\n0         69         4.384615               13                      13   \n1         38         4.571429                7                       7   \n2        133         5.090909               22                      20   \n3         65         7.125000                8                       8   \n4         88         4.500000               16                      15   \n...      ...              ...              ...                     ...   \n7604     136         6.210526               19                      19   \n7605     114         3.423077               26                      25   \n7606     121         5.100000               20                      18   \n7608      83         6.636364               11                      11   \n7612      94         6.307692               13                      13   \n\n      sentiment  stopwords_count  punctuation_count  mentions_count  \\\n0        0.2732                6                  1               0   \n1       -0.3400                0                  1               0   \n2       -0.2960               11                  3               0   \n3        0.0000                1                  2               0   \n4        0.0000                7                  2               0   \n...         ...              ...                ...             ...   \n7604    -0.6841                6                 12               0   \n7605    -0.4939               16                  1               0   \n7606    -0.7650                1                 11               0   \n7608    -0.4939                2                  5               0   \n7612     0.0000                3                  7               0   \n\n      hashtags_count  longest_word_length_without_stopwords  \\\n0                  1                                      7   \n1                  0                                      6   \n2                  0                                     10   \n3                  1                                     10   \n4                  2                                      6   \n...              ...                                    ...   \n7604               1                                     10   \n7605               0                                      8   \n7606               0                                      8   \n7608               0                                      8   \n7612               0                                     10   \n\n      stopword_word_ratio  adjectives_count  nouns_count  verbs_count  \\\n0                0.461538                 0            6            1   \n1                0.000000                 0            6            0   \n2                0.500000                 1            7            7   \n3                0.125000                 1            4            1   \n4                0.437500                 0            6            3   \n...                   ...               ...          ...          ...   \n7604             0.315789                 0           13            3   \n7605             0.615385                 2            4            5   \n7606             0.050000                 0           14            0   \n7608             0.181818                 2            6            1   \n7612             0.230769                 2            8            1   \n\n      adverbs_count  \n0                 0  \n1                 0  \n2                 0  \n3                 0  \n4                 1  \n...             ...  \n7604              0  \n7605              3  \n7606              0  \n7608              0  \n7612              0  \n\n[7434 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>avg_word_length</th>\n      <th>amount_of_words</th>\n      <th>amount_of_unique_words</th>\n      <th>sentiment</th>\n      <th>stopwords_count</th>\n      <th>punctuation_count</th>\n      <th>mentions_count</th>\n      <th>hashtags_count</th>\n      <th>longest_word_length_without_stopwords</th>\n      <th>stopword_word_ratio</th>\n      <th>adjectives_count</th>\n      <th>nouns_count</th>\n      <th>verbs_count</th>\n      <th>adverbs_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69</td>\n      <td>4.384615</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.2732</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.461538</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>4.571429</td>\n      <td>7</td>\n      <td>7</td>\n      <td>-0.3400</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>133</td>\n      <td>5.090909</td>\n      <td>22</td>\n      <td>20</td>\n      <td>-0.2960</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.500000</td>\n      <td>1</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>65</td>\n      <td>7.125000</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.125000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>88</td>\n      <td>4.500000</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.0000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.437500</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7604</th>\n      <td>136</td>\n      <td>6.210526</td>\n      <td>19</td>\n      <td>19</td>\n      <td>-0.6841</td>\n      <td>6</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.315789</td>\n      <td>0</td>\n      <td>13</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7605</th>\n      <td>114</td>\n      <td>3.423077</td>\n      <td>26</td>\n      <td>25</td>\n      <td>-0.4939</td>\n      <td>16</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.615385</td>\n      <td>2</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7606</th>\n      <td>121</td>\n      <td>5.100000</td>\n      <td>20</td>\n      <td>18</td>\n      <td>-0.7650</td>\n      <td>1</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.050000</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>83</td>\n      <td>6.636364</td>\n      <td>11</td>\n      <td>11</td>\n      <td>-0.4939</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0.181818</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>94</td>\n      <td>6.307692</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0.0000</td>\n      <td>3</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.230769</td>\n      <td>2</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7434 rows × 15 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "tweets_metrics.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_train = tweets_metrics.text\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comentar para generar submit\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional, Concatenate, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer = LSTM(256,return_sequences=True)(layer)\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = Bidirectional(LSTM(16,return_sequences=True))(layer)\n",
    "    layer = Bidirectional(LSTM(4))(layer)\n",
    "    layer = Dense(64)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1)(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_34\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninputs (InputLayer)          [(None, 100)]             0         \n_________________________________________________________________\nembedding_34 (Embedding)     (None, 100, 50)           500000    \n_________________________________________________________________\nbidirectional_56 (Bidirectio (None, 100, 1024)         2306048   \n_________________________________________________________________\ndense_66 (Dense)             (None, 100, 64)           65600     \n_________________________________________________________________\nactivation_62 (Activation)   (None, 100, 64)           0         \n_________________________________________________________________\ndropout_30 (Dropout)         (None, 100, 64)           0         \n_________________________________________________________________\ndense_67 (Dense)             (None, 100, 1)            65        \n_________________________________________________________________\nactivation_63 (Activation)   (None, 100, 1)            0         \n=================================================================\nTotal params: 2,871,713\nTrainable params: 2,871,713\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sin features\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(sequences_matrix,Y_train,batch_size=71,epochs=10,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss')])\n",
    "\n",
    "#Comentar para generar submit - Sin features\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n186/186 [==============================] - 10s 53ms/step - loss: 0.5963 - accuracy: 0.6740 - val_loss: 0.5010 - val_accuracy: 0.7740\nEpoch 2/10\n186/186 [==============================] - 9s 49ms/step - loss: 0.3246 - accuracy: 0.8778 - val_loss: 0.5129 - val_accuracy: 0.7857\n59/59 [==============================] - 1s 9ms/step - loss: 0.4491 - accuracy: 0.8085\nTest set\n  Loss: 0.449\n  Accuracy: 0.808\n"
    }
   ],
   "source": [
    "#Multiples parametros\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "X_train = tweets_metrics.iloc[:,4:]\n",
    "X_train[\"text\"] = tweets_metrics[\"text\"]\n",
    "\n",
    "Y_train = tweets_metrics.target\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,Y_train,test_size=0.25)\n",
    "\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train[\"text\"])\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train[\"text\"])\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test[\"text\"])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "features = StandardScaler()\n",
    "X_train_features = features.fit_transform(X_train.iloc[:,:-1])\n",
    "X_test_features = features.transform(X_test.iloc[:,:-1])\n",
    "model = RNN()\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit([sequences_matrix,X_train_features],Y_train,batch_size=24,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss')],verbose=1)\n",
    "\n",
    "\n",
    "accr = model.evaluate([test_sequences_matrix,X_test_features],Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - TEST.csv (no tiene features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics['text'] = test_metrics['text'].apply(lambda x: x.translate({ord(i): ' ' for i in string.punctuation}))\n",
    "test_metrics['text'] = test_metrics['text'].apply(lambda x: x.lower())\n",
    "test_metrics['text'] = test_metrics['text'].str.split()\n",
    "test_metrics['text'] = test_metrics['text'].apply(remove_stopword)\n",
    "test_metrics['text'] = test_metrics['text'].apply(stemm)\n",
    "test_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tweets_test.text\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission1['id'] = tweets_test['id']\n",
    "submission1['prob'] = model.predict(test_sequences_matrix)\n",
    "submission1['target'] = submission1['prob'].apply(lambda x: 0 if x < .5 else 1)\n",
    "del submission1[\"prob\"]\n",
    "submission1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(\"submit_prueba_7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE PARA TODOS LOS METODOS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model = LogisticRegression()    \n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", smooth_idf = True)\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train = tweets_metrics.text\n",
    "y_train = tweets_metrics.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.25)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(analyzer=\"word\")\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train)\n",
    "x_test  = vectorizer.transform(x_test)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Presicion:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1594358874725"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}