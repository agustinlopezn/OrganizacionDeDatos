{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75.06/95.58 Organización de Datos: Trabajo Práctico 1\n",
    "### Primer Cuatrimestre de 2020 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import string\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n",
    "\n",
    "tweets = pd.read_csv('train.csv') \n",
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['location'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = tweets.duplicated(subset = 'text', keep = False)\n",
    "duplicates.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de reales vs falsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tweets['target'].value_counts(normalize=True).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.columns = ['target', 'total']\n",
    "dist['total'] = dist['total'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.barplot(x=\"target\", y=\"total\", data=dist)\n",
    "ax.set_title('Porcentaje de tweets en el dataset: reales vs falsos\\n', fontsize=25)\n",
    "ax.set_ylabel('Porcentaje de tweets', fontsize=20)\n",
    "ax.set_xlabel('Veracidad. Real = 1; Falso = 0.', fontsize=20)\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desastres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 50 desastres comentados en los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desastres = tweets.groupby(\"keyword\").agg({\"target\":[\"count\",\"mean\"]})\n",
    "level0 = desastres.columns.get_level_values(0)\n",
    "level1 = desastres.columns.get_level_values(1)\n",
    "desastres.columns = level0 + \"_\" + level1\n",
    "desastres.sort_values(by=\"target_count\",ascending = False,inplace = True)\n",
    "desastres = desastres.head(50) #TOP 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "grafico = sns.barplot(data = desastres,x = \"target_count\",y = desastres.index)\n",
    "grafico.set_title(\"Top 50 desastres comentados en tweets\",fontsize = 25)\n",
    "grafico.set_xlabel(\"Cantidad de veces mencionado\",fontsize = 20)\n",
    "grafico.set_ylabel(\"Desastre\",fontsize = 20)\n",
    "grafico.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 50 desastres comentados en los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desastres = tweets.groupby(\"keyword\").agg({\"target\":[\"count\",\"mean\"]}).copy()\n",
    "level0 = desastres.columns.get_level_values(0)\n",
    "level1 = desastres.columns.get_level_values(1)\n",
    "desastres.columns = level0 + \"_\" + level1\n",
    "desastres.sort_values(by=\"target_count\",ascending = False,inplace = True)\n",
    "desastres = desastres.head(50) #TOP 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "grafico = sns.barplot(data = desastres,x = \"target_count\",y = desastres.index)\n",
    "grafico.set_title(\"Top 50 desastres comentados en tweets\",fontsize = 25)\n",
    "grafico.set_xlabel(\"Cantidad de veces mencionado\",fontsize = 20)\n",
    "grafico.set_ylabel(\"Desastre\",fontsize = 20)\n",
    "grafico.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top palabras con más apariciones en tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_word_count(dataframe):\n",
    "    word_list = dataframe[\"text\"].str.split()\n",
    "    all_stopwords_gensim = STOPWORDS.union(set(string.punctuation))\n",
    "    filtered = [word.lower() for word in np.concatenate(word_list.values) if not word in all_stopwords_gensim]\n",
    "    words = pd.DataFrame(filtered,columns = [\"word\"])\n",
    "    words[\"amount\"] = 1\n",
    "    by_count = words.groupby([\"word\"]).agg({\"amount\":[\"count\"]})\n",
    "    level0 = by_count.columns.get_level_values(0)\n",
    "    level1 = by_count.columns.get_level_values(1)\n",
    "    by_count.columns = level0 + \"_\" + level1\n",
    "    by_count.sort_values(by=\"amount_count\",ascending = False,inplace = True)\n",
    "    return by_count.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_by_word = tweets[\"target\"] == 1\n",
    "true_by_word = by_word_count(tweets[true_by_word])\n",
    "false_by_word = tweets[\"target\"] == 0\n",
    "false_by_word = by_word_count(tweets[false_by_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "grafico = sns.barplot(data = true_by_word,x = \"amount_count\",y=true_by_word.index)\n",
    "grafico.set_title(\"Top 50 palabras usadas en tweets verdaderos\",fontsize = 25)\n",
    "grafico.set_xlabel(\"Cantidad de apariciones\",fontsize = 20)\n",
    "grafico.set_ylabel(\"Palabra\",fontsize = 20)\n",
    "grafico.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "grafico = sns.barplot(data = false_by_word,x = \"amount_count\",y=false_by_word.index)\n",
    "grafico.set_title(\"Top 50 palabras usadas en tweets falsos\",fontsize = 25)\n",
    "grafico.set_xlabel(\"Cantidad de apariciones\",fontsize = 20)\n",
    "grafico.set_ylabel(\"Palabra\",fontsize = 20)\n",
    "grafico.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitud de tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length = tweets[['text','target']]\n",
    "tweets_length.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['text'].hasnans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['length'] = tweets_length['text'].str.len()\n",
    "tweets_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,8))\n",
    "tweet_len = tweets_length[tweets_length['target'] == 1]['length']\n",
    "ax1.hist(tweet_len,color='green')\n",
    "ax1.set_title('Tweets reales')\n",
    "tweet_len = tweets_length[tweets_length['target'] == 0]['length']\n",
    "ax2.hist(tweet_len,color='red')\n",
    "ax2.set_title('Tweets falsos')\n",
    "fig.suptitle('Comportamiento de la longitud del tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.boxplot(x = 'target', y = 'length', data = tweets_length, width = 0.4,\\\n",
    "                palette = [(0.86, 0.3712, 0.33999999999999997),(0.30196078431372547, 0.6862745098039216, 0.2901960784313726)])\n",
    "g.set_title(\"Largo de los tweets según target\", fontsize = 25)\n",
    "g.set_xlabel(\"Veracidad (0 = Falso, 1 = Verdadero)\", fontsize = 20)\n",
    "g.set_ylabel(\"Largo de los tweets (en caracteres)\", fontsize = 20)\n",
    "sns.set(rc = {'figure.figsize' : (15,10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['avg_word_length'] = tweets_length['text'].str.split().apply(lambda x: [len(y) for y in x]).transform(lambda x: np.mean(x))\n",
    "tweets_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(tweets_length, x=\"avg_word_length\", color=\"target\", marginal=\"box\", width = 1200, height = 550,\n",
    "                             nbins = 300, title = 'Distribución de largo promedio de palabras por tweet individual')\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Largo promedio de la palabra\",\n",
    "    yaxis_title=\"Cantidad\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=15,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.update_layout(legend_title_text=\"Veracidad\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['amount_of_words'] = tweets_length['text'].str.split().transform(lambda x: len(x))\n",
    "tweets_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.kdeplot(tweets_length[tweets_length['target'] == 1]['amount_of_words'], color=\"blue\", label='Cant de palabras por tweet Target = 1', shade=True)\n",
    "g = sns.kdeplot(tweets_length[tweets_length['target'] == 0]['amount_of_words'], color=\"red\", label='Cant de palabras por tweet Target = 0', shade=True)\n",
    "g.set_title('Cantidad de palabras por tweet según target', fontsize = 25)\n",
    "g.set_xlabel('Longitud en palabras del tweet', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de palabras únicas en el tweet\n",
    "unique_words_by_tweet = tweets_length['text'].transform(lambda x: x.split()).transform(lambda x: pd.Series(x).unique()).transform(lambda x: len(x))\n",
    "tweets_length['amount_of_unique_words'] = unique_words_by_tweet\n",
    "tweets_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.kdeplot(tweets_length[tweets_length['target'] == 1]['amount_of_unique_words'], color=\"blue\", label='Cant de palabras únicas por tweet Target = 1', shade=True)\n",
    "g = sns.kdeplot(tweets_length[tweets_length['target'] == 0]['amount_of_unique_words'], color=\"red\", label='Cant de palabras únicas por tweet Target = 0', shade=True)\n",
    "g.set_title('Cantidad de palabras únicas por tweet según target', fontsize = 25)\n",
    "g.set_xlabel('Longitud en palabras únicas del tweet', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['avg_length'] = tweets_length.groupby('target')['length'].transform('mean')\n",
    "tweets_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['is_above_avg_length'] = tweets_length['length'] > tweets_length['avg_length']\n",
    "tweets_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedby_is_above_avg_length = tweets_length.groupby('is_above_avg_length').agg({'target':['mean','count']})\n",
    "groupedby_is_above_avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedby_is_above_avg_length[('target','mean')].plot(kind = 'bar', figsize = (15,10), rot = 0, colormap = 'Accent')\n",
    "plt.title('Target promedio segun si sobrepasan el largo promedio', fontsize = 25)\n",
    "plt.xlabel('Está por arriba del largo promedio', fontsize = 20)\n",
    "plt.ylabel('Target promedio', fontsize = 20)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length['contains_link'] = tweets_length['text'].str.contains('http://' or 'https://')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_corr = tweets_length[['length','target','is_above_avg_length','avg_word_length','amount_of_words','amount_of_unique_words','contains_link']]\n",
    "tweets_to_corr['avg_target_by_length'] = tweets_length.groupby('length')['target'].transform('mean')\n",
    "tweets_to_corr.head(20)\n",
    "tweets_to_corr.corr(method = 'spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tweets_to_corr.corr(method = 'spearman'), annot = True)\n",
    "plt.title('Heatmap de correlación entre distintas columnas', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimiento de tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera hipotesis: Al ser cosas noticiosas deberian ser mas neutras.\n",
    "\n",
    "Pero, teniendo en cuenta que procesa el sentimiento.\n",
    "Deberian ser neutros tendiendo a negativos ya que al ser desastres naturales son literalmente eso, desastres, y se espera el uso de ciertas palabras que no implican algo muy positivo.\n",
    "\n",
    "Además, al informar sobre desastres naturales las palabras que describen este tipo de eventos apuntan en general a cosas más negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def return_sia_compound_values(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "tweets_feeling_1 = tweets_length.groupby('target')['text'].get_group(1).apply(lambda x: return_sia_compound_values(x))\n",
    "tweets_feeling_0 = tweets_length.groupby('target')['text'].get_group(0).apply(lambda x: return_sia_compound_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,8))\n",
    "\n",
    "sns.distplot(tweets_feeling_0, bins = 40, color = 'SkyBlue', ax = ax1)\n",
    "ax1.set_title('Sentimiento - Target = 0', fontsize = 25)\n",
    "ax1.set_xlabel('Sentimiento', fontsize = 20)\n",
    "\n",
    "sns.distplot(tweets_feeling_1, bins = 40, color = 'SkyBlue', ax = ax2)\n",
    "ax2.set_title('Sentimiento - Target = 1', fontsize = 25)\n",
    "ax2.set_xlabel('Sentimiento', fontsize = 20)\n",
    "\n",
    "ax1.set_ylim(0,6)\n",
    "ax2.set_ylim(0,6)\n",
    "fig.suptitle('Sentimiento del tweet. De -1 a 1, más negativo a más positivo.', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sia_values(text):\n",
    "    return [sia.polarity_scores(text)['pos'], sia.polarity_scores(text)['neg'], sia.polarity_scores(text)['neu']]\n",
    "\n",
    "sia_values = tweets_length['text'].transform(lambda x: return_sia_values(x)).to_frame()\n",
    "\n",
    "sia_values['pos'] = sia_values['text'].apply(lambda x: pd.Series(x[0]))\n",
    "sia_values['neg'] = sia_values['text'].apply(lambda x: pd.Series(x[1]))\n",
    "sia_values['neu'] = sia_values['text'].apply(lambda x: pd.Series(x[2]))\n",
    "\n",
    "sia_values['target'] = tweets_length['target']\n",
    "sia_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sia_values.groupby('target').mean()[['pos', 'neg', 'neu']].plot.area(cmap='Pastel2', figsize=(10, 8))\n",
    "ax.set_xlabel('Veracidad. 0: Falso, 1: Verdadero.')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], ['Sentimiento Neutro', 'Sentimiento Negativo', 'Sentimiento Positivo'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords_gensim = STOPWORDS.union(set(string.punctuation))\n",
    "tweets_by_stopwords = tweets[['target', 'text']]\n",
    "tweets_by_stopwords['stopwords_count'] = tweets_by_stopwords['text']\\\n",
    ".apply(lambda x: len([word for word in str(x).lower().split() if word in all_stopwords_gensim]))\n",
    "tweets_by_stopwords.drop(columns='text', inplace=True)\n",
    "tweets_by_stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_by_stopwords = tweets_by_stopwords.groupby('target').agg({'stopwords_count' : 'sum'})\n",
    "tweets_by_stopwords.reset_index(inplace = True)\n",
    "tweets_by_stopwords.loc[0,'target'] = \"False\"\n",
    "tweets_by_stopwords.loc[1, 'target'] = \"True\"\n",
    "tweets_by_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x = 'target', y = 'stopwords_count', data = tweets_by_stopwords, palette = \"Set2\")\n",
    "g.set_title(\"Cantidad de stopwords segun target\", fontsize = 25)\n",
    "g.set_xlabel(\"Veracidad\", fontsize = 15)\n",
    "g.set_ylabel(\"Cantidad de stopwords\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_punctuation = tweets[['text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for character in text:\n",
    "        if character in punctuation:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def amount_of_punctuation(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    amount = 0\n",
    "    for character in text:\n",
    "        if character in punctuation: amount += 1\n",
    "    return amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_punctuation['contains_punctuation'] = tweets_punctuation['text'].apply(lambda x: contains_punctuation(x))\n",
    "tweets_punctuation['amount_of_punctuation'] = tweets_punctuation['text'].apply(lambda x: amount_of_punctuation(x))\n",
    "tweets_punctuation = tweets_punctuation[(tweets_punctuation['amount_of_punctuation']) < (tweets_punctuation['amount_of_punctuation'].std() * 10)]\n",
    "tweets_punctuation.groupby('target')['amount_of_punctuation'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x  =\"target\", y = \"amount_of_punctuation\", data = tweets_punctuation, palette = [(0.8, 0.7254901960784313, 0.4549019607843137),\n",
    " (0.39215686274509803, 0.7098039215686275, 0.803921568627451)])\n",
    "\n",
    "ax.set_title('Cantidad de caracteres de puntuación utilizados en un tweet por target', fontsize = 25)\n",
    "ax.set_xlabel('Veracidad. Verdadero: target = 1; Falso: target = 0 ', fontsize = 20)\n",
    "ax.set_ylabel('Cantidad de caracteres de puntuación utilizados en el tweet', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud entre tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = tweets.groupby(\"target\").get_group(0)\n",
    "real = tweets.groupby(\"target\").get_group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash,MinHashLSH\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    return tokens\n",
    "\n",
    "def get_minhash(data):\n",
    "    minhash = []\n",
    "    for row in data.iterrows():\n",
    "        text = row[1][3]\n",
    "        id = row[1][0]\n",
    "        tokens = preprocess(text)\n",
    "        m = MinHash(num_perm=512)\n",
    "        for s in tokens:\n",
    "            m.update(s.encode('utf8'))\n",
    "        minhash.append([id,m])\n",
    "    return minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_minhashs = get_minhash(fake)\n",
    "real_minhashs = get_minhash(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(minhashs):\n",
    "    total = []\n",
    "    for i in range(1,100):\n",
    "        lsh = MinHashLSH(threshold=i/100, num_perm=512)\n",
    "        for minhash in minhashs:\n",
    "            lsh.insert(minhash[0],minhash[1])\n",
    "        values = []\n",
    "        for minhash in minhashs:\n",
    "            values.append(len(lsh.query(minhash[1])))\n",
    "        total.append(values)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold_fake = get_similar(fake_minhashs)\n",
    "treshold_real = get_similar(real_minhashs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_avg = []\n",
    "fake_avg = []\n",
    "indx = []\n",
    "\n",
    "for i in range(1,99): \n",
    "    real_avg.append(sum(treshold_real[i])//len(treshold_real[i]))\n",
    "    fake_avg.append(sum(treshold_fake[i])//len(treshold_fake[i]))\n",
    "    indx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_and_fake = pd.DataFrame(real_avg,index = indx)\n",
    "real_and_fake.columns = [\"AVG Match Real\"]\n",
    "real_and_fake[\"AVG Match Fake\"] = fake_avg\n",
    "real_and_fake.reset_index(inplace=True)\n",
    "real_and_fake = real_and_fake.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = real_and_fake.plot.scatter(x=\"AVG Match Real\",y=\"index\",color=\"r\",label=\"Reales\",figsize=(12,10))\n",
    "real_and_fake.plot.scatter(x=\"AVG Match Fake\",y=\"index\",color=\"b\",label=\"Falsos\",ax=ax)\n",
    "real_and_fake.plot(x=\"AVG Match Fake\",y=\"index\",color=\"b\",label=\"\",ax=ax)\n",
    "real_and_fake.plot(x=\"AVG Match Real\",y=\"index\",color=\"r\",label=\"\",ax=ax)\n",
    "plt.legend(loc='best')\n",
    "ax.set_title(\"Parecido entre tweets segun su similitud\",fontsize = 25)\n",
    "ax.set_xlabel(\"Promedio de tweets parecidos\",fontsize=20)\n",
    "ax.set_ylabel(\"Minimo porcentaje de similitud\",fontsize=20)\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texto: Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relacion: links en tweet vs. veracidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.countplot(x = 'target', hue = \"contains_link\", data = tweets_length, palette = [sns.color_palette()[3], sns.color_palette()[2]])\n",
    "\n",
    "g.set_title('Cantidad de tweets que poseen o no links según target', fontsize = 25)\n",
    "g.set_xlabel('Veracidad (verdadero o falso)', fontsize = 20)\n",
    "g.set_ylabel('Cantidad de tweets', fontsize = 20)\n",
    "plt.ylim(0,3000)\n",
    "sns.set(rc = {'figure.figsize' : (15,10)})\n",
    "\n",
    "g.legend(['No','Si'], loc = 'upper right', title = 'Contiene link');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length[tweets_length['contains_link']].groupby('target').agg({'length':['count', 'mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.boxplot(x = 'target', y = 'length', data = tweets_length[tweets_length['contains_link']], palette = [(0.596078431372549, 0.3058823529411765, 0.6392156862745098),\n",
    " (1.0, 0.4980392156862745, 0.0)], width = 0.4)\n",
    "g.set_title(\"Largo de los tweets que poseen links según target\", fontsize = 25)\n",
    "g.set_xlabel(\"Veracidad (0 = Falso, 1 = Verdadero)\", fontsize = 20)\n",
    "g.set_ylabel(\"Largo de los tweets (en caracteres)\", fontsize = 20)\n",
    "sns.set(rc = {'figure.figsize' : (15,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según twitter developer:\n",
    "When a HTTPS-based URL is passed while link wrapping is enabled, a HTTPS-based t.co link will be produced. HTTPS-based t.co links are one character longer than standard t.co links to account for the protocol change.\n",
    "\n",
    "Esto es porque los HTTPS son más seguros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_https = tweets_length[tweets_length['text'].str.contains('https://')].groupby('target')['text'].count()\n",
    "contains_http = tweets_length[tweets_length['text'].str.contains('http://')].groupby('target')['text'].count()\n",
    "contains_link = pd.DataFrame({'contains_safer_link': contains_https, 'contains_non_safer_link': contains_http})\n",
    "contains_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = contains_link.plot(color=[\"SkyBlue\",\"IndianRed\"], kind = 'bar', figsize = (15,10), rot = 0)\n",
    "graph.legend(['Contiene link más seguro','Contiene link menos seguro'], loc = 'upper right')\n",
    "plt.title('Cantidad de tweets que poseen links seguros o no seguros según target', fontsize = 25)\n",
    "plt.xlabel('Veracidad (0 = Falso, 1 = Verdadero)', fontsize = 20)\n",
    "plt.ylabel('Cantidad de tweets con links', fontsize = 20)\n",
    "plt.ylim(0,2250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texto: Menciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personas más mencionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_mentions = tweets.loc[tweets['text'].str.contains('@'), ['text', 'target']]\n",
    "mentions = tweets_with_mentions['text'].str.findall(r'@.\\S*?(?=\\s|[:]|$)').to_frame()\n",
    "#La regex encuentra todas las palabras que empiecen con '@' y al mismo tiempo elimina los '@:' y '@ '\n",
    "#El arroba se usa como expresion para indicar la hora tambien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions['target'] = tweets_with_mentions['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_to_group = mentions.explode('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_to_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_grouped = mentions_to_group.groupby(['text']).agg({'text': 'count', 'target':'sum'}).nlargest(20, 'text')\n",
    "mentions_grouped['fake'] = mentions_grouped['text'] - mentions_grouped['target']\n",
    "mentions_grouped.columns = ['total', 'real', 'fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_grouped.loc[:, 'total'].plot(kind='barh', color=(0.2, 0.2, 0.6, 0.8), figsize=(20, 15))\n",
    "ax = plt.gca()\n",
    "ax.set_title('Cuentas mas mencionadas\\n', fontsize=25)\n",
    "ax.set_xlabel('Cuenta', fontsize=20)\n",
    "ax.set_ylabel('Cantidad de menciones', fontsize=20)\n",
    "ax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mentions_grouped.sort_values(by=['fake']).loc[:, ['real', 'fake']].plot(kind='barh', figsize=(15, 15), colormap='tab20b')\n",
    "ax.set_title('Cuentas más mencionadas en tweets reales y falsos\\n', fontsize=25)\n",
    "ax.set_ylabel('Cuenta', fontsize=20)\n",
    "ax.set_xlabel('Cantidad de menciones', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.legend(labels=['Reales', 'falsos'], fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texto: Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Hastags más usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_ht = tweets.loc[tweets['text'].str.contains('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_ht['hashtags'] = tweets_with_ht['text'].str.findall(r'#[^?\\s].*?(?=\\s|$)')\n",
    "tweets_hashtags = tweets_with_ht.explode('hashtags')\n",
    "tweets_hashtags.dropna(subset = [\"hashtags\"],inplace = True)\n",
    "tweets_hashtags[\"hashtags\"] = tweets_hashtags[\"hashtags\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_top = tweets_hashtags.groupby('hashtags').agg({'target':['count', 'mean']})\n",
    "hashtags_top.sort_values(by=(\"target\", \"count\"), ascending = False, inplace = True)\n",
    "hashtags_top[(\"target\", \"mean\")] = (hashtags_top[(\"target\", \"mean\")] * 100).round()\n",
    "hashtags_top = hashtags_top.head(50).reset_index()\n",
    "hashtags_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 20))\n",
    "plot = sns.barplot(data = hashtags_top, x = (\"target\",\"count\"), y = 'hashtags', hue = ('target','mean'), dodge = False, palette = \"viridis_r\")\n",
    "plot.set_title(\"Top 50 Hashtags mas usados\", fontsize = 25)\n",
    "plot.set_xlabel(\"Cantidad de veces mencionado\", fontsize = 20)\n",
    "plot.set_ylabel(\"Hashtag\", fontsize = 20)\n",
    "plt.legend(title='Porcentaje veracidad', loc='lower right')\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags más usados en tweets reales y falsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_by_hashtags = tweets_hashtags.groupby(\"target\").get_group(1)\n",
    "false_by_hashtags = tweets_hashtags.groupby(\"target\").get_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "true_hashtags = ' '.join(true_by_hashtags[\"hashtags\"].str.lower())\n",
    "fake_hashtags = ' '.join(false_by_hashtags[\"hashtags\"].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_real_hashtags = true_by_hashtags.groupby('hashtags').agg({'target':'count'}).nlargest(50, columns='target')\n",
    "top_real_hashtags.columns = [\"real\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fake_hashtags = false_by_hashtags.groupby('hashtags').agg({'target':'count'}).nlargest(50, columns=\"target\")\n",
    "top_fake_hashtags.columns = [\"fake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_hashtags = pd.concat([top_real_hashtags,top_fake_hashtags],join=\"outer\",axis=1)\n",
    "all_hashtags.fillna(0,inplace=True)\n",
    "all_hashtags['total'] = all_hashtags['real'] + all_hashtags['fake']\n",
    "top_20_hashtags = all_hashtags.nlargest(40, \"total\")\n",
    "ax = top_20_hashtags.sort_values(by='total').loc[:, ['real', 'fake']].plot(kind='barh', figsize=(20, 20),colormap='tab20b')\n",
    "ax.set_title(\"Top 50 Hashtags mas usados segun veracidad\", fontsize = 25)\n",
    "ax.set_xlabel(\"Cantidad de veces mencionado\", fontsize = 20)\n",
    "ax.set_ylabel(\"Hashtag\", fontsize = 20)\n",
    "ax.legend([\"Real\",\"Falso\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import imageio\n",
    "\n",
    "twitter_coloring = imageio.imread(\"img/twitter.png\",pilmode='RGB')\n",
    "wc = WordCloud(width = 1920,height = 1080,background_color = \"black\",mask=twitter_coloring)\n",
    "image_colors = ImageColorGenerator(twitter_coloring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate(fake_hashtags)\n",
    "fig, ax = plt.subplots(figsize=(17,17))\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wc.generate(true_hashtags)\n",
    "fig, ax = plt.subplots(figsize=(17,17))\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags en las ciudades con más desastres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags.dropna(subset = ['location', 'hashtags'], inplace= True)\n",
    "tweets_hashtags = tweets_hashtags.loc[(tweets.target == 1), :]\n",
    "hashtag_by_loc = tweets_hashtags.groupby('location').agg({'location':'count', 'hashtags': 'max'})\n",
    "hashtag_by_loc = hashtag_by_loc.rename(columns = {'location': 'count'})\n",
    "hashtag_by_loc = hashtag_by_loc.reset_index()\n",
    "hashtag_by_loc = hashtag_by_loc.nlargest(25, 'count')\n",
    "hashtag_by_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 20))\n",
    "palette = sns.diverging_palette(220, 20, n=25)\n",
    "grafico = sns.barplot(data = hashtag_by_loc, x = 'count', y = 'hashtags', hue = 'location', dodge = False, palette = palette)\n",
    "grafico.set_title(\"Hashtags mas usados en las top 25 ubicaciones del mundo\", fontsize = 14)\n",
    "grafico.set_xlabel(\"Cantidad de veces mencionado\", fontsize = 14)\n",
    "grafico.set_ylabel(\"Hashtag\", fontsize = 14)\n",
    "plt.legend(title='Ubicacion', loc='lower right')\n",
    "plt.show(grafico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top ciudades con mayor cantidad de tweets reales y falsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No importan las locaciones NAN\n",
    "partial = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = partial[partial['location'].str.match(r'^([a-zA-Z,\\s])*$') > 0]\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['location'] = locations['location'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_locations = locations['location'].value_counts().nlargest(20).to_frame()\n",
    "top_20_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_locations.plot(kind='barh',  rot=0, color=(0.2, 0.4, 0.6, 0.9), figsize=(16, 8))\n",
    "ax = plt.gca()\n",
    "ax.set_title('Top 20 ubicaciones con más tweets\\n', fontsize=40)\n",
    "ax.set_xlabel('Cantidad de tweets', fontsize=30)\n",
    "ax.set_ylabel('Ubicación', fontsize=30)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_location = locations.groupby(['location']).agg({'target':['count', 'sum']})\n",
    "level_0 = by_location.columns.get_level_values(0)\n",
    "level_1 = by_location.columns.get_level_values(1)\n",
    "by_location.columns = level_0 + '_' + level_1\n",
    "by_location.reset_index(inplace=True)\n",
    "by_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_location.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if(not os.path.isfile('locations.csv')):\n",
    "    get_geodata()\n",
    "\n",
    "def get_geodata():\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from tqdm import tqdm\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    geolocator = Nominatim(user_agent=\"mile.marchese@gmail.com\")\n",
    "    tqdm.pandas()\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=2, max_retries=0)\n",
    "    by_location['geodata'] = by_location['location'].progress_apply(geocode)\n",
    "    by_location['address'] = by_location['geodata'].apply(lambda loc: loc.address if loc else None)\n",
    "    by_location['point'] = by_location['geodata'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "    by_location.to_csv('locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = pd.read_csv('locations.csv', usecols=['location', 'target_count', 'target_sum', 'address']) \n",
    "address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay 374 que geopy no encontró\n",
    "address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_addr = address[address['address'].isnull()]\n",
    "not_found_addr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address['address'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_addr['target_count'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address['address'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address = address.groupby(['address']).agg({'target_count':'sum', 'target_sum': 'sum'\\\n",
    "                                               , 'location': lambda x: \"%s\" % '-'.join(set(x))}).reset_index()\n",
    "by_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address.columns = ['address', 'total', 'real', 'location_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address['fake'] = by_address['total'] - by_address['real']\n",
    "by_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_real = by_address.nlargest(10, 'real').loc[:, ['address', 'real']].set_index('address')\n",
    "most_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_real.plot(kind='barh',  rot=0, color=(0.2, 0.5, 0.4, 0.8), figsize=(16, 8))\n",
    "ax = plt.gca()\n",
    "ax.set_title('Top 10 ubicaciones con más tweets reales\\n', fontsize=25)\n",
    "ax.set_xlabel('Cantidad de tweets', fontsize=20)\n",
    "ax.set_ylabel('Ubicación', fontsize=20)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_fake = by_address.nlargest(10, 'fake').loc[:,['address', 'fake']].set_index('address')\n",
    "most_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_fake.plot(kind='barh',  rot=0, color=(0.8, 0.1, 0.2, 0.8), figsize=(16, 8))\n",
    "ax = plt.gca()\n",
    "ax.set_title('Top 10 ubicaciones con más tweets falsos\\n', fontsize=25)\n",
    "ax.set_xlabel('Cantidad de tweets', fontsize=20)\n",
    "ax.set_ylabel('Ubicación', fontsize=20)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ubicaciones no encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_addr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_addr = not_found_addr.drop(columns=['address'])\n",
    "not_found_addr.columns = ['location', 'total', 'real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_fake_loc = not_found_addr.groupby(['location']).agg({'total':'sum', 'real':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_fake_loc['fake'] = grouped_by_fake_loc['total'] - grouped_by_fake_loc['real']\n",
    "grouped_by_fake_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_fake_loc = grouped_by_fake_loc.nlargest(15, 'total')\n",
    "most_used_fake_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_fake_loc_real = grouped_by_fake_loc.nlargest(15, 'real')\n",
    "most_used_fake_loc_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_fake_loc_fake = grouped_by_fake_loc.nlargest(15, 'fake')\n",
    "most_used_fake_loc_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paises en el set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "by_address['country'] = by_address['address'].str.split(pat = \",\")\n",
    "by_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address.loc[:, 'country'] = by_address.country.map(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address.loc[:, 'country'] = by_address.country.str.strip()\n",
    "by_country = by_address.groupby(['country']).agg({'total': 'sum', 'real': 'sum', 'fake': 'sum'}).reset_index().sort_values('total', ascending=False)\n",
    "by_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.Series(by_country.country).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alpha_space = re.compile('^[a-zA-Z_ ]*$')\n",
    "\n",
    "to_change = {}\n",
    "for element in countries:\n",
    "    coincidences = [s for s in countries if element in s]\n",
    "    if len(coincidences) > 1:\n",
    "        name = max(coincidences, key=len)\n",
    "        if not alpha_space.match(name):\n",
    "            name = min(coincidences, key=len)\n",
    "        to_change.update({coincidence:name for coincidence in coincidences if coincidence != name})\n",
    "    \n",
    "print(to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_change.pop('Africa', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country.country = by_country.country.replace(to_change) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country = by_country.groupby(['country']).agg({'total': 'sum', 'real': 'sum', 'fake': 'sum'}).reset_index().sort_values('total', ascending=False)\n",
    "by_country.nlargest(10, 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from googletrans import Translator\n",
    "    translator = Translator()\n",
    "    by_country['translated_country'] = by_country['country'].apply(translator.translate, dest='en').apply(getattr, args=('text',))\n",
    "    by_country.head()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = world.merge(by_country, left_on='name', right_on='translated_country', how='left')\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = world[(world.name!=\"Antarctica\")]\n",
    "fig= plt.figure(figsize=(15,10))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.set_title('Paises presentes en las ubicaciones del dataset', fontsize=25)\n",
    "ax.axis('off')\n",
    "world.plot(column=\"total\", legend=True,\\\n",
    "           legend_kwds={'label': \"Cantidad de tweets por pais\", 'orientation': \"horizontal\"},\\\n",
    "           missing_kwds={\"color\": \"lightgrey\", \"label\": \"Missing values\"}, cmap='tab20b', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_by_country = by_country.nlargest(10, 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify\n",
    "import matplotlib\n",
    "\n",
    "norm = matplotlib.colors.Normalize(vmin=min(top_by_country.total), vmax=max(top_by_country.total))\n",
    "colors = [matplotlib.cm.Blues(norm(value)) for value in top_by_country.total]\n",
    "fig = plt.gcf()\n",
    "ax = fig.add_subplot()\n",
    "fig.set_size_inches(16, 4.5)\n",
    "squarify.plot(label=top_by_country.country,sizes=top_by_country.total, alpha=.6)\n",
    "plt.title('Top paises con más tweets', fontsize=23)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = by_country.set_index('country').nlargest(10, 'total').sort_values(by=['real']).loc[:, ['real', 'fake']].plot(kind='barh', figsize=(10, 10), colormap='tab20')\n",
    "ax.set_title('Top 10 paises con más cantidad de tweets: reales vs falsos\\n', fontsize=25)\n",
    "ax.set_xlabel('Cantidad de tweets', fontsize=20)\n",
    "ax.set_ylabel('Ubicación', fontsize=20)\n",
    "ax.legend(labels=['reales', 'falsos'], fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relacion real-falso con location, keyword vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_location = tweets[tweets['location'].isnull()]\n",
    "no_keyword = tweets[tweets['keyword'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_location.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keyword.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_location['len'] = no_location['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_location['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keyword['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets con ubicación real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_merge = tweets.loc[:, ['text', 'target', 'keyword', 'location']]\n",
    "text_merge['location'] = text_merge['location'].str.lower()\n",
    "locations_data = address.loc[:, ['location', 'address']]\n",
    "text_merge = text_merge.merge(locations_data, left_on='location', right_on='location', how='left')\n",
    "text_merge = text_merge.loc[ (text_merge['address'].isnull() == False),:]\n",
    "text_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_merge['len'] = text_merge['text'].str.len()\n",
    "text_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "sns.boxplot(x=\"target\", y=\"len\", data=text_merge, width = 0.5, ax=ax1)\n",
    "ax1.set_xlabel(\"Veracidad. Real = 1; Falso = 0.\", fontsize=20)\n",
    "ax1.set_ylabel(\"Largo del tweet en caracteres\", fontsize=20)\n",
    "ax1.set_title(\"Tweets con ubicación\",fontsize=25)\n",
    "\n",
    "sns.boxplot(x=\"target\", y=\"len\", data=no_location, width = 0.5, ax=ax2)\n",
    "ax2.set_xlabel(\"Veracidad. Real = 1; Falso = 0.\", fontsize=20)\n",
    "ax2.set_ylabel(\"Largo del tweet en caracteres\", fontsize=20)\n",
    "ax2.set_title(\"Tweets sin ubicación\", fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relación condados costeros de Estados Unidos y el ratio de desastres reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_counties = pd.read_csv('coastal_counties_usa.csv', usecols = ['County','State','FIPS'])\n",
    "coastal_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_counties['Location'] = coastal_counties[['County','State']].agg(','.join, axis = 1) + ',' + coastal_counties['FIPS'].astype('str')\n",
    "#coastal_counties['Location'] = coastal_counties['State'] + ',' + coastal_counties['FIPS'].astype('str')\n",
    "coastal_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_counties_list = coastal_counties['Location'].to_list()\n",
    "coastal_counties_dict = {}\n",
    "for location in coastal_counties_list:\n",
    "    county, state, FIPS = location.split(',')\n",
    "    coastal_counties_dict[county] = coastal_counties_dict.get(county, (state,FIPS))\n",
    "\n",
    "#coastal_counties_dict = pd.Series(coastal_counties['Location'].to_list(),index=coastal_counties['County']).to_dict()\n",
    "#coastal_counties_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def is_coastal(lista):\n",
    "    for word in lista:\n",
    "        if word in coastal_counties_dict: return True\n",
    "    return False\n",
    "\n",
    "def return_coastal_county(lista):\n",
    "    for word in lista:\n",
    "        if word in coastal_counties_dict: \n",
    "            return word + ',' + coastal_counties_dict[word][0]\n",
    "            #return word + ',' + coastal_counties_dict[word].split(',')[0]\n",
    "\n",
    "def return_coastal_county_FIPS(lista):\n",
    "    for word in lista:\n",
    "        if word in coastal_counties_dict: \n",
    "            return coastal_counties_dict[word][1]\n",
    "            #return coastal_counties_dict[word].split(',')[1]\n",
    "\n",
    "us_address = by_address[by_address['address'].str.contains('United States')]\n",
    "us_address['address'] = us_address['address'].apply(lambda x: x.replace(',','').split())\n",
    "us_address['is_coastal'] = us_address['address'].apply(lambda x: is_coastal(x))\n",
    "us_address['coastal_county_name'] = us_address[us_address['is_coastal']]['address'].apply(lambda x: return_coastal_county(x))\n",
    "us_address['coastal_county_FIPS'] = us_address[us_address['is_coastal']]['address'].apply(lambda x: return_coastal_county_FIPS(x))\n",
    "us_address[us_address['is_coastal']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_realdisasters_FIPS = us_address[us_address['is_coastal']].groupby('coastal_county_FIPS')['real'].sum().to_frame()\n",
    "us_realdisasters_FIPS = us_realdisasters_FIPS[us_realdisasters_FIPS['real'] > 0]\n",
    "us_realdisasters_FIPS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fips = us_realdisasters_FIPS.index.tolist()\n",
    "values = us_realdisasters_FIPS['real'].tolist()\n",
    "\n",
    "colorscale = ['rgb(0.2235294117647059, 0.23137254901960785, 0.4745098039215686)',\n",
    " 'rgb(0.3215686274509804, 0.32941176470588235, 0.6392156862745098)',\n",
    " 'rgb(0.4196078431372549, 0.43137254901960786, 0.8117647058823529)',\n",
    " 'rgb(0.611764705882353, 0.6196078431372549, 0.8705882352941177)',\n",
    " 'rgb(0.38823529411764707, 0.4745098039215686, 0.2235294117647059)',\n",
    " 'rgb(0.5490196078431373, 0.6352941176470588, 0.3215686274509804)',\n",
    " 'rgb(0.7098039215686275, 0.8117647058823529, 0.4196078431372549)',\n",
    " 'rgb(0.807843137254902, 0.8588235294117647, 0.611764705882353)',\n",
    " 'rgb(0.5490196078431373, 0.42745098039215684, 0.19215686274509805)',\n",
    " 'rgb(0.7411764705882353, 0.6196078431372549, 0.2235294117647059)',\n",
    " 'rgb(0.9058823529411765, 0.7294117647058823, 0.3215686274509804)',\n",
    " 'rgb(0.9058823529411765, 0.796078431372549, 0.5803921568627451)',\n",
    " 'rgb(0.5176470588235295, 0.23529411764705882, 0.2235294117647059)',\n",
    " 'rgb(0.6784313725490196, 0.28627450980392155, 0.2901960784313726)',\n",
    " 'rgb(0.8392156862745098, 0.3803921568627451, 0.4196078431372549)']\n",
    " \n",
    "fig = ff.create_choropleth(fips=fips, values=values,colorscale = colorscale, title_text = 'Distribución de tweets reales según condado costero de USA',legend_title = 'Cantidad de tweets reales', state_outline={'color': 'rgb(99,99,99)', 'width': 0.3}, asp = 2.9)\n",
    "fig.layout.template = None\n",
    "fig.layout.update({'height': 720})\n",
    "fig.layout.update({'width': 1280})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a lo visto arriba en el mapa, se analiza qué tipo de desastres ocurrieron en dichas ubicaciones.\n",
    "\n",
    "Hipotesis: Como se vio en las coastal counties uno imaginaria que las keywords de dichos tweets estarian relacionados a desastres comunes en proporcion normal y naturales en proporcion mayor, entre los cuales podria estar inundaciones, huracanes, tornados, tormentas tropicales (florida), entre otros desastres naturales que suelen ocurrir en zonas costeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_address_keyword = text_merge[text_merge['address'].str.contains('United States')]\n",
    "us_address_keyword['address'] = us_address_keyword['address'].apply(lambda x: x.replace(',','').split())\n",
    "us_address_keyword['is_coastal'] = us_address_keyword['address'].apply(lambda x: is_coastal(x))\n",
    "grouped_by_keyword = us_address_keyword[us_address_keyword['is_coastal']].groupby('target')['keyword']\n",
    "grouped_by_keyword = grouped_by_keyword.value_counts().to_frame()\n",
    "grouped_by_keyword.columns = ['count']\n",
    "grouped_by_keyword = grouped_by_keyword.reset_index().set_index('keyword')\n",
    "grouped_by_keyword.index = grouped_by_keyword.index.str.replace('%20',' ')\n",
    "grouped_by_keyword[grouped_by_keyword['target'] == 1]['count'].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "data = grouped_by_keyword[grouped_by_keyword['target'] == 1]['count'].nlargest(20).to_frame()\n",
    "\n",
    "ax = sns.barplot(x = \"count\", y = data.index, data = data, orient = 'h', palette = 'tab20c')\n",
    "ax.set_title('Top 20 tipos de desastres reales en zona costera', fontsize = 20)\n",
    "ax.set_xlabel('Cantidad de desastres ', fontsize = 15)\n",
    "ax.set_ylabel('Tipo de desastre', fontsize = 15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
